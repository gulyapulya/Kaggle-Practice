{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bags of Popcorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup    \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Hash numbers and remove non-letters \n",
    "    hashed_review_text = re.sub('\\d', '#', review_text) \n",
    "    lim_hashed_review_text = re.sub(\"#####+\", \"#####\", hashed_review_text)\n",
    "    letters_only = re.sub(\"[^a-zA-Z#]\", \" \", lim_hashed_review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words and lemmatize\n",
    "    meaningful_words = [WordNetLemmatizer().lemmatize(w) for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'##### flower liking like like #####'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the clean\n",
    "review_to_words(\"56567 flowers /liking likes like 6789098765678 ///\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set movie reviews...\n",
      "\n",
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = train[\"review\"].size\n",
    "print \"Cleaning and parsing the training set movie reviews...\\n\"\n",
    "clean_train_reviews = []\n",
    "for i in xrange( 0, num_reviews ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % ( i+1, num_reviews )                                                                    \n",
    "    clean_train_reviews.append( review_to_words( train[\"review\"][i] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set movie reviews...\n",
      "\n",
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num__imdb_reviews = train_imdb[\"review\"].size\n",
    "print \"Cleaning and parsing the training set movie reviews...\\n\"\n",
    "clean_train_imdb_reviews = []\n",
    "for i in xrange( 0, num__imdb_reviews ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % ( i+1, num__imdb_reviews )      \n",
    "    clean_train_imdb_reviews.append(review_to_words( train_imdb[\"review\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000)\n",
    "\n",
    "train_data_features_tfidf = vectorizer.fit_transform(clean_train_reviews)\n",
    "train_data_features_tfidf = train_data_features_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imdb_data_features_tfidf = vectorizer.fit_transform(clean_train_imdb_reviews)\n",
    "train_imdb_data_features_tfidf = train_imdb_data_features_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression for Tf-Idf Vec IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'warn',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'warn',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_regression_tfidf = LogisticRegression()\n",
    "log_regression_tfidf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Best CV score: 0.936\n",
      "('Best parameters set:', {'warm_start': False, 'C': 1.0, 'n_jobs': None, 'verbose': 0, 'intercept_scaling': 1, 'fit_intercept': True, 'max_iter': 100, 'penalty': 'l1', 'multi_class': 'warn', 'random_state': None, 'dual': False, 'tol': 0.0001, 'solver': 'warn', 'class_weight': None})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': (0.25, 0.5, 1.0), 'penalty': ('l1', 'l2')}\n",
    "grid_search = GridSearchCV(log_regression_tfidf, param_grid, n_jobs=-1, scoring=\"roc_auc\")\n",
    "print(\"Performing grid search...\")\n",
    "\n",
    "grid_search.fit(train_imdb_data_features_tfidf, train_imdb.sentiment)\n",
    "print(\"Best CV score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\", grid_search.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "Cleaning and parsing the training set movie reviews...\n",
      "\n",
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n",
      "Test score with best_estimator_: 0.877\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88     12500\n",
      "           1       0.87      0.89      0.88     12500\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print test_imdb.shape\n",
    "\n",
    "# Create an empty list and append the clean reviews one by one\n",
    "num__imdb_reviews = test_imdb[\"review\"].size\n",
    "print \"Cleaning and parsing the training set movie reviews...\\n\"\n",
    "clean_test_imdb_reviews = []\n",
    "for i in xrange( 0, num__imdb_reviews ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % ( i+1, num__imdb_reviews )      \n",
    "    clean_test_imdb_reviews.append(review_to_words( test_imdb[\"review\"][i]))\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_imdb_data_features_tfidf = vectorizer.transform(clean_test_imdb_reviews)\n",
    "test_imdb_data_features_tfidf = test_imdb_data_features_tfidf.toarray()\n",
    "    \n",
    "print(\"Test score with best_estimator_: %0.3f\" % grid_search.best_estimator_.score(test_imdb_data_features_tfidf, test_imdb.sentiment))\n",
    "print(\"Classification Report Test Data\")\n",
    "print(classification_report(test_imdb.sentiment, grid_search.best_estimator_.predict(test_imdb_data_features_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression for Tf-Idf Vec Kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'warn',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'warn',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_regression = LogisticRegression()\n",
    "log_regression.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Best CV score: 0.951\n",
      "('Best parameters set:', {'warm_start': False, 'C': 1.0, 'n_jobs': None, 'verbose': 0, 'intercept_scaling': 1, 'fit_intercept': True, 'max_iter': 100, 'penalty': 'l2', 'multi_class': 'warn', 'random_state': None, 'dual': False, 'tol': 0.0001, 'solver': 'warn', 'class_weight': None})\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': (0.25, 0.5, 1.0), 'penalty': ('l1', 'l2')}\n",
    "grid_search_tfidf = GridSearchCV(log_regression, param_grid, n_jobs=-1, cv=5, scoring=\"roc_auc\")\n",
    "print(\"Performing grid search...\")\n",
    "\n",
    "grid_search_tfidf.fit(train_data_features_tfidf, train.sentiment)\n",
    "print(\"Best CV score: %0.3f\" % grid_search_tfidf.best_score_)\n",
    "print(\"Best parameters set:\", grid_search_tfidf.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the test data\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
    "                   quoting=3 )\n",
    "\n",
    "# Verify that there are 25,000 rows and 2 columns\n",
    "print test.shape\n",
    "\n",
    "# Create an empty list and append the clean reviews one by one\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = [] \n",
    "\n",
    "print \"Cleaning and parsing the test set movie reviews...\\n\"\n",
    "for i in xrange(0,num_reviews):\n",
    "    if( (i+1) % 1000 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % (i+1, num_reviews)\n",
    "    clean_review = review_to_words( test[\"review\"][i] )\n",
    "    clean_test_reviews.append( clean_review )\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features_tfidf = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features_tfidf = test_data_features_tfidf.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = grid_search_tfidf.best_estimator_.predict(test_data_features_tfidf)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"Tfidf_LinReg_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest for Tf-Idf Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "print \"Training the random forest...\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "forest = forest.fit(train_data_features, train[\"sentiment\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features_tfidf)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"Tfidf_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for Tf-Idf Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "mlp_tfidf = mlp.fit(train_data_features, train[\"sentiment\"])\n",
    "\n",
    "# Make sentiment label predictions\n",
    "result = mlp_tfidf.predict(test_data_features_tfidf)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"Tfidf_mlp_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features_bw = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features_bw = train_data_features_bw.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'abandoned', u'abc', u'ability', u'able', u'abraham', u'abrupt', u'absence', u'absent', u'absolute', u'absolutely', u'absurd', u'absurdity', u'abuse', u'abused', u'abusive', u'abysmal', u'academy', u'accent', u'accept', u'acceptable', u'accepted', u'accepts', u'access', u'accident', u'accidentally', u'acclaimed', u'accompanied', u'accomplish', u'accomplished', u'according', u'account', u'accuracy', u'accurate', u'accused', u'ace', u'achieve', u'achieved', u'achievement', u'acid', u'across', u'act', u'acted', u'acting', u'action', u'active', u'activity', u'actor', u'actress', u'actual', u'actually', u'ad', u'adam', u'adaptation', u'adapted', u'add', u'added', u'addict', u'addicted', u'addiction', u'adding', u'addition', u'additional', u'address', u'adequate', u'admirable', u'admire', u'admit', u'admittedly', u'adolescent', u'adopted', u'adorable', u'adult', u'advance', u'advanced', u'advantage', u'adventure', u'advertising', u'advice', u'advise', u'affair', u'affect', u'affected', u'affection', u'afford', u'aforementioned', u'afraid', u'africa', u'african', u'afternoon', u'afterwards', u'age', u'aged', u'agency', u'agenda', u'agent', u'aging', u'ago', u'agree', u'agreed', u'agrees', u'ah', u'ahead', u'aid', u'aim', u'aimed', u'air', u'aired', u'airplane', u'airport', u'aka', u'akshay', u'al', u'ala', u'alan', u'albeit', u'albert', u'album', u'alcohol', u'alcoholic', u'alec', u'alert', u'alex', u'alexander', u'alfred', u'alice', u'alicia', u'alien', u'alike', u'alison', u'alive', u'allen', u'alley', u'allow', u'allowed', u'allowing', u'allows', u'ally', u'almost', u'alone', u'along', u'alongside', u'already', u'alright', u'also', u'alternate', u'alternative', u'although', u'altman', u'altogether', u'always', u'amanda', u'amateur', u'amateurish', u'amazed', u'amazing', u'amazingly', u'amazon', u'ambiguous', u'ambition', u'ambitious', u'america', u'american', u'amitabh', u'among', u'amongst', u'amount', u'amused', u'amusement', u'amusing', u'amy', u'analysis', u'ancient', u'anderson', u'andre', u'andrew', u'andy', u'angel', u'angela', u'angeles', u'anger', u'angle', u'angry', u'animal', u'animated', u'animation', u'animator', u'anime', u'ann', u'anna', u'anne', u'annie', u'annoyed', u'annoying', u'another', u'answer', u'ant', u'anthony', u'anti', u'antic', u'anticipation', u'antonioni', u'antwone', u'anybody', u'anymore', u'anyone', u'anything', u'anyway', u'anyways', u'anywhere', u'apart', u'apartment', u'ape', u'appalling', u'apparent', u'apparently', u'appeal', u'appealing', u'appear', u'appearance', u'appeared', u'appearing', u'appears', u'appreciate', u'appreciated', u'appreciation', u'approach', u'appropriate', u'appropriately', u'april', u'arab', u'arc', u'area', u'arguably', u'argue', u'argument', u'arm', u'armed', u'army', u'arnold', u'around', u'arrested', u'arrival', u'arrive', u'arrived', u'arrives', u'arrogant', u'arrow', u'art', u'arthur', u'artificial', u'artist', u'artistic', u'artsy', u'as', u'ashamed', u'ashley', u'asian', u'aside', u'ask', u'asked', u'asking', u'asks', u'asleep', u'aspect', u'aspiring', u'assassin', u'assault', u'assigned', u'assistant', u'associate', u'associated', u'assume', u'assumed', u'astaire', u'astonishing', u'asylum', u'atlantis', u'atmosphere', u'atmospheric', u'atrocious', u'atrocity', u'attached', u'attack', u'attacked', u'attempt', u'attempted', u'attempting', u'attenborough', u'attend', u'attention', u'attitude', u'attorney', u'attracted', u'attraction', u'attractive', u'audience', u'audio', u'audition', u'aunt', u'austen', u'austin', u'australia', u'australian', u'authentic', u'author', u'authority', u'automatically', u'available', u'average', u'avoid', u'avoided', u'awake', u'award', u'aware', u'away', u'awe', u'awesome', u'awful', u'awfully', u'awhile', u'awkward', u'babe', u'baby', u'bacall', u'back', u'backdrop', u'background', u'bacon', u'bad', u'baddie', u'badly', u'bag', u'baker', u'bakshi', u'balance', u'baldwin', u'ball', u'ballet', u'band', u'bang', u'bank', u'banned', u'bar', u'barbara', u'bare', u'barely', u'bargain', u'barrel', u'barry', u'barrymore', u'base', u'baseball', u'based', u'basement', u'basic', u'basically', u'basinger', u'basis', u'basketball', u'bat', u'bath', u'bathroom', u'batman', u'battle', u'bay', u'bbc', u'beach', u'bear', u'beast', u'beat', u'beaten', u'beating', u'beatles', u'beatty', u'beautiful', u'beautifully', u'beauty', u'became', u'become', u'becomes', u'becoming', u'bed', u'bedroom', u'beer', u'began', u'begin', u'beginning', u'behave', u'behavior', u'behaviour', u'behind', u'being', u'bela', u'belief', u'believable', u'believe', u'believed', u'believing', u'bell', u'belong', u'belongs', u'beloved', u'belt', u'belushi', u'ben', u'beneath', u'benefit', u'bergman', u'berlin', u'bernard', u'besides', u'best', u'bet', u'betrayal', u'bette', u'better', u'bettie', u'betty', u'beverly', u'beware', u'beyond', u'biased', u'bible', u'big', u'bigger', u'biggest', u'bike', u'biko', u'bill', u'billed', u'billy', u'bin', u'biography', u'bird', u'birth', u'birthday', u'bit', u'bitch', u'bite', u'bitter', u'bizarre', u'black', u'blade', u'blah', u'blair', u'blake', u'blame', u'bland', u'blank', u'blast', u'blatant', u'bleak', u'blend', u'blew', u'blind', u'blob', u'block', u'blockbuster', u'blond', u'blonde', u'blood', u'bloody', u'blow', u'blowing', u'blown', u'blue', u'blunt', u'bo', u'board', u'boast', u'boat', u'bob', u'bobby', u'body', u'bold', u'boll', u'bollywood', u'bomb', u'bond', u'bone', u'bonus', u'book', u'boom', u'boot', u'border', u'bore', u'bored', u'boredom', u'boring', u'born', u'borrowed', u'bos', u'bother', u'bothered', u'bottle', u'bottom', u'bought', u'bound', u'bourne', u'bow', u'box', u'boxer', u'boxing', u'boy', u'boyfriend', u'boyle', u'brad', u'brady', u'brain', u'branagh', u'brand', u'brando', u'brave', u'brazil', u'bread', u'break', u'breaking', u'breast', u'breath', u'breathtaking', u'breed', u'brenda', u'brendan', u'brian', u'bride', u'bridge', u'bridget', u'brief', u'briefly', u'bright', u'brilliance', u'brilliant', u'brilliantly', u'bring', u'bringing', u'brings', u'brit', u'britain', u'british', u'broad', u'broadcast', u'broadway', u'broke', u'broken', u'bronson', u'brook', u'brooklyn', u'brosnan', u'brother', u'brought', u'brown', u'bruce', u'brutal', u'brutality', u'brutally', u'bubble', u'buck', u'bud', u'buddy', u'budget', u'buff', u'buffalo', u'bug', u'build', u'building', u'built', u'bull', u'bullet', u'bully', u'bumbling', u'bunch', u'bunny', u'buried', u'burn', u'burned', u'burning', u'burst', u'burt', u'burton', u'bus', u'bush', u'business', u'businessman', u'buster', u'busy', u'butler', u'butt', u'button', u'buy', u'buying', u'bye', u'cabin', u'cable', u'cage', u'cagney', u'caine', u'cake', u'caliber', u'california', u'call', u'called', u'calling', u'calm', u'came', u'cameo', u'camera', u'cameron', u'camp', u'campaign', u'campbell', u'campy', u'canada', u'canadian', u'cancer', u'candidate', u'candle', u'candy', u'cannibal', u'cannon', u'cannot', u'cant', u'canyon', u'capable', u'capital', u'captain', u'captivating', u'capture', u'captured', u'capturing', u'car', u'card', u'cardboard', u'care', u'cared', u'career', u'careful', u'carefully', u'carell', u'caricature', u'caring', u'carl', u'carla', u'carol', u'carpenter', u'carradine', u'carrey', u'carrie', u'carried', u'carry', u'carrying', u'carter', u'cartoon', u'cary', u'case', u'cash', u'cassidy', u'cast', u'casting', u'castle', u'cat', u'catch', u'catching', u'catchy', u'category', u'catherine', u'catholic', u'cattle', u'caught', u'cause', u'caused', u'causing', u'cave', u'cd', u'celebration', u'celebrity', u'cell', u'celluloid', u'cent', u'center', u'centered', u'central', u'centre', u'century', u'certain', u'certainly', u'cg', u'cgi', u'chain', u'chainsaw', u'chair', u'challenge', u'challenged', u'challenging', u'chamberlain', u'champion', u'championship', u'chan', u'chance', u'change', u'changed', u'changing', u'channel', u'chaos', u'chaplin', u'chapter', u'character', u'characteristic', u'characterization', u'charge', u'charged', u'charisma', u'charismatic', u'charles', u'charlie', u'charlotte', u'charm', u'charming', u'chase', u'chased', u'chasing', u'chavez', u'che', u'cheap', u'cheated', u'cheating', u'check', u'checked', u'checking', u'cheek', u'cheer', u'cheese', u'cheesy', u'chemistry', u'chess', u'chest', u'chicago', u'chick', u'chicken', u'chief', u'child', u'childhood', u'childish', u'chill', u'chilling', u'china', u'chinese', u'choice', u'choose', u'chooses', u'chop', u'choppy', u'choreographed', u'choreography', u'chorus', u'chose', u'chosen', u'chris', u'christ', u'christian', u'christianity', u'christine', u'christmas', u'christopher', u'christy', u'chronicle', u'chuck', u'chuckle', u'church', u'cia', u'cigarette', u'cinderella', u'cinema', u'cinematic', u'cinematographer', u'cinematography', u'circle', u'circumstance', u'circus', u'citizen', u'city', u'civil', u'civilization', u'claim', u'claimed', u'claire', u'clan', u'clark', u'clarke', u'clash', u'class', u'classic', u'classical', u'claus', u'clean', u'clear', u'clearly', u'clerk', u'clever', u'cleverly', u'clich', u'cliche', u'cliff', u'climactic', u'climax', u'climb', u'clint', u'clip', u'clock', u'clone', u'close', u'closed', u'closely', u'closer', u'closest', u'closet', u'closing', u'clothes', u'clothing', u'cloud', u'clown', u'club', u'clue', u'clumsy', u'co', u'coach', u'coast', u'coaster', u'coat', u'code', u'coffee', u'coherent', u'coincidence', u'cold', u'cole', u'colleague', u'collection', u'college', u'colonel', u'color', u'colorful', u'colour', u'columbo', u'com', u'combat', u'combination', u'combine', u'combined', u'come', u'comedian', u'comedic', u'comedy', u'comfort', u'comfortable', u'comic', u'comical', u'coming', u'command', u'commander', u'comment', u'commentary', u'commented', u'commercial', u'commit', u'committed', u'common', u'communist', u'community', u'companion', u'company', u'compare', u'compared', u'comparing', u'comparison', u'compassion', u'compelled', u'compelling', u'competent', u'competition', u'complain', u'complaining', u'complaint', u'complete', u'completely', u'complex', u'complexity', u'complicated', u'compliment', u'composed', u'composer', u'composition', u'computer', u'con', u'conceived', u'concentrate', u'concept', u'concern', u'concerned', u'concerning', u'concert', u'conclusion', u'condition', u'confess', u'confidence', u'conflict', u'confrontation', u'confused', u'confusing', u'confusion', u'connect', u'connected', u'connection', u'connery', u'conscience', u'conscious', u'consequence', u'conservative', u'consider', u'considerable', u'considered', u'considering', u'consistent', u'consistently', u'consists', u'conspiracy', u'constant', u'constantly', u'constructed', u'construction', u'contact', u'contain', u'contained', u'contains', u'contemporary', u'content', u'contest', u'contestant', u'context', u'continue', u'continued', u'continues', u'continuity', u'contract', u'contrary', u'contrast', u'contribution', u'contrived', u'control', u'controlled', u'controversial', u'convention', u'conventional', u'conversation', u'convey', u'convict', u'conviction', u'convince', u'convinced', u'convincing', u'convincingly', u'convoluted', u'cook', u'cool', u'cooper', u'cop', u'cope', u'copy', u'core', u'corner', u'corny', u'corporate', u'corporation', u'corps', u'corpse', u'correct', u'correctly', u'corrupt', u'corruption', u'cost', u'costume', u'couch', u'could', u'count', u'counter', u'counterpart', u'countless', u'country', u'countryside', u'coup', u'couple', u'courage', u'course', u'court', u'cousin', u'cover', u'covered', u'cowboy', u'cox', u'crack', u'cracking', u'craft', u'crafted', u'craig', u'crap', u'crappy', u'crash', u'craven', u'crawford', u'crazed', u'crazy', u'cream', u'create', u'created', u'creates', u'creating', u'creation', u'creative', u'creativity', u'creator', u'creature', u'credibility', u'credible', u'credit', u'credited', u'creep', u'creepy', u'crew', u'cried', u'crime', u'criminal', u'cringe', u'crisis', u'critic', u'critical', u'criticism', u'critique', u'crocodile', u'crook', u'cross', u'crowd', u'crucial', u'crude', u'cruel', u'cruelty', u'cruise', u'crush', u'cry', u'crystal', u'cuba', u'cube', u'cue', u'cult', u'cultural', u'culture', u'cup', u'cure', u'curiosity', u'curious', u'curly', u'current', u'currently', u'curse', u'curtis', u'cusack', u'cut', u'cute', u'cutting', u'cyborg', u'cynical', u'da', u'dad', u'daddy', u'daily', u'daisy', u'dalton', u'damage', u'damme', u'damn', u'damon', u'dan', u'dana', u'dance', u'dancer', u'dancing', u'dandy', u'dane', u'danger', u'dangerous', u'daniel', u'danny', u'dare', u'daring', u'dark', u'darker', u'darkness', u'darn', u'darren', u'date', u'dated', u'dating', u'daughter', u'dave', u'david', u'davis', u'davy', u'dawn', u'dawson', u'day', u'de', u'dead', u'deadly', u'deaf', u'deal', u'dealer', u'dealing', u'dealt', u'dean', u'dear', u'death', u'debate', u'debt', u'debut', u'decade', u'deceased', u'decent', u'decide', u'decided', u'decides', u'decision', u'dedicated', u'dee', u'deed', u'deep', u'deeper', u'deeply', u'defeat', u'defend', u'defense', u'defined', u'definite', u'definitely', u'definition', u'degree', u'del', u'deleted', u'deliberately', u'delight', u'delightful', u'deliver', u'delivered', u'delivering', u'delivers', u'delivery', u'demand', u'demented', u'demise', u'demon', u'deniro', u'dennis', u'dentist', u'denzel', u'department', u'depicted', u'depicting', u'depiction', u'depicts', u'depressed', u'depressing', u'depression', u'depth', u'der', u'deranged', u'derek', u'descent', u'describe', u'described', u'describes', u'description', u'desert', u'deserted', u'deserve', u'deserved', u'deserves', u'design', u'designed', u'designer', u'desire', u'desired', u'despair', u'desperate', u'desperately', u'desperation', u'despite', u'destiny', u'destroy', u'destroyed', u'destroying', u'destruction', u'detail', u'detailed', u'detective', u'determined', u'develop', u'developed', u'developing', u'development', u'develops', u'device', u'devil', u'devoid', u'devoted', u'dialog', u'dialogue', u'diamond', u'diana', u'diane', u'diary', u'dick', u'dickens', u'die', u'died', u'difference', u'different', u'difficult', u'difficulty', u'dig', u'digital', u'dignity', u'dilemma', u'dimension', u'dimensional', u'din', u'dinner', u'dinosaur', u'dire', u'direct', u'directed', u'directing', u'direction', u'directly', u'director', u'directorial', u'directs', u'dirt', u'dirty', u'disagree', u'disappear', u'disappeared', u'disappears', u'disappoint', u'disappointed', u'disappointing', u'disappointment', u'disaster', u'disbelief', u'disc', u'discover', u'discovered', u'discovering', u'discovers', u'discovery', u'discus', u'discussion', u'disease', u'disguise', u'disgusting', u'disjointed', u'dislike', u'disliked', u'disney', u'disorder', u'display', u'displayed', u'distance', u'distant', u'distinct', u'distracting', u'distribution', u'disturbed', u'disturbing', u'divorce', u'dixon', u'doc', u'doctor', u'document', u'documentary', u'dog', u'doll', u'dollar', u'dolph', u'domestic', u'domino', u'donald', u'done', u'donna', u'dont', u'doo', u'doom', u'doomed', u'door', u'dorothy', u'dose', u'double', u'doubt', u'douglas', u'downey', u'downhill', u'downright', u'dozen', u'dr', u'dracula', u'drag', u'dragged', u'dragon', u'drake', u'drama', u'dramatic', u'draw', u'drawing', u'drawn', u'dreadful', u'dream', u'dreary', u'dreck', u'dress', u'dressed', u'dressing', u'drew', u'drink', u'drinking', u'drive', u'drivel', u'driven', u'driver', u'driving', u'drop', u'dropped', u'dropping', u'drove', u'drug', u'drum', u'drunk', u'drunken', u'dry', u'dub', u'dubbed', u'dubbing', u'duck', u'dud', u'dude', u'due', u'duke', u'dull', u'dumb', u'dump', u'duo', u'dust', u'dutch', u'duty', u'duvall', u'dvd', u'dy', u'dying', u'dynamic', u'dysfunctional', u'eager', u'ear', u'earl', u'earlier', u'early', u'earned', u'earth', u'ease', u'easier', u'easily', u'east', u'eastern', u'eastwood', u'easy', u'eat', u'eaten', u'eating', u'eccentric', u'echo', u'ed', u'eddie', u'edgar', u'edge', u'edgy', u'edie', u'edit', u'edited', u'editing', u'edition', u'editor', u'education', u'educational', u'edward', u'eerie', u'effect', u'effective', u'effectively', u'effort', u'egg', u'ego', u'eight', u'eighty', u'either', u'el', u'elaborate', u'elderly', u'election', u'electric', u'elegant', u'element', u'elephant', u'elevator', u'elite', u'elizabeth', u'ellen', u'elm', u'else', u'elsewhere', u'elvira', u'elvis', u'em', u'embarrassed', u'embarrassing', u'embarrassment', u'embrace', u'emily', u'emma', u'emotion', u'emotional', u'emotionally', u'empathy', u'emperor', u'emphasis', u'empire', u'employee', u'empty', u'en', u'encounter', u'encourage', u'end', u'endearing', u'ended', u'ending', u'endless', u'endure', u'enemy', u'energy', u'engage', u'engaged', u'engaging', u'england', u'english', u'enjoy', u'enjoyable', u'enjoyed', u'enjoying', u'enjoyment', u'enjoys', u'enormous', u'enough', u'ensemble', u'ensues', u'enter', u'enterprise', u'enters', u'entertain', u'entertained', u'entertaining', u'entertainment', u'enthusiasm', u'entire', u'entirely', u'entry', u'environment', u'epic', u'episode', u'equal', u'equally', u'equipment', u'equivalent', u'er', u'era', u'eric', u'erotic', u'error', u'escape', u'escaped', u'especially', u'essence', u'essential', u'essentially', u'established', u'estate', u'esther', u'et', u'etc', u'eternal', u'ethan', u'ethnic', u'eugene', u'europe', u'european', u'eva', u'eve', u'even', u'evening', u'event', u'eventually', u'ever', u'every', u'everybody', u'everyday', u'everyone', u'everything', u'everywhere', u'evidence', u'evident', u'evidently', u'evil', u'ex', u'exact', u'exactly', u'exaggerated', u'examination', u'example', u'excellent', u'except', u'exception', u'exceptional', u'exceptionally', u'excess', u'excessive', u'exchange', u'excited', u'excitement', u'exciting', u'excuse', u'executed', u'execution', u'executive', u'exercise', u'exist', u'existed', u'existence', u'existent', u'exists', u'exotic', u'expect', u'expectation', u'expected', u'expecting', u'expedition', u'expensive', u'experience', u'experienced', u'experiment', u'experimental', u'expert', u'explain', u'explained', u'explaining', u'explains', u'explanation', u'explicit', u'exploit', u'exploitation', u'exploration', u'explore', u'explored', u'explosion', u'expose', u'exposed', u'exposition', u'exposure', u'express', u'expressed', u'expression', u'extended', u'extent', u'exterior', u'extra', u'extraordinary', u'extreme', u'extremely', u'eye', u'eyed', u'eyre', u'fabulous', u'face', u'faced', u'facial', u'facing', u'fact', u'factor', u'factory', u'fade', u'fail', u'failed', u'failing', u'fails', u'failure', u'fair', u'fairly', u'fairy', u'faith', u'faithful', u'fake', u'falk', u'fall', u'fallen', u'falling', u'false', u'fame', u'familiar', u'family', u'famous', u'fan', u'fanatic', u'fancy', u'fantastic', u'fantasy', u'far', u'farce', u'fare', u'farm', u'farmer', u'farrell', u'fascinated', u'fascinating', u'fascination', u'fashion', u'fashioned', u'fast', u'faster', u'fat', u'fatal', u'fate', u'father', u'fault', u'favor', u'favorite', u'favour', u'favourite', u'fay', u'fbi', u'fear', u'feat', u'feature', u'featured', u'featuring', u'fed', u'feed', u'feel', u'feeling', u'felix', u'fell', u'fellow', u'felt', u'female', u'feminist', u'femme', u'fest', u'festival', u'fetched', u'fever', u'fi', u'fianc', u'fiction', u'fictional', u'fido', u'field', u'fifteen', u'fifty', u'fight', u'fighter', u'fighting', u'figure', u'figured', u'file', u'fill', u'filled', u'filler', u'filling', u'film', u'filmed', u'filming', u'filmmaker', u'final', u'finale', u'finally', u'financial', u'find', u'finding', u'fine', u'finest', u'finger', u'finish', u'finished', u'fire', u'fired', u'firm', u'first', u'firstly', u'fish', u'fisher', u'fishing', u'fist', u'fit', u'fitting', u'five', u'fix', u'flair', u'flame', u'flash', u'flashback', u'flat', u'flaw', u'flawed', u'flawless', u'flesh', u'flick', u'flight', u'floating', u'floor', u'flop', u'florida', u'flow', u'flower', u'fly', u'flying', u'flynn', u'focus', u'focused', u'focusing', u'fog', u'folk', u'follow', u'followed', u'following', u'follows', u'fond', u'fonda', u'fontaine', u'food', u'fool', u'fooled', u'foot', u'footage', u'football', u'forbidden', u'force', u'forced', u'ford', u'foreign', u'forest', u'forever', u'forget', u'forgettable', u'forgive', u'forgot', u'forgotten', u'form', u'format', u'former', u'formula', u'formulaic', u'forth', u'fortunately', u'fortune', u'forty', u'forward', u'foster', u'fought', u'foul', u'found', u'four', u'fourth', u'fox', u'foxx', u'frame', u'framed', u'france', u'franchise', u'francis', u'francisco', u'franco', u'frank', u'frankenstein', u'frankie', u'frankly', u'freak', u'fred', u'freddy', u'free', u'freedom', u'freeman', u'freeze', u'french', u'frequent', u'frequently', u'fresh', u'friday', u'friend', u'friendly', u'friendship', u'frightened', u'frightening', u'front', u'frustrated', u'frustrating', u'frustration', u'fu', u'fulci', u'full', u'fuller', u'fully', u'fun', u'function', u'funeral', u'funnier', u'funniest', u'funny', u'furious', u'furthermore', u'fury', u'future', u'futuristic', u'fx', u'gabriel', u'gadget', u'gag', u'gain', u'gal', u'gambling', u'game', u'gandhi', u'gang', u'gangster', u'gap', u'garbage', u'garbo', u'garden', u'garner', u'gary', u'gas', u'gate', u'gather', u'gave', u'gay', u'gear', u'geek', u'gem', u'gender', u'gene', u'general', u'generally', u'generated', u'generation', u'generic', u'generous', u'genie', u'genius', u'genre', u'gentle', u'gentleman', u'genuine', u'genuinely', u'george', u'gerard', u'german', u'germany', u'gesture', u'get', u'getting', u'ghost', u'giallo', u'giant', u'gift', u'gifted', u'gillian', u'gimmick', u'gina', u'ginger', u'girl', u'girlfriend', u'give', u'given', u'giving', u'glad', u'glance', u'glass', u'glenn', u'glimpse', u'global', u'globe', u'glorious', u'glory', u'glover', u'glowing', u'go', u'goal', u'god', u'godfather', u'godzilla', u'goer', u'going', u'gold', u'goldberg', u'golden', u'gone', u'gonna', u'good', u'goodness', u'goofy', u'gordon', u'gore', u'gorgeous', u'gory', u'got', u'gothic', u'gotta', u'gotten', u'government', u'grab', u'grace', u'grade', u'gradually', u'graham', u'grand', u'grandfather', u'grandmother', u'grant', u'granted', u'graphic', u'grasp', u'gratuitous', u'grave', u'gray', u'grayson', u'great', u'greater', u'greatest', u'greatly', u'greatness', u'greed', u'greedy', u'greek', u'green', u'greg', u'gregory', u'grew', u'grey', u'grief', u'griffith', u'grim', u'grinch', u'grip', u'gripping', u'gritty', u'gross', u'grotesque', u'ground', u'group', u'grow', u'growing', u'grown', u'grows', u'grudge', u'gruesome', u'guarantee', u'guaranteed', u'guard', u'guess', u'guessed', u'guessing', u'guest', u'guide', u'guilt', u'guilty', u'guitar', u'gun', u'gundam', u'gut', u'guy', u'ha', u'habit', u'hack', u'hackneyed', u'hair', u'haired', u'hal', u'half', u'halfway', u'hall', u'halloween', u'ham', u'hamilton', u'hamlet', u'hammer', u'hammy', u'han', u'hand', u'handed', u'handful', u'handle', u'handled', u'handling', u'handsome', u'hang', u'hanging', u'hank', u'happen', u'happened', u'happening', u'happens', u'happily', u'happiness', u'happy', u'hard', u'hardcore', u'harder', u'hardly', u'hardy', u'harm', u'harris', u'harry', u'harsh', u'hart', u'hartley', u'harvey', u'hat', u'hate', u'hated', u'hatred', u'haunt', u'haunted', u'haunting', u'hawke', u'hbo', u'head', u'headache', u'headed', u'heading', u'health', u'hear', u'heard', u'hearing', u'hears', u'heart', u'heartbreaking', u'hearted', u'heartfelt', u'heat', u'heaven', u'heavily', u'heavy', u'heck', u'heel', u'height', u'heist', u'held', u'helen', u'helicopter', u'hell', u'hello', u'help', u'helped', u'helping', u'hence', u'henchman', u'henry', u'hepburn', u'hero', u'heroic', u'heroine', u'heston', u'hey', u'hidden', u'hide', u'hideous', u'hiding', u'high', u'higher', u'highest', u'highlight', u'highly', u'hilarious', u'hilariously', u'hill', u'hint', u'hip', u'hippie', u'hippy', u'hire', u'hired', u'historic', u'historical', u'historically', u'history', u'hit', u'hitchcock', u'hitler', u'hitting', u'hk', u'ho', u'hoffman', u'hold', u'holding', u'hole', u'holiday', u'hollow', u'holly', u'hollywood', u'holmes', u'holy', u'homage', u'home', u'homeless', u'homer', u'homosexual', u'honest', u'honestly', u'honesty', u'hong', u'honor', u'hood', u'hook', u'hooked', u'hop', u'hope', u'hoped', u'hopefully', u'hopeless', u'hopelessly', u'hoping', u'hopkins', u'hopper', u'horrendous', u'horrible', u'horribly', u'horrid', u'horrific', u'horrifying', u'horror', u'horse', u'hospital', u'host', u'hot', u'hotel', u'hour', u'house', u'household', u'housewife', u'howard', u'however', u'hudson', u'huge', u'hugh', u'hughes', u'huh', u'human', u'humanity', u'humble', u'humor', u'humorous', u'humour', u'hundred', u'hung', u'hungry', u'hunt', u'hunter', u'hunting', u'hurt', u'husband', u'hyde', u'hype', u'hyped', u'hysterical', u'ian', u'ice', u'icon', u'idea', u'ideal', u'identify', u'identity', u'idiot', u'idiotic', u'ignorance', u'ignorant', u'ignore', u'ignored', u'ii', u'iii', u'ill', u'illegal', u'illness', u'illogical', u'im', u'image', u'imagery', u'imaginable', u'imagination', u'imaginative', u'imagine', u'imagined', u'imdb', u'imitation', u'immediate', u'immediately', u'immensely', u'immigrant', u'impact', u'implausible', u'importance', u'important', u'importantly', u'impossible', u'impress', u'impressed', u'impression', u'impressive', u'improve', u'improved', u'improvement', u'inability', u'inane', u'inappropriate', u'incident', u'include', u'included', u'includes', u'including', u'incoherent', u'incompetent', u'incomprehensible', u'inconsistent', u'increasingly', u'incredible', u'incredibly', u'indeed', u'independent', u'india', u'indian', u'indication', u'indie', u'individual', u'inducing', u'indulgent', u'industrial', u'industry', u'inept', u'inevitable', u'inevitably', u'inexplicably', u'infamous', u'infected', u'inferior', u'influence', u'influenced', u'information', u'ingredient', u'inhabitant', u'initial', u'initially', u'injury', u'inmate', u'inner', u'innocence', u'innocent', u'innovative', u'insane', u'inside', u'insight', u'inspector', u'inspiration', u'inspired', u'inspiring', u'installment', u'instance', u'instant', u'instantly', u'instead', u'instinct', u'institution', u'insult', u'insulting', u'insurance', u'integrity', u'intellectual', u'intelligence', u'intelligent', u'intended', u'intense', u'intensity', u'intent', u'intention', u'intentionally', u'interaction', u'interest', u'interested', u'interesting', u'interior', u'international', u'internet', u'interpretation', u'interview', u'intimate', u'intrigue', u'intrigued', u'intriguing', u'introduce', u'introduced', u'introduces', u'introduction', u'invasion', u'invented', u'inventive', u'investigate', u'investigation', u'invisible', u'invite', u'invited', u'involve', u'involved', u'involvement', u'involves', u'involving', u'iran', u'iraq', u'ireland', u'irish', u'iron', u'ironic', u'ironically', u'irony', u'irrelevant', u'irritating', u'ish', u'island', u'isolated', u'israel', u'issue', u'italian', u'italy', u'item', u'iv', u'jack', u'jacket', u'jackie', u'jackson', u'jail', u'jake', u'james', u'jamie', u'jane', u'japan', u'japanese', u'jason', u'jaw', u'jay', u'jazz', u'jealous', u'jean', u'jeff', u'jeffrey', u'jennifer', u'jenny', u'jeremy', u'jerk', u'jerry', u'jess', u'jesse', u'jessica', u'jesus', u'jet', u'jew', u'jewel', u'jewish', u'jim', u'jimmy', u'joan', u'job', u'joe', u'joel', u'joey', u'john', u'johnny', u'johnson', u'join', u'joined', u'joke', u'jon', u'jonathan', u'jones', u'joseph', u'josh', u'journalist', u'journey', u'joy', u'jr', u'judge', u'judging', u'judy', u'julia', u'julian', u'julie', u'jump', u'jumped', u'jumping', u'june', u'jungle', u'junior', u'junk', u'justice', u'justify', u'justin', u'juvenile', u'kane', u'kansa', u'kapoor', u'karen', u'karl', u'karloff', u'kate', u'kay', u'keaton', u'keep', u'keeping', u'keith', u'kelly', u'ken', u'kennedy', u'kenneth', u'kept', u'kevin', u'key', u'khan', u'kick', u'kicked', u'kicking', u'kid', u'kidding', u'kidman', u'kidnapped', u'kill', u'killed', u'killer', u'killing', u'kim', u'kind', u'kinda', u'king', u'kingdom', u'kirk', u'kiss', u'kissing', u'kitchen', u'kitty', u'kline', u'knew', u'knife', u'knight', u'knock', u'knocked', u'know', u'knowing', u'knowledge', u'known', u'kolchak', u'kong', u'korean', u'kubrick', u'kudos', u'kumar', u'kung', u'kurosawa', u'kurt', u'kyle', u'la', u'lab', u'label', u'labor', u'lack', u'lacked', u'lacking', u'lackluster', u'ladder', u'lady', u'laid', u'lake', u'lame', u'land', u'landing', u'landscape', u'lane', u'language', u'large', u'largely', u'larger', u'larry', u'last', u'lasted', u'late', u'lately', u'later', u'latest', u'latin', u'latter', u'laugh', u'laughable', u'laughably', u'laughed', u'laughing', u'laughter', u'laura', u'laurel', u'laurence', u'law', u'lawrence', u'lawyer', u'lay', u'layer', u'lazy', u'le', u'lead', u'leader', u'leading', u'leaf', u'league', u'lean', u'leap', u'learn', u'learned', u'learning', u'learns', u'least', u'leave', u'leaving', u'led', u'lee', u'left', u'leg', u'legacy', u'legal', u'legend', u'legendary', u'leigh', u'lemmon', u'lena', u'length', u'lengthy', u'leo', u'leon', u'leonard', u'lesbian', u'leslie', u'lesser', u'lesson', u'lester', u'let', u'letter', u'letting', u'level', u'lewis', u'li', u'liberal', u'liberty', u'library', u'lie', u'life', u'lifestyle', u'lifetime', u'lift', u'light', u'lighting', u'likable', u'like', u'liked', u'likely', u'likewise', u'liking', u'lily', u'limit', u'limited', u'lincoln', u'linda', u'lindsay', u'line', u'liner', u'link', u'lion', u'lip', u'lisa', u'list', u'listed', u'listen', u'listening', u'lit', u'literally', u'literary', u'literature', u'little', u'live', u'lived', u'lively', u'living', u'lloyd', u'load', u'loaded', u'local', u'locale', u'location', u'lock', u'locked', u'logan', u'logic', u'logical', u'lois', u'lol', u'london', u'lone', u'loneliness', u'lonely', u'long', u'longer', u'look', u'looked', u'looking', u'loose', u'loosely', u'lord', u'los', u'lose', u'loser', u'loses', u'losing', u'loss', u'lost', u'lot', u'lou', u'loud', u'louis', u'louise', u'lousy', u'lovable', u'love', u'loved', u'lovely', u'lover', u'loving', u'low', u'lower', u'lowest', u'loyal', u'loyalty', u'lucas', u'luck', u'luckily', u'lucky', u'lucy', u'ludicrous', u'lugosi', u'luke', u'lumet', u'lundgren', u'lush', u'lust', u'lying', u'lynch', u'lyric', u'macarthur', u'machine', u'macho', u'macy', u'mad', u'made', u'madness', u'madonna', u'mafia', u'magazine', u'maggie', u'magic', u'magical', u'magnificent', u'maid', u'mail', u'main', u'mainly', u'mainstream', u'maintain', u'major', u'majority', u'make', u'maker', u'makeup', u'making', u'male', u'mall', u'malone', u'man', u'manage', u'managed', u'manager', u'manages', u'manhattan', u'maniac', u'manipulative', u'mankind', u'mann', u'manner', u'mansion', u'many', u'map', u'mar', u'marc', u'march', u'margaret', u'maria', u'marie', u'marine', u'mario', u'marion', u'mark', u'market', u'marketing', u'marriage', u'married', u'marry', u'marshall', u'martha', u'martial', u'martian', u'martin', u'marty', u'marvel', u'marvelous', u'mary', u'mask', u'mass', u'massacre', u'massive', u'master', u'masterful', u'masterpiece', u'match', u'matched', u'mate', u'material', u'matrix', u'matt', u'matter', u'matthau', u'matthew', u'mature', u'max', u'may', u'maybe', u'mayhem', u'mayor', u'mccoy', u'mclaglen', u'mean', u'meaning', u'meaningful', u'meaningless', u'meant', u'meanwhile', u'measure', u'meat', u'mechanical', u'medical', u'mediocre', u'medium', u'meet', u'meeting', u'meg', u'mel', u'melodrama', u'melodramatic', u'melody', u'melting', u'member', u'memorable', u'memory', u'men', u'menace', u'menacing', u'mental', u'mentality', u'mentally', u'mention', u'mentioned', u'mentioning', u'mere', u'merely', u'merit', u'mermaid', u'meryl', u'mess', u'message', u'messed', u'met', u'metal', u'metaphor', u'method', u'mexican', u'mexico', u'mgm', u'michael', u'michelle', u'mickey', u'mid', u'middle', u'midnight', u'might', u'mighty', u'miike', u'mike', u'mild', u'mildly', u'mildred', u'mile', u'military', u'milk', u'mill', u'miller', u'million', u'millionaire', u'min', u'mind', u'minded', u'mindless', u'mine', u'mini', u'minimal', u'minimum', u'minister', u'minor', u'minority', u'minus', u'minute', u'miracle', u'mirror', u'miscast', u'miserable', u'miserably', u'misery', u'misleading', u'miss', u'missed', u'missile', u'missing', u'mission', u'mistake', u'mistaken', u'mistress', u'mitchell', u'mix', u'mixed', u'mixture', u'miyazaki', u'mm', u'mob', u'mode', u'model', u'modern', u'modesty', u'molly', u'mom', u'moment', u'mon', u'money', u'monk', u'monkey', u'monologue', u'monster', u'montage', u'montana', u'month', u'mood', u'moody', u'moon', u'moore', u'moral', u'morality', u'morbid', u'moreover', u'morgan', u'mormon', u'morning', u'moron', u'moronic', u'morris', u'mostly', u'mother', u'motif', u'motion', u'motivation', u'mountain', u'mouse', u'mouth', u'move', u'moved', u'movement', u'movie', u'moving', u'mr', u'mst', u'mtv', u'much', u'muddled', u'multi', u'multiple', u'mummy', u'mundane', u'muppet', u'murder', u'murdered', u'murderer', u'murdering', u'murderous', u'murphy', u'murray', u'museum', u'music', u'musical', u'musician', u'muslim', u'must', u'mutant', u'myers', u'mysterious', u'mystery', u'myth', u'nail', u'naive', u'naked', u'name', u'named', u'namely', u'nancy', u'narration', u'narrative', u'narrator', u'nasty', u'nathan', u'nation', u'national', u'native', u'natural', u'naturally', u'nature', u'naughty', u'navy', u'nazi', u'nd', u'near', u'nearby', u'nearly', u'neat', u'necessarily', u'necessary', u'neck', u'ned', u'need', u'needed', u'needle', u'negative', u'neighbor', u'neighborhood', u'neil', u'neither', u'nelson', u'nemesis', u'neo', u'nephew', u'nerd', u'nerve', u'nervous', u'net', u'network', u'never', u'nevertheless', u'new', u'newcomer', u'newly', u'newman', u'news', u'newspaper', u'next', u'nice', u'nicely', u'nicholas', u'nicholson', u'nick', u'nicole', u'niece', u'night', u'nightmare', u'nine', u'ninety', u'ninja', u'niro', u'noble', u'nobody', u'nod', u'noir', u'noise', u'nolan', u'nominated', u'nomination', u'non', u'none', u'nonetheless', u'nonsense', u'nonsensical', u'norm', u'normal', u'normally', u'norman', u'north', u'nose', u'nostalgia', u'nostalgic', u'notable', u'notably', u'notch', u'note', u'noted', u'nothing', u'notice', u'noticed', u'notion', u'notorious', u'novak', u'novel', u'nowadays', u'nowhere', u'nuance', u'nuclear', u'nude', u'nudity', u'number', u'numerous', u'nun', u'nurse', u'nut', u'nyc', u'object', u'objective', u'obnoxious', u'obscure', u'observation', u'obsessed', u'obsession', u'obstacle', u'obvious', u'obviously', u'occasion', u'occasional', u'occasionally', u'occur', u'occurred', u'occurs', u'ocean', u'odd', u'oddly', u'odds', u'offended', u'offensive', u'offer', u'offered', u'offering', u'office', u'officer', u'official', u'often', u'oh', u'oil', u'ok', u'okay', u'old', u'older', u'oliver', u'olivier', u'ollie', u'omen', u'one', u'online', u'onto', u'open', u'opened', u'opening', u'opera', u'operation', u'opinion', u'opportunity', u'opposed', u'opposite', u'option', u'orange', u'order', u'ordered', u'ordinary', u'origin', u'original', u'originality', u'originally', u'orleans', u'orson', u'oscar', u'othello', u'others', u'otherwise', u'ought', u'out', u'outcome', u'outer', u'outfit', u'outing', u'outrageous', u'outside', u'outstanding', u'overacting', u'overall', u'overcome', u'overdone', u'overlook', u'overlooked', u'overly', u'overrated', u'overwhelming', u'owen', u'owner', u'oz', u'pace', u'paced', u'pacing', u'pacino', u'pack', u'package', u'packed', u'page', u'paid', u'pain', u'painful', u'painfully', u'paint', u'painted', u'painter', u'painting', u'pair', u'pal', u'palace', u'palance', u'pale', u'palma', u'paltrow', u'pamela', u'pan', u'panic', u'pant', u'paper', u'par', u'parade', u'paradise', u'parallel', u'paramount', u'paranoia', u'parent', u'paris', u'park', u'parker', u'parody', u'part', u'particular', u'particularly', u'partly', u'partner', u'party', u'pas', u'pass', u'passable', u'passage', u'passed', u'passenger', u'passing', u'passion', u'passionate', u'past', u'pat', u'path', u'pathetic', u'patience', u'patient', u'patricia', u'patrick', u'pattern', u'paul', u'paulie', u'pause', u'paxton', u'pay', u'paying', u'peace', u'peak', u'pearl', u'peck', u'penn', u'penny', u'people', u'per', u'perception', u'perfect', u'perfection', u'perfectly', u'perform', u'performance', u'performed', u'performer', u'performing', u'performs', u'perhaps', u'period', u'perry', u'person', u'persona', u'personal', u'personality', u'personally', u'perspective', u'pet', u'pete', u'peter', u'petty', u'pg', u'phantom', u'phenomenon', u'phil', u'philip', u'philosophical', u'philosophy', u'phone', u'phony', u'photo', u'photograph', u'photographed', u'photographer', u'photography', u'phrase', u'physical', u'physically', u'piano', u'pick', u'picked', u'picking', u'picture', u'pie', u'piece', u'pierce', u'pig', u'pile', u'pilot', u'pin', u'pink', u'pirate', u'pit', u'pitch', u'pitiful', u'pitt', u'pity', u'place', u'placed', u'plague', u'plain', u'plan', u'plane', u'planet', u'planned', u'planning', u'plant', u'plastic', u'plausible', u'play', u'played', u'player', u'playing', u'pleasant', u'pleasantly', u'please', u'pleased', u'pleasure', u'plenty', u'plight', u'plot', u'plus', u'pocket', u'poe', u'poem', u'poetic', u'poetry', u'poignant', u'point', u'pointed', u'pointless', u'poison', u'pokemon', u'polanski', u'pole', u'police', u'policeman', u'policy', u'polished', u'political', u'politically', u'politician', u'politics', u'pool', u'poor', u'poorly', u'pop', u'popcorn', u'popular', u'popularity', u'population', u'porn', u'porno', u'portion', u'portrait', u'portray', u'portrayal', u'portrayed', u'portraying', u'portrays', u'pose', u'posey', u'position', u'positive', u'positively', u'posse', u'possessed', u'possibility', u'possible', u'possibly', u'post', u'poster', u'pot', u'potential', u'potentially', u'pound', u'poverty', u'powell', u'power', u'powerful', u'practically', u'practice', u'praise', u'praised', u'prank', u'pre', u'precious', u'precisely', u'predator', u'predecessor', u'predictable', u'prefer', u'pregnant', u'prejudice', u'premiere', u'preminger', u'premise', u'prepare', u'prepared', u'preposterous', u'prequel', u'presence', u'present', u'presentation', u'presented', u'presenting', u'president', u'press', u'pressure', u'preston', u'presumably', u'pretend', u'pretending', u'pretentious', u'pretty', u'prevent', u'preview', u'previous', u'previously', u'prey', u'price', u'priceless', u'pride', u'priest', u'primarily', u'primary', u'prime', u'primitive', u'prince', u'princess', u'principal', u'principle', u'print', u'prior', u'prison', u'prisoner', u'private', u'prize', u'pro', u'probably', u'problem', u'proceeding', u'proceeds', u'process', u'produce', u'produced', u'producer', u'producing', u'product', u'production', u'prof', u'profanity', u'professional', u'professor', u'profound', u'program', u'progress', u'project', u'prom', u'promise', u'promised', u'promising', u'proof', u'prop', u'propaganda', u'proper', u'properly', u'property', u'prostitute', u'protagonist', u'protect', u'proud', u'prove', u'proved', u'provide', u'provided', u'provides', u'providing', u'provocative', u'provoking', u'pseudo', u'psychiatrist', u'psychic', u'psycho', u'psychological', u'psychology', u'psychopath', u'psychotic', u'public', u'pull', u'pulled', u'pulling', u'pulp', u'pun', u'punch', u'punishment', u'punk', u'puppet', u'puppy', u'purchase', u'purchased', u'pure', u'purely', u'purple', u'purpose', u'pursuit', u'push', u'pushed', u'pushing', u'put', u'putting', u'puzzle', u'quaid', u'quality', u'quarter', u'queen', u'quest', u'question', u'questionable', u'quick', u'quickly', u'quiet', u'quietly', u'quinn', u'quirky', u'quit', u'quite', u'quote', u'rabbit', u'race', u'rachel', u'racial', u'racism', u'racist', u'radical', u'radio', u'rage', u'rain', u'raines', u'raise', u'raised', u'raising', u'ralph', u'rambo', u'ramones', u'ran', u'random', u'randomly', u'randy', u'range', u'ranger', u'rank', u'rap', u'rape', u'raped', u'rapist', u'rare', u'rarely', u'rat', u'rate', u'rated', u'rather', u'rating', u'ratso', u'rave', u'raw', u'ray', u'raymond', u'rd', u'reach', u'reached', u'reaching', u'react', u'reaction', u'read', u'reader', u'reading', u'ready', u'real', u'realise', u'realised', u'realism', u'realistic', u'reality', u'realize', u'realized', u'realizes', u'realizing', u'really', u'realm', u'rear', u'reason', u'reasonable', u'reasonably', u'rebel', u'recall', u'receive', u'received', u'receives', u'recent', u'recently', u'recognition', u'recognize', u'recognized', u'recommend', u'recommendation', u'recommended', u'record', u'recorded', u'recording', u'red', u'redeeming', u'redemption', u'reduced', u'reed', u'reef', u'reel', u'refer', u'reference', u'referred', u'reflect', u'reflection', u'reflects', u'refreshing', u'refuse', u'refused', u'regard', u'regarded', u'regarding', u'regardless', u'region', u'regret', u'regular', u'reid', u'reign', u'reject', u'rejected', u'relate', u'related', u'relation', u'relationship', u'relative', u'relatively', u'relax', u'release', u'released', u'relevant', u'relief', u'relies', u'religion', u'religious', u'reluctant', u'rely', u'remain', u'remained', u'remaining', u'remains', u'remake', u'remark', u'remarkable', u'remarkably', u'remember', u'remembered', u'remind', u'reminded', u'reminds', u'reminiscent', u'remote', u'remotely', u'remove', u'removed', u'renaissance', u'rendition', u'rent', u'rental', u'rented', u'renting', u'repeat', u'repeated', u'repeatedly', u'repetitive', u'replace', u'replaced', u'reply', u'report', u'reporter', u'represent', u'representation', u'represented', u'represents', u'reputation', u'require', u'required', u'requires', u'rescue', u'research', u'resemblance', u'resemble', u'resembles', u'resident', u'resist', u'resolution', u'resort', u'resource', u'respect', u'respected', u'respective', u'respectively', u'response', u'responsibility', u'responsible', u'rest', u'restaurant', u'restored', u'result', u'resulting', u'retarded', u'retired', u'return', u'returned', u'returning', u'reunion', u'reveal', u'revealed', u'revealing', u'reveals', u'revelation', u'revenge', u'review', u'reviewer', u'revolution', u'revolutionary', u'revolves', u'reward', u'rex', u'reynolds', u'rhythm', u'rich', u'richard', u'richards', u'richardson', u'rick', u'rid', u'ridden', u'ride', u'rider', u'ridiculous', u'ridiculously', u'riding', u'rifle', u'right', u'ring', u'riot', u'rip', u'ripped', u'rise', u'rising', u'risk', u'rita', u'ritter', u'ritual', u'rival', u'river', u'riveting', u'road', u'rob', u'robber', u'robbery', u'robbins', u'robert', u'robin', u'robinson', u'robot', u'rochester', u'rock', u'rocket', u'rocky', u'rod', u'roger', u'rogers', u'role', u'roll', u'rolled', u'roller', u'rolling', u'roman', u'romance', u'romantic', u'rome', u'romero', u'romp', u'ron', u'roof', u'room', u'roommate', u'rooney', u'root', u'rope', u'rose', u'rosemary', u'ross', u'roth', u'rotten', u'rough', u'round', u'route', u'routine', u'row', u'roy', u'royal', u'rubber', u'rubbish', u'ruby', u'rude', u'ruin', u'ruined', u'rukh', u'rule', u'run', u'runner', u'running', u'rural', u'rush', u'rushed', u'russell', u'russia', u'russian', u'ruth', u'ruthless', u'ryan', u'sabrina', u'sacrifice', u'sad', u'sadistic', u'sadly', u'sadness', u'safe', u'safety', u'saga', u'said', u'sailor', u'saint', u'sake', u'sale', u'sally', u'sam', u'samurai', u'san', u'sand', u'sandler', u'sandra', u'santa', u'sappy', u'sarah', u'sat', u'satan', u'satire', u'satisfied', u'satisfy', u'satisfying', u'saturday', u'savage', u'save', u'saved', u'saving', u'saw', u'say', u'saying', u'scale', u'scare', u'scarecrow', u'scared', u'scary', u'scenario', u'scene', u'scenery', u'scheme', u'school', u'sci', u'science', u'scientific', u'scientist', u'scooby', u'scope', u'score', u'scotland', u'scott', u'scottish', u'scratch', u'scream', u'screaming', u'screen', u'screening', u'screenplay', u'screenwriter', u'screw', u'script', u'scripted', u'scrooge', u'sea', u'seagal', u'sean', u'search', u'searching', u'season', u'seat', u'second', u'secondary', u'secondly', u'secret', u'secretary', u'secretly', u'section', u'security', u'see', u'seed', u'seeing', u'seek', u'seeking', u'seem', u'seemed', u'seemingly', u'seems', u'seen', u'segment', u'seldom', u'selection', u'self', u'selfish', u'sell', u'seller', u'selling', u'semi', u'send', u'sends', u'sens', u'sense', u'senseless', u'sensibility', u'sensitive', u'sent', u'sentence', u'sentiment', u'sentimental', u'sentinel', u'separate', u'september', u'sequel', u'sequence', u'serf', u'sergeant', u'serial', u'series', u'serious', u'seriously', u'servant', u'serve', u'served', u'service', u'serving', u'session', u'set', u'setting', u'settle', u'setup', u'seven', u'seventy', u'several', u'severe', u'severely', u'sex', u'sexual', u'sexuality', u'sexually', u'sexy', u'sh', u'shade', u'shadow', u'shake', u'shakespeare', u'shaky', u'shall', u'shallow', u'shame', u'shanghai', u'shape', u'share', u'shared', u'shark', u'sharon', u'sharp', u'shaw', u'shed', u'sheer', u'sheet', u'shelf', u'shell', u'shelley', u'sheriff', u'shift', u'shine', u'shining', u'ship', u'shirley', u'shirt', u'shock', u'shocked', u'shocking', u'shoddy', u'shoe', u'shoot', u'shooting', u'shootout', u'shop', u'short', u'shortcoming', u'shortly', u'shot', u'shoulder', u'show', u'showcase', u'showdown', u'showed', u'shower', u'showing', u'shown', u'shut', u'shy', u'sibling', u'sick', u'sid', u'side', u'sidekick', u'sidney', u'sight', u'sign', u'signed', u'significance', u'significant', u'silence', u'silent', u'silliness', u'silly', u'silver', u'similar', u'similarity', u'similarly', u'simmons', u'simon', u'simple', u'simplicity', u'simplistic', u'simply', u'simpson', u'simultaneously', u'sin', u'sinatra', u'since', u'sincere', u'sing', u'singer', u'singing', u'single', u'sings', u'sinister', u'sink', u'sir', u'sirk', u'sissy', u'sister', u'sit', u'sitcom', u'site', u'sits', u'sitting', u'situation', u'six', u'sixty', u'size', u'sketch', u'skill', u'skin', u'skip', u'skit', u'skull', u'sky', u'slap', u'slapstick', u'slasher', u'slaughter', u'slave', u'sleazy', u'sleep', u'sleeping', u'slice', u'slick', u'slide', u'slight', u'slightest', u'slightly', u'slip', u'sloppy', u'slow', u'slowly', u'slug', u'small', u'smaller', u'smart', u'smash', u'smile', u'smiling', u'smith', u'smoke', u'smoking', u'smooth', u'snake', u'sneak', u'sniper', u'snl', u'snow', u'soap', u'soccer', u'social', u'society', u'soderbergh', u'soft', u'sold', u'soldier', u'sole', u'solely', u'solid', u'solo', u'solution', u'solve', u'somebody', u'somehow', u'someone', u'something', u'sometimes', u'somewhat', u'somewhere', u'son', u'song', u'soon', u'sooner', u'sophisticated', u'soprano', u'sorry', u'sort', u'soul', u'sound', u'sounded', u'sounding', u'soundtrack', u'soup', u'source', u'south', u'southern', u'soviet', u'space', u'spacey', u'spain', u'span', u'spanish', u'spare', u'spark', u'speak', u'speaking', u'speaks', u'special', u'specially', u'specie', u'specific', u'specifically', u'spectacular', u'speech', u'speed', u'spell', u'spend', u'spending', u'spends', u'spent', u'spider', u'spielberg', u'spike', u'spin', u'spirit', u'spirited', u'spiritual', u'spit', u'spite', u'splatter', u'splendid', u'split', u'spock', u'spoil', u'spoiled', u'spoiler', u'spoke', u'spoken', u'spoof', u'spooky', u'sport', u'spot', u'spread', u'spring', u'springer', u'spy', u'squad', u'square', u'st', u'stab', u'stack', u'staff', u'stage', u'staged', u'stake', u'stale', u'stalker', u'stallone', u'stan', u'stand', u'standard', u'standing', u'stanley', u'stanwyck', u'star', u'stare', u'stargate', u'staring', u'stark', u'starred', u'starring', u'start', u'started', u'starting', u'state', u'stated', u'statement', u'station', u'statue', u'status', u'stay', u'stayed', u'staying', u'steady', u'steal', u'stealing', u'steel', u'stellar', u'step', u'stephen', u'stereotype', u'stereotypical', u'steve', u'steven', u'stevens', u'stewart', u'stick', u'stiff', u'still', u'stiller', u'stilted', u'stink', u'stinker', u'stock', u'stole', u'stolen', u'stomach', u'stone', u'stood', u'stooge', u'stop', u'stopped', u'store', u'storm', u'story', u'storyline', u'storytelling', u'straight', u'straightforward', u'strange', u'strangely', u'stranger', u'streep', u'street', u'streisand', u'strength', u'stress', u'stretch', u'stretched', u'strictly', u'strike', u'striking', u'string', u'strip', u'stroke', u'strong', u'stronger', u'strongly', u'struck', u'structure', u'struggle', u'struggling', u'stuart', u'stuck', u'student', u'studio', u'study', u'stuff', u'stumble', u'stumbled', u'stunned', u'stunning', u'stunt', u'stupid', u'stupidity', u'style', u'stylish', u'sub', u'subject', u'subjected', u'subplot', u'subplots', u'subsequent', u'substance', u'subtitle', u'subtle', u'subtlety', u'succeed', u'succeeded', u'succeeds', u'success', u'successful', u'successfully', u'suck', u'sucked', u'sudden', u'suddenly', u'sue', u'suffer', u'suffered', u'suffering', u'suffers', u'suffice', u'sugar', u'suggest', u'suggested', u'suggestion', u'suggests', u'suicide', u'suit', u'suitable', u'suited', u'sullivan', u'sum', u'summary', u'summer', u'sun', u'sunday', u'sung', u'sunny', u'sunshine', u'super', u'superb', u'superbly', u'superficial', u'superhero', u'superior', u'superman', u'supernatural', u'supply', u'support', u'supported', u'supporting', u'suppose', u'supposed', u'supposedly', u'sure', u'surely', u'surface', u'surfing', u'surprise', u'surprised', u'surprising', u'surprisingly', u'surreal', u'surround', u'surrounded', u'surrounding', u'survival', u'survive', u'survived', u'surviving', u'survivor', u'susan', u'suspect', u'suspend', u'suspense', u'suspenseful', u'suspension', u'suspicion', u'suspicious', u'sutherland', u'swear', u'swedish', u'sweet', u'swim', u'swimming', u'swing', u'switch', u'sword', u'symbol', u'symbolism', u'sympathetic', u'sympathize', u'sympathy', u'synopsis', u'system', u'table', u'taboo', u'tacky', u'tad', u'tag', u'tail', u'take', u'taken', u'taking', u'tale', u'talent', u'talented', u'talk', u'talked', u'talking', u'tall', u'tame', u'tank', u'tap', u'tape', u'tarantino', u'target', u'tarzan', u'task', u'taste', u'tasteless', u'taught', u'tax', u'taxi', u'taylor', u'te', u'tea', u'teach', u'teacher', u'teaching', u'team', u'tear', u'tech', u'technical', u'technically', u'technicolor', u'technique', u'technology', u'ted', u'tedious', u'teen', u'teenage', u'teenager', u'teeth', u'television', u'tell', u'telling', u'temple', u'ten', u'tenant', u'tend', u'tendency', u'tender', u'tends', u'tense', u'tension', u'term', u'terrible', u'terribly', u'terrific', u'terrifying', u'territory', u'terror', u'terrorist', u'terry', u'test', u'testament', u'texas', u'text', u'th', u'thank', u'thankfully', u'thanks', u'thats', u'theater', u'theatre', u'theatrical', u'theme', u'theory', u'therefore', u'thick', u'thief', u'thin', u'thing', u'think', u'thinking', u'third', u'thirty', u'thomas', u'thompson', u'thoroughly', u'though', u'thought', u'thoughtful', u'thousand', u'thread', u'threat', u'threatening', u'three', u'threw', u'thrill', u'thriller', u'thrilling', u'throat', u'throughout', u'throw', u'throwing', u'thrown', u'thru', u'thug', u'thumb', u'thunderbird', u'thus', u'ticket', u'tie', u'tied', u'tierney', u'tiger', u'tight', u'till', u'tim', u'time', u'timeless', u'timing', u'timon', u'timothy', u'tiny', u'tip', u'tired', u'tiresome', u'titanic', u'title', u'titled', u'today', u'todd', u'together', u'toilet', u'told', u'tom', u'tomato', u'tommy', u'tomorrow', u'ton', u'tone', u'tongue', u'tonight', u'tony', u'took', u'tool', u'tooth', u'top', u'topic', u'topless', u'torn', u'torture', u'tortured', u'total', u'totally', u'touch', u'touched', u'touching', u'tough', u'tour', u'tourist', u'toward', u'towards', u'tower', u'town', u'toy', u'trace', u'track', u'tracking', u'tracy', u'trade', u'trademark', u'tradition', u'traditional', u'tragedy', u'tragic', u'trail', u'trailer', u'train', u'trained', u'training', u'trait', u'transfer', u'transformation', u'transition', u'translated', u'translation', u'trap', u'trapped', u'trash', u'trashy', u'travel', u'traveling', u'travesty', u'treasure', u'treat', u'treated', u'treatment', u'tree', u'trek', u'tremendous', u'trend', u'trial', u'tribe', u'tribute', u'trick', u'tried', u'trilogy', u'trio', u'trip', u'tripe', u'trite', u'triumph', u'troop', u'trouble', u'troubled', u'truck', u'true', u'truly', u'trust', u'truth', u'try', u'trying', u'tube', u'tune', u'tunnel', u'turkey', u'turn', u'turned', u'turner', u'turning', u'tv', u'twelve', u'twenty', u'twice', u'twilight', u'twin', u'twist', u'twisted', u'two', u'tyler', u'type', u'typical', u'typically', u'ugly', u'uk', u'ultimate', u'ultimately', u'ultra', u'un', u'unable', u'unaware', u'unbearable', u'unbelievable', u'unbelievably', u'uncle', u'uncomfortable', u'unconvincing', u'uncut', u'underground', u'underlying', u'underneath', u'underrated', u'understand', u'understandable', u'understanding', u'understated', u'understood', u'underworld', u'undoubtedly', u'uneven', u'unexpected', u'unexpectedly', u'unfair', u'unfold', u'unfolds', u'unforgettable', u'unfortunate', u'unfortunately', u'unfunny', u'unhappy', u'uniform', u'uninspired', u'unintentional', u'unintentionally', u'uninteresting', u'union', u'unique', u'unit', u'united', u'universal', u'universe', u'university', u'unknown', u'unless', u'unlikable', u'unlike', u'unlikely', u'unnecessary', u'unoriginal', u'unpleasant', u'unpredictable', u'unreal', u'unrealistic', u'unseen', u'unsettling', u'unusual', u'unwatchable', u'uplifting', u'upon', u'upper', u'ups', u'upset', u'urban', u'urge', u'us', u'usa', u'use', u'used', u'useful', u'useless', u'user', u'using', u'ustinov', u'usual', u'usually', u'utter', u'utterly', u'uwe', u'vacation', u'vader', u'vague', u'vaguely', u'valentine', u'valley', u'valuable', u'value', u'vampire', u'van', u'vance', u'variation', u'variety', u'various', u'vast', u'vega', u'vehicle', u'vein', u'vengeance', u'venture', u'verhoeven', u'version', u'versus', u'vet', u'veteran', u'vhs', u'via', u'vice', u'vicious', u'victim', u'victor', u'victoria', u'victory', u'video', u'vietnam', u'view', u'viewed', u'viewer', u'viewing', u'viewpoint', u'village', u'villain', u'vincent', u'violence', u'violent', u'virgin', u'virginia', u'virtually', u'virtue', u'virus', u'visible', u'vision', u'visit', u'visited', u'visiting', u'visitor', u'visual', u'visually', u'visuals', u'vivid', u'vocal', u'voice', u'voiced', u'voight', u'volume', u'von', u'vote', u'vulgar', u'vulnerable', u'wacky', u'wagner', u'wait', u'waited', u'waiting', u'waitress', u'wake', u'walk', u'walked', u'walken', u'walker', u'walking', u'wall', u'wallace', u'walsh', u'walter', u'wandering', u'wang', u'wanna', u'wannabe', u'want', u'wanted', u'wanting', u'war', u'ward', u'warm', u'warming', u'warmth', u'warn', u'warned', u'warner', u'warning', u'warren', u'warrior', u'washington', u'waste', u'wasted', u'wasting', u'watch', u'watchable', u'watched', u'watcher', u'watching', u'water', u'watson', u'wave', u'wax', u'way', u'wayne', u'weak', u'weakest', u'weakness', u'wealth', u'wealthy', u'weapon', u'wear', u'wearing', u'web', u'website', u'wedding', u'week', u'weekend', u'weight', u'weird', u'welcome', u'well', u'welles', u'wendy', u'went', u'werewolf', u'wes', u'west', u'western', u'wet', u'whale', u'whatever', u'whats', u'whatsoever', u'wheel', u'whenever', u'whereas', u'whether', u'whilst', u'white', u'whoever', u'whole', u'wholly', u'whoopi', u'whose', u'wicked', u'wide', u'widely', u'widmark', u'widow', u'wife', u'wig', u'wild', u'wilder', u'wilderness', u'wildly', u'william', u'williams', u'willie', u'willing', u'willis', u'wilson', u'win', u'wind', u'window', u'wing', u'winner', u'winning', u'winter', u'wire', u'wisdom', u'wise', u'wish', u'wished', u'wishing', u'wit', u'witch', u'within', u'without', u'witness', u'witnessed', u'witty', u'wizard', u'wolf', u'woman', u'wonder', u'wondered', u'wonderful', u'wonderfully', u'wondering', u'wont', u'woo', u'wood', u'wooden', u'woody', u'word', u'wore', u'work', u'worked', u'worker', u'working', u'world', u'worm', u'worn', u'worried', u'worry', u'worse', u'worst', u'worth', u'worthless', u'worthwhile', u'worthy', u'would', u'wound', u'wounded', u'wow', u'wrap', u'wrapped', u'wreck', u'wrestling', u'wretched', u'write', u'writer', u'writes', u'writing', u'written', u'wrong', u'wrote', u'ww', u'wwe', u'wwii', u'ya', u'yard', u'yawn', u'yeah', u'year', u'yell', u'yelling', u'yellow', u'yes', u'yesterday', u'yet', u'yeti', u'york', u'young', u'younger', u'youngest', u'youth', u'zero', u'zizek', u'zombi', u'zombie', u'zone']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 abandoned\n",
      "125 abc\n",
      "562 ability\n",
      "1259 able\n",
      "92 abraham\n",
      "73 abrupt\n",
      "118 absence\n",
      "83 absent\n",
      "352 absolute\n",
      "1485 absolutely\n",
      "306 absurd\n",
      "86 absurdity\n",
      "202 abuse\n",
      "77 abused\n",
      "91 abusive\n",
      "98 abysmal\n",
      "298 academy\n",
      "688 accent\n",
      "300 accept\n",
      "130 acceptable\n",
      "144 accepted\n",
      "74 accepts\n",
      "92 access\n",
      "337 accident\n",
      "200 accidentally\n",
      "75 acclaimed\n",
      "88 accompanied\n",
      "77 accomplish\n",
      "124 accomplished\n",
      "295 according\n",
      "243 account\n",
      "82 accuracy\n",
      "284 accurate\n",
      "123 accused\n",
      "73 ace\n",
      "179 achieve\n",
      "139 achieved\n",
      "169 achievement\n",
      "91 acid\n",
      "971 across\n",
      "1645 act\n",
      "658 acted\n",
      "6491 acting\n",
      "3665 action\n",
      "76 active\n",
      "146 activity\n",
      "6875 actor\n",
      "1588 actress\n",
      "793 actual\n",
      "4237 actually\n",
      "188 ad\n",
      "400 adam\n",
      "533 adaptation\n",
      "154 adapted\n",
      "1147 add\n",
      "439 added\n",
      "82 addict\n",
      "78 addicted\n",
      "72 addiction\n",
      "166 adding\n",
      "371 addition\n",
      "74 additional\n",
      "103 address\n",
      "113 adequate\n",
      "73 admirable\n",
      "124 admire\n",
      "621 admit\n",
      "134 admittedly\n",
      "94 adolescent\n",
      "75 adopted\n",
      "101 adorable\n",
      "886 adult\n",
      "140 advance\n",
      "90 advanced\n",
      "170 advantage\n",
      "714 adventure\n",
      "91 advertising\n",
      "262 advice\n",
      "90 advise\n",
      "417 affair\n",
      "155 affect\n",
      "113 affected\n",
      "105 affection\n",
      "104 afford\n",
      "126 aforementioned\n",
      "343 afraid\n",
      "212 africa\n",
      "283 african\n",
      "197 afternoon\n",
      "128 afterwards\n",
      "1370 age\n",
      "233 aged\n",
      "79 agency\n",
      "86 agenda\n",
      "455 agent\n",
      "111 aging\n",
      "1033 ago\n",
      "572 agree\n",
      "88 agreed\n",
      "96 agrees\n",
      "119 ah\n",
      "396 ahead\n",
      "202 aid\n",
      "112 aim\n",
      "120 aimed\n",
      "652 air\n",
      "146 aired\n",
      "105 airplane\n",
      "96 airport\n",
      "194 aka\n",
      "100 akshay\n",
      "379 al\n",
      "208 ala\n",
      "351 alan\n",
      "157 albeit\n",
      "265 albert\n",
      "116 album\n",
      "84 alcohol\n",
      "103 alcoholic\n",
      "81 alec\n",
      "95 alert\n",
      "231 alex\n",
      "122 alexander\n",
      "85 alfred\n",
      "199 alice\n",
      "79 alicia\n",
      "572 alien\n",
      "152 alike\n",
      "86 alison\n",
      "463 alive\n",
      "408 allen\n",
      "79 alley\n",
      "308 allow\n",
      "325 allowed\n",
      "128 allowing\n",
      "252 allows\n",
      "100 ally\n",
      "3139 almost\n",
      "1061 alone\n",
      "1776 along\n",
      "90 alongside\n",
      "1381 already\n",
      "185 alright\n",
      "9155 also\n",
      "100 alternate\n",
      "83 alternative\n",
      "2537 although\n",
      "114 altman\n",
      "112 altogether\n",
      "3239 always\n",
      "101 amanda\n",
      "246 amateur\n",
      "216 amateurish\n",
      "183 amazed\n",
      "1319 amazing\n",
      "174 amazingly\n",
      "77 amazon\n",
      "80 ambiguous\n",
      "118 ambition\n",
      "126 ambitious\n",
      "737 america\n",
      "2593 american\n",
      "92 amitabh\n",
      "783 among\n",
      "160 amongst\n",
      "585 amount\n",
      "78 amused\n",
      "78 amusement\n",
      "509 amusing\n",
      "104 amy\n",
      "92 analysis\n",
      "237 ancient\n",
      "225 anderson\n",
      "79 andre\n",
      "298 andrew\n",
      "318 andy\n",
      "391 angel\n",
      "85 angela\n",
      "101 angeles\n",
      "206 anger\n",
      "391 angle\n",
      "336 angry\n",
      "752 animal\n",
      "516 animated\n",
      "864 animation\n",
      "76 animator\n",
      "248 anime\n",
      "288 ann\n",
      "251 anna\n",
      "254 anne\n",
      "117 annie\n",
      "143 annoyed\n",
      "998 annoying\n",
      "4325 another\n",
      "537 answer\n",
      "99 ant\n",
      "263 anthony\n",
      "480 anti\n",
      "117 antic\n",
      "72 anticipation\n",
      "82 antonioni\n",
      "88 antwone\n",
      "310 anybody\n",
      "333 anymore\n",
      "2630 anyone\n",
      "2949 anything\n",
      "1117 anyway\n",
      "113 anyways\n",
      "304 anywhere\n",
      "622 apart\n",
      "372 apartment\n",
      "218 ape\n",
      "125 appalling\n",
      "309 apparent\n",
      "917 apparently\n",
      "506 appeal\n",
      "225 appealing\n",
      "619 appear\n",
      "590 appearance\n",
      "371 appeared\n",
      "141 appearing\n",
      "841 appears\n",
      "507 appreciate\n",
      "196 appreciated\n",
      "88 appreciation\n",
      "435 approach\n",
      "221 appropriate\n",
      "77 appropriately\n",
      "101 april\n",
      "77 arab\n",
      "95 arc\n",
      "453 area\n",
      "90 arguably\n",
      "111 argue\n",
      "174 argument\n",
      "327 arm\n",
      "95 armed\n",
      "474 army\n",
      "148 arnold\n",
      "3616 around\n",
      "126 arrested\n",
      "89 arrival\n",
      "114 arrive\n",
      "93 arrived\n",
      "162 arrives\n",
      "99 arrogant\n",
      "78 arrow\n",
      "1603 art\n",
      "374 arthur\n",
      "94 artificial\n",
      "513 artist\n",
      "339 artistic\n",
      "72 artsy\n",
      "270 as\n",
      "158 ashamed\n",
      "89 ashley\n",
      "260 asian\n",
      "486 aside\n",
      "648 ask\n",
      "295 asked\n",
      "228 asking\n",
      "329 asks\n",
      "213 asleep\n",
      "852 aspect\n",
      "76 aspiring\n",
      "115 assassin\n",
      "103 assault\n",
      "83 assigned\n",
      "167 assistant\n",
      "77 associate\n",
      "131 associated\n",
      "229 assume\n",
      "82 assumed\n",
      "132 astaire\n",
      "79 astonishing\n",
      "78 asylum\n",
      "111 atlantis\n",
      "741 atmosphere\n",
      "148 atmospheric\n",
      "197 atrocious\n",
      "76 atrocity\n",
      "132 attached\n",
      "606 attack\n",
      "158 attacked\n",
      "1632 attempt\n",
      "136 attempted\n",
      "163 attempting\n",
      "75 attenborough\n",
      "83 attend\n",
      "929 attention\n",
      "327 attitude\n",
      "95 attorney\n",
      "123 attracted\n",
      "159 attraction\n",
      "352 attractive\n",
      "2674 audience\n",
      "113 audio\n",
      "75 audition\n",
      "204 aunt\n",
      "92 austen\n",
      "82 austin\n",
      "126 australia\n",
      "223 australian\n",
      "166 authentic\n",
      "313 author\n",
      "167 authority\n",
      "75 automatically\n",
      "388 available\n",
      "723 average\n",
      "775 avoid\n",
      "102 avoided\n",
      "106 awake\n",
      "664 award\n",
      "277 aware\n",
      "2774 away\n",
      "102 awe\n",
      "484 awesome\n",
      "1724 awful\n",
      "84 awfully\n",
      "78 awhile\n",
      "248 awkward\n",
      "144 babe\n",
      "769 baby\n",
      "90 bacall\n",
      "5045 back\n",
      "140 backdrop\n",
      "720 background\n",
      "78 bacon\n",
      "9302 bad\n",
      "86 baddie\n",
      "662 badly\n",
      "179 bag\n",
      "159 baker\n",
      "130 bakshi\n",
      "175 balance\n",
      "83 baldwin\n",
      "383 ball\n",
      "97 ballet\n",
      "622 band\n",
      "125 bang\n",
      "306 bank\n",
      "108 banned\n",
      "452 bar\n",
      "243 barbara\n",
      "114 bare\n",
      "483 barely\n",
      "88 bargain\n",
      "85 barrel\n",
      "134 barry\n",
      "88 barrymore\n",
      "198 base\n",
      "217 baseball\n",
      "1430 based\n",
      "145 basement\n",
      "547 basic\n",
      "906 basically\n",
      "77 basinger\n",
      "169 basis\n",
      "88 basketball\n",
      "217 bat\n",
      "98 bath\n",
      "117 bathroom\n",
      "434 batman\n",
      "750 battle\n",
      "90 bay\n",
      "189 bbc\n",
      "213 beach\n",
      "391 bear\n",
      "205 beast\n",
      "465 beat\n",
      "124 beaten\n",
      "154 beating\n",
      "74 beatles\n",
      "113 beatty\n",
      "2177 beautiful\n",
      "436 beautifully\n",
      "679 beauty\n",
      "697 became\n",
      "1544 become\n",
      "1380 becomes\n",
      "348 becoming\n",
      "409 bed\n",
      "112 bedroom\n",
      "168 beer\n",
      "318 began\n",
      "1473 begin\n",
      "1426 beginning\n",
      "94 behave\n",
      "287 behavior\n",
      "76 behaviour\n",
      "1280 behind\n",
      "128 being\n",
      "94 bela\n",
      "508 belief\n",
      "711 believable\n",
      "2505 believe\n",
      "208 believed\n",
      "137 believing\n",
      "136 bell\n",
      "85 belong\n",
      "142 belongs\n",
      "178 beloved\n",
      "84 belt\n",
      "87 belushi\n",
      "617 ben\n",
      "104 beneath\n",
      "150 benefit\n",
      "97 bergman\n",
      "99 berlin\n",
      "75 bernard\n",
      "410 besides\n",
      "6419 best\n",
      "274 bet\n",
      "78 betrayal\n",
      "155 bette\n",
      "5739 better\n",
      "163 bettie\n",
      "89 betty\n",
      "76 beverly\n",
      "72 beware\n",
      "866 beyond\n",
      "75 biased\n",
      "139 bible\n",
      "3476 big\n",
      "268 bigger\n",
      "515 biggest\n",
      "85 bike\n",
      "89 biko\n",
      "671 bill\n",
      "79 billed\n",
      "377 billy\n",
      "118 bin\n",
      "99 biography\n",
      "229 bird\n",
      "199 birth\n",
      "140 birthday\n",
      "3349 bit\n",
      "85 bitch\n",
      "155 bite\n",
      "164 bitter\n",
      "499 bizarre\n",
      "2108 black\n",
      "112 blade\n",
      "195 blah\n",
      "167 blair\n",
      "110 blake\n",
      "331 blame\n",
      "273 bland\n",
      "152 blank\n",
      "95 blast\n",
      "104 blatant\n",
      "119 bleak\n",
      "144 blend\n",
      "101 blew\n",
      "267 blind\n",
      "124 blob\n",
      "133 block\n",
      "235 blockbuster\n",
      "99 blond\n",
      "291 blonde\n",
      "1186 blood\n",
      "302 bloody\n",
      "324 blow\n",
      "108 blowing\n",
      "184 blown\n",
      "531 blue\n",
      "94 blunt\n",
      "156 bo\n",
      "290 board\n",
      "87 boast\n",
      "280 boat\n",
      "271 bob\n",
      "142 bobby\n",
      "1190 body\n",
      "102 bold\n",
      "145 boll\n",
      "169 bollywood\n",
      "301 bomb\n",
      "376 bond\n",
      "175 bone\n",
      "104 bonus\n",
      "2926 book\n",
      "96 boom\n",
      "142 boot\n",
      "178 border\n",
      "210 bore\n",
      "530 bored\n",
      "141 boredom\n",
      "1809 boring\n",
      "385 born\n",
      "84 borrowed\n",
      "416 bos\n",
      "432 bother\n",
      "187 bothered\n",
      "109 bottle\n",
      "433 bottom\n",
      "458 bought\n",
      "200 bound\n",
      "190 bourne\n",
      "85 bow\n",
      "672 box\n",
      "88 boxer\n",
      "126 boxing\n",
      "2178 boy\n",
      "433 boyfriend\n",
      "79 boyle\n",
      "176 brad\n",
      "110 brady\n",
      "583 brain\n",
      "186 branagh\n",
      "137 brand\n",
      "158 brando\n",
      "207 brave\n",
      "116 brazil\n",
      "76 bread\n",
      "870 break\n",
      "231 breaking\n",
      "179 breast\n",
      "182 breath\n",
      "168 breathtaking\n",
      "84 breed\n",
      "84 brenda\n",
      "73 brendan\n",
      "352 brian\n",
      "185 bride\n",
      "224 bridge\n",
      "78 bridget\n",
      "397 brief\n",
      "135 briefly\n",
      "273 bright\n",
      "129 brilliance\n",
      "1195 brilliant\n",
      "248 brilliantly\n",
      "869 bring\n",
      "216 bringing\n",
      "630 brings\n",
      "76 brit\n",
      "153 britain\n",
      "898 british\n",
      "118 broad\n",
      "127 broadcast\n",
      "243 broadway\n",
      "151 broke\n",
      "278 broken\n",
      "73 bronson\n",
      "184 brook\n",
      "105 brooklyn\n",
      "128 brosnan\n",
      "1670 brother\n",
      "737 brought\n",
      "276 brown\n",
      "393 bruce\n",
      "303 brutal\n",
      "83 brutality\n",
      "90 brutally\n",
      "77 bubble\n",
      "299 buck\n",
      "93 bud\n",
      "367 buddy\n",
      "1894 budget\n",
      "200 buff\n",
      "89 buffalo\n",
      "271 bug\n",
      "432 build\n",
      "489 building\n",
      "236 built\n",
      "113 bull\n",
      "233 bullet\n",
      "108 bully\n",
      "85 bumbling\n",
      "817 bunch\n",
      "86 bunny\n",
      "126 buried\n",
      "310 burn\n",
      "114 burned\n",
      "146 burning\n",
      "105 burst\n",
      "167 burt\n",
      "153 burton\n",
      "203 bus\n",
      "150 bush\n",
      "639 business\n",
      "83 businessman\n",
      "84 buster\n",
      "162 busy\n",
      "88 butler\n",
      "147 butt\n",
      "138 button\n",
      "823 buy\n",
      "184 buying\n",
      "74 bye\n",
      "180 cabin\n",
      "285 cable\n",
      "309 cage\n",
      "124 cagney\n",
      "204 caine\n",
      "104 cake\n",
      "84 caliber\n",
      "189 california\n",
      "1183 call\n",
      "1433 called\n",
      "176 calling\n",
      "87 calm\n",
      "1673 came\n",
      "405 cameo\n",
      "1888 camera\n",
      "123 cameron\n",
      "495 camp\n",
      "77 campaign\n",
      "97 campbell\n",
      "179 campy\n",
      "139 canada\n",
      "242 canadian\n",
      "75 cancer\n",
      "97 candidate\n",
      "72 candle\n",
      "292 candy\n",
      "123 cannibal\n",
      "91 cannon\n",
      "1096 cannot\n",
      "202 cant\n",
      "80 canyon\n",
      "228 capable\n",
      "90 capital\n",
      "339 captain\n",
      "119 captivating\n",
      "503 capture\n",
      "261 captured\n",
      "88 capturing\n",
      "1504 car\n",
      "249 card\n",
      "132 cardboard\n",
      "1613 care\n",
      "121 cared\n",
      "1114 career\n",
      "90 careful\n",
      "134 carefully\n",
      "88 carell\n",
      "139 caricature\n",
      "166 caring\n",
      "109 carl\n",
      "85 carla\n",
      "139 carol\n",
      "167 carpenter\n",
      "94 carradine\n",
      "131 carrey\n",
      "117 carrie\n",
      "163 carried\n",
      "492 carry\n",
      "165 carrying\n",
      "151 carter\n",
      "749 cartoon\n",
      "108 cary\n",
      "1696 case\n",
      "238 cash\n",
      "85 cassidy\n",
      "3894 cast\n",
      "626 casting\n",
      "356 castle\n",
      "657 cat\n",
      "553 catch\n",
      "92 catching\n",
      "91 catchy\n",
      "252 category\n",
      "143 catherine\n",
      "168 catholic\n",
      "75 cattle\n",
      "555 caught\n",
      "700 cause\n",
      "237 caused\n",
      "107 causing\n",
      "124 cave\n",
      "109 cd\n",
      "80 celebration\n",
      "147 celebrity\n",
      "220 cell\n",
      "107 celluloid\n",
      "90 cent\n",
      "323 center\n",
      "109 centered\n",
      "411 central\n",
      "89 centre\n",
      "580 century\n",
      "764 certain\n",
      "1462 certainly\n",
      "96 cg\n",
      "325 cgi\n",
      "155 chain\n",
      "78 chainsaw\n",
      "181 chair\n",
      "238 challenge\n",
      "77 challenged\n",
      "88 challenging\n",
      "76 chamberlain\n",
      "89 champion\n",
      "85 championship\n",
      "207 chan\n",
      "1200 chance\n",
      "1345 change\n",
      "484 changed\n",
      "194 changing\n",
      "528 channel\n",
      "105 chaos\n",
      "152 chaplin\n",
      "123 chapter\n",
      "14177 character\n",
      "104 characteristic\n",
      "171 characterization\n",
      "213 charge\n",
      "76 charged\n",
      "138 charisma\n",
      "135 charismatic\n",
      "408 charles\n",
      "439 charlie\n",
      "98 charlotte\n",
      "475 charm\n",
      "471 charming\n",
      "581 chase\n",
      "98 chased\n",
      "145 chasing\n",
      "76 chavez\n",
      "217 che\n",
      "892 cheap\n",
      "92 cheated\n",
      "103 cheating\n",
      "792 check\n",
      "80 checked\n",
      "139 checking\n",
      "124 cheek\n",
      "112 cheer\n",
      "159 cheese\n",
      "633 cheesy\n",
      "490 chemistry\n",
      "96 chess\n",
      "99 chest\n",
      "92 chicago\n",
      "322 chick\n",
      "93 chicken\n",
      "237 chief\n",
      "2833 child\n",
      "360 childhood\n",
      "117 childish\n",
      "130 chill\n",
      "169 chilling\n",
      "188 china\n",
      "337 chinese\n",
      "699 choice\n",
      "227 choose\n",
      "85 chooses\n",
      "80 chop\n",
      "76 choppy\n",
      "99 choreographed\n",
      "116 choreography\n",
      "99 chorus\n",
      "202 chose\n",
      "232 chosen\n",
      "421 chris\n",
      "183 christ\n",
      "465 christian\n",
      "79 christianity\n",
      "73 christine\n",
      "629 christmas\n",
      "415 christopher\n",
      "97 christy\n",
      "75 chronicle\n",
      "146 chuck\n",
      "117 chuckle\n",
      "428 church\n",
      "124 cia\n",
      "79 cigarette\n",
      "236 cinderella\n",
      "1553 cinema\n",
      "412 cinematic\n",
      "109 cinematographer\n",
      "983 cinematography\n",
      "156 circle\n",
      "248 circumstance\n",
      "76 circus\n",
      "200 citizen\n",
      "1283 city\n",
      "140 civil\n",
      "100 civilization\n",
      "427 claim\n",
      "100 claimed\n",
      "173 claire\n",
      "75 clan\n",
      "207 clark\n",
      "76 clarke\n",
      "72 clash\n",
      "999 class\n",
      "2061 classic\n",
      "88 classical\n",
      "80 claus\n",
      "256 clean\n",
      "791 clear\n",
      "899 clearly\n",
      "80 clerk\n",
      "533 clever\n",
      "94 cleverly\n",
      "840 clich\n",
      "155 cliche\n",
      "127 cliff\n",
      "93 climactic\n",
      "439 climax\n",
      "90 climb\n",
      "110 clint\n",
      "245 clip\n",
      "108 clock\n",
      "81 clone\n",
      "1323 close\n",
      "92 closed\n",
      "140 closely\n",
      "206 closer\n",
      "94 closest\n",
      "120 closet\n",
      "177 closing\n",
      "327 clothes\n",
      "110 clothing\n",
      "77 cloud\n",
      "120 clown\n",
      "463 club\n",
      "347 clue\n",
      "105 clumsy\n",
      "635 co\n",
      "109 coach\n",
      "84 coast\n",
      "77 coaster\n",
      "75 coat\n",
      "260 code\n",
      "107 coffee\n",
      "109 coherent\n",
      "107 coincidence\n",
      "571 cold\n",
      "134 cole\n",
      "106 colleague\n",
      "359 collection\n",
      "509 college\n",
      "106 colonel\n",
      "601 color\n",
      "144 colorful\n",
      "198 colour\n",
      "208 columbo\n",
      "218 com\n",
      "111 combat\n",
      "239 combination\n",
      "151 combine\n",
      "198 combined\n",
      "5673 come\n",
      "236 comedian\n",
      "315 comedic\n",
      "3684 comedy\n",
      "99 comfort\n",
      "110 comfortable\n",
      "1034 comic\n",
      "171 comical\n",
      "1065 coming\n",
      "128 command\n",
      "73 commander\n",
      "1432 comment\n",
      "347 commentary\n",
      "100 commented\n",
      "355 commercial\n",
      "127 commit\n",
      "195 committed\n",
      "509 common\n",
      "126 communist\n",
      "317 community\n",
      "137 companion\n",
      "596 company\n",
      "355 compare\n",
      "538 compared\n",
      "96 comparing\n",
      "320 comparison\n",
      "81 compassion\n",
      "87 compelled\n",
      "385 compelling\n",
      "140 competent\n",
      "133 competition\n",
      "115 complain\n",
      "76 complaining\n",
      "216 complaint\n",
      "1035 complete\n",
      "1889 completely\n",
      "429 complex\n",
      "119 complexity\n",
      "178 complicated\n",
      "74 compliment\n",
      "111 composed\n",
      "100 composer\n",
      "84 composition\n",
      "534 computer\n",
      "241 con\n",
      "120 conceived\n",
      "92 concentrate\n",
      "586 concept\n",
      "237 concern\n",
      "269 concerned\n",
      "115 concerning\n",
      "160 concert\n",
      "534 conclusion\n",
      "223 condition\n",
      "81 confess\n",
      "95 confidence\n",
      "366 conflict\n",
      "90 confrontation\n",
      "377 confused\n",
      "368 confusing\n",
      "166 confusion\n",
      "117 connect\n",
      "147 connected\n",
      "328 connection\n",
      "94 connery\n",
      "76 conscience\n",
      "79 conscious\n",
      "173 consequence\n",
      "101 conservative\n",
      "513 consider\n",
      "97 considerable\n",
      "483 considered\n",
      "544 considering\n",
      "96 consistent\n",
      "104 consistently\n",
      "144 consists\n",
      "146 conspiracy\n",
      "292 constant\n",
      "416 constantly\n",
      "95 constructed\n",
      "92 construction\n",
      "182 contact\n",
      "156 contain\n",
      "113 contained\n",
      "411 contains\n",
      "236 contemporary\n",
      "392 content\n",
      "96 contest\n",
      "98 contestant\n",
      "264 context\n",
      "307 continue\n",
      "128 continued\n",
      "261 continues\n",
      "214 continuity\n",
      "138 contract\n",
      "116 contrary\n",
      "251 contrast\n",
      "86 contribution\n",
      "227 contrived\n",
      "557 control\n",
      "74 controlled\n",
      "152 controversial\n",
      "125 convention\n",
      "109 conventional\n",
      "298 conversation\n",
      "171 convey\n",
      "79 convict\n",
      "91 conviction\n",
      "197 convince\n",
      "216 convinced\n",
      "538 convincing\n",
      "93 convincingly\n",
      "121 convoluted\n",
      "174 cook\n",
      "971 cool\n",
      "167 cooper\n",
      "921 cop\n",
      "75 cope\n",
      "665 copy\n",
      "259 core\n",
      "181 corner\n",
      "258 corny\n",
      "96 corporate\n",
      "91 corporation\n",
      "73 corps\n",
      "127 corpse\n",
      "223 correct\n",
      "82 correctly\n",
      "133 corrupt\n",
      "100 corruption\n",
      "453 cost\n",
      "641 costume\n",
      "73 couch\n",
      "7922 could\n",
      "426 count\n",
      "94 counter\n",
      "76 counterpart\n",
      "136 countless\n",
      "1086 country\n",
      "103 countryside\n",
      "79 coup\n",
      "1812 couple\n",
      "148 courage\n",
      "2517 course\n",
      "208 court\n",
      "183 cousin\n",
      "661 cover\n",
      "212 covered\n",
      "243 cowboy\n",
      "132 cox\n",
      "150 crack\n",
      "79 cracking\n",
      "127 craft\n",
      "167 crafted\n",
      "122 craig\n",
      "1049 crap\n",
      "242 crappy\n",
      "262 crash\n",
      "123 craven\n",
      "83 crawford\n",
      "79 crazed\n",
      "670 crazy\n",
      "79 cream\n",
      "612 create\n",
      "542 created\n",
      "245 creates\n",
      "284 creating\n",
      "173 creation\n",
      "362 creative\n",
      "85 creativity\n",
      "219 creator\n",
      "554 creature\n",
      "153 credibility\n",
      "153 credible\n",
      "1204 credit\n",
      "76 credited\n",
      "125 creep\n",
      "638 creepy\n",
      "582 crew\n",
      "117 cried\n",
      "896 crime\n",
      "499 criminal\n",
      "107 cringe\n",
      "164 crisis\n",
      "522 critic\n",
      "175 critical\n",
      "215 criticism\n",
      "72 critique\n",
      "111 crocodile\n",
      "78 crook\n",
      "379 cross\n",
      "259 crowd\n",
      "86 crucial\n",
      "187 crude\n",
      "181 cruel\n",
      "74 cruelty\n",
      "134 cruise\n",
      "115 crush\n",
      "649 cry\n",
      "91 crystal\n",
      "109 cuba\n",
      "151 cube\n",
      "104 cue\n",
      "502 cult\n",
      "184 cultural\n",
      "550 culture\n",
      "134 cup\n",
      "107 cure\n",
      "124 curiosity\n",
      "262 curious\n",
      "76 curly\n",
      "266 current\n",
      "113 currently\n",
      "163 curse\n",
      "143 curtis\n",
      "137 cusack\n",
      "1309 cut\n",
      "581 cute\n",
      "211 cutting\n",
      "89 cyborg\n",
      "154 cynical\n",
      "96 da\n",
      "511 dad\n",
      "109 daddy\n",
      "168 daily\n",
      "89 daisy\n",
      "97 dalton\n",
      "115 damage\n",
      "84 damme\n",
      "359 damn\n",
      "89 damon\n",
      "264 dan\n",
      "83 dana\n",
      "847 dance\n",
      "265 dancer\n",
      "529 dancing\n",
      "72 dandy\n",
      "149 dane\n",
      "251 danger\n",
      "291 dangerous\n",
      "310 daniel\n",
      "330 danny\n",
      "168 dare\n",
      "114 daring\n",
      "1380 dark\n",
      "117 darker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 darkness\n",
      "73 darn\n",
      "83 darren\n",
      "507 date\n",
      "267 dated\n",
      "107 dating\n",
      "1312 daughter\n",
      "117 dave\n",
      "1025 david\n",
      "354 davis\n",
      "140 davy\n",
      "147 dawn\n",
      "133 dawson\n",
      "4013 day\n",
      "792 de\n",
      "1881 dead\n",
      "201 deadly\n",
      "82 deaf\n",
      "975 deal\n",
      "99 dealer\n",
      "284 dealing\n",
      "137 dealt\n",
      "189 dean\n",
      "147 dear\n",
      "2113 death\n",
      "78 debate\n",
      "81 debt\n",
      "266 debut\n",
      "432 decade\n",
      "82 deceased\n",
      "1157 decent\n",
      "482 decide\n",
      "705 decided\n",
      "546 decides\n",
      "345 decision\n",
      "99 dedicated\n",
      "82 dee\n",
      "97 deed\n",
      "654 deep\n",
      "177 deeper\n",
      "320 deeply\n",
      "127 defeat\n",
      "95 defend\n",
      "92 defense\n",
      "87 defined\n",
      "117 definite\n",
      "1580 definitely\n",
      "90 definition\n",
      "256 degree\n",
      "97 del\n",
      "72 deleted\n",
      "108 deliberately\n",
      "179 delight\n",
      "274 delightful\n",
      "328 deliver\n",
      "243 delivered\n",
      "94 delivering\n",
      "356 delivers\n",
      "188 delivery\n",
      "203 demand\n",
      "89 demented\n",
      "83 demise\n",
      "375 demon\n",
      "82 deniro\n",
      "176 dennis\n",
      "168 dentist\n",
      "141 denzel\n",
      "223 department\n",
      "210 depicted\n",
      "82 depicting\n",
      "222 depiction\n",
      "90 depicts\n",
      "99 depressed\n",
      "226 depressing\n",
      "129 depression\n",
      "570 depth\n",
      "86 der\n",
      "78 deranged\n",
      "151 derek\n",
      "91 descent\n",
      "330 describe\n",
      "233 described\n",
      "102 describes\n",
      "195 description\n",
      "272 desert\n",
      "73 deserted\n",
      "287 deserve\n",
      "291 deserved\n",
      "591 deserves\n",
      "433 design\n",
      "206 designed\n",
      "90 designer\n",
      "382 desire\n",
      "91 desired\n",
      "87 despair\n",
      "323 desperate\n",
      "179 desperately\n",
      "105 desperation\n",
      "1364 despite\n",
      "113 destiny\n",
      "219 destroy\n",
      "178 destroyed\n",
      "94 destroying\n",
      "142 destruction\n",
      "755 detail\n",
      "108 detailed\n",
      "526 detective\n",
      "164 determined\n",
      "252 develop\n",
      "404 developed\n",
      "102 developing\n",
      "679 development\n",
      "144 develops\n",
      "256 device\n",
      "342 devil\n",
      "104 devoid\n",
      "109 devoted\n",
      "911 dialog\n",
      "1652 dialogue\n",
      "153 diamond\n",
      "101 diana\n",
      "103 diane\n",
      "74 diary\n",
      "298 dick\n",
      "94 dickens\n",
      "794 die\n",
      "512 died\n",
      "491 difference\n",
      "2385 different\n",
      "695 difficult\n",
      "148 difficulty\n",
      "170 dig\n",
      "126 digital\n",
      "120 dignity\n",
      "72 dilemma\n",
      "110 dimension\n",
      "255 dimensional\n",
      "83 din\n",
      "157 dinner\n",
      "210 dinosaur\n",
      "122 dire\n",
      "366 direct\n",
      "1204 directed\n",
      "644 directing\n",
      "1453 direction\n",
      "193 directly\n",
      "5119 director\n",
      "121 directorial\n",
      "103 directs\n",
      "76 dirt\n",
      "334 dirty\n",
      "131 disagree\n",
      "101 disappear\n",
      "99 disappeared\n",
      "72 disappears\n",
      "98 disappoint\n",
      "917 disappointed\n",
      "420 disappointing\n",
      "426 disappointment\n",
      "356 disaster\n",
      "175 disbelief\n",
      "139 disc\n",
      "266 discover\n",
      "267 discovered\n",
      "76 discovering\n",
      "240 discovers\n",
      "128 discovery\n",
      "126 discus\n",
      "152 discussion\n",
      "151 disease\n",
      "88 disguise\n",
      "221 disgusting\n",
      "102 disjointed\n",
      "160 dislike\n",
      "79 disliked\n",
      "756 disney\n",
      "82 disorder\n",
      "319 display\n",
      "99 displayed\n",
      "127 distance\n",
      "124 distant\n",
      "84 distinct\n",
      "107 distracting\n",
      "82 distribution\n",
      "107 disturbed\n",
      "484 disturbing\n",
      "107 divorce\n",
      "94 dixon\n",
      "143 doc\n",
      "692 doctor\n",
      "87 document\n",
      "1074 documentary\n",
      "887 dog\n",
      "200 doll\n",
      "369 dollar\n",
      "85 dolph\n",
      "86 domestic\n",
      "103 domino\n",
      "191 donald\n",
      "3096 done\n",
      "116 donna\n",
      "74 dont\n",
      "123 doo\n",
      "92 doom\n",
      "104 doomed\n",
      "557 door\n",
      "142 dorothy\n",
      "75 dose\n",
      "419 double\n",
      "839 doubt\n",
      "322 douglas\n",
      "79 downey\n",
      "96 downhill\n",
      "188 downright\n",
      "299 dozen\n",
      "702 dr\n",
      "121 dracula\n",
      "341 drag\n",
      "138 dragged\n",
      "245 dragon\n",
      "89 drake\n",
      "1552 drama\n",
      "667 dramatic\n",
      "316 draw\n",
      "155 drawing\n",
      "428 drawn\n",
      "240 dreadful\n",
      "1099 dream\n",
      "87 dreary\n",
      "81 dreck\n",
      "256 dress\n",
      "293 dressed\n",
      "87 dressing\n",
      "248 drew\n",
      "207 drink\n",
      "175 drinking\n",
      "603 drive\n",
      "125 drivel\n",
      "239 driven\n",
      "230 driver\n",
      "269 driving\n",
      "307 drop\n",
      "131 dropped\n",
      "85 dropping\n",
      "79 drove\n",
      "728 drug\n",
      "76 drum\n",
      "310 drunk\n",
      "119 drunken\n",
      "234 dry\n",
      "102 dub\n",
      "224 dubbed\n",
      "154 dubbing\n",
      "86 duck\n",
      "104 dud\n",
      "206 dude\n",
      "918 due\n",
      "188 duke\n",
      "815 dull\n",
      "609 dumb\n",
      "84 dump\n",
      "125 duo\n",
      "91 dust\n",
      "105 dutch\n",
      "139 duty\n",
      "76 duvall\n",
      "2411 dvd\n",
      "389 dy\n",
      "314 dying\n",
      "165 dynamic\n",
      "78 dysfunctional\n",
      "94 eager\n",
      "185 ear\n",
      "88 earl\n",
      "662 earlier\n",
      "1605 early\n",
      "100 earned\n",
      "929 earth\n",
      "110 ease\n",
      "132 easier\n",
      "892 easily\n",
      "170 east\n",
      "83 eastern\n",
      "138 eastwood\n",
      "802 easy\n",
      "275 eat\n",
      "90 eaten\n",
      "278 eating\n",
      "111 eccentric\n",
      "80 echo\n",
      "344 ed\n",
      "310 eddie\n",
      "95 edgar\n",
      "483 edge\n",
      "82 edgy\n",
      "107 edie\n",
      "76 edit\n",
      "262 edited\n",
      "774 editing\n",
      "106 edition\n",
      "144 editor\n",
      "97 education\n",
      "83 educational\n",
      "233 edward\n",
      "141 eerie\n",
      "2837 effect\n",
      "512 effective\n",
      "187 effectively\n",
      "1046 effort\n",
      "74 egg\n",
      "147 ego\n",
      "222 eight\n",
      "127 eighty\n",
      "1866 either\n",
      "81 el\n",
      "118 elaborate\n",
      "119 elderly\n",
      "77 election\n",
      "77 electric\n",
      "93 elegant\n",
      "1175 element\n",
      "141 elephant\n",
      "77 elevator\n",
      "75 elite\n",
      "175 elizabeth\n",
      "122 ellen\n",
      "85 elm\n",
      "1998 else\n",
      "139 elsewhere\n",
      "153 elvira\n",
      "154 elvis\n",
      "158 em\n",
      "163 embarrassed\n",
      "226 embarrassing\n",
      "98 embarrassment\n",
      "72 embrace\n",
      "122 emily\n",
      "202 emma\n",
      "785 emotion\n",
      "657 emotional\n",
      "241 emotionally\n",
      "84 empathy\n",
      "97 emperor\n",
      "105 emphasis\n",
      "127 empire\n",
      "94 employee\n",
      "275 empty\n",
      "91 en\n",
      "315 encounter\n",
      "75 encourage\n",
      "6632 end\n",
      "139 endearing\n",
      "556 ended\n",
      "2462 ending\n",
      "235 endless\n",
      "99 endure\n",
      "307 enemy\n",
      "324 energy\n",
      "93 engage\n",
      "110 engaged\n",
      "312 engaging\n",
      "306 england\n",
      "986 english\n",
      "1812 enjoy\n",
      "842 enjoyable\n",
      "1244 enjoyed\n",
      "162 enjoying\n",
      "150 enjoyment\n",
      "112 enjoys\n",
      "103 enormous\n",
      "3451 enough\n",
      "152 ensemble\n",
      "88 ensues\n",
      "196 enter\n",
      "107 enterprise\n",
      "131 enters\n",
      "172 entertain\n",
      "237 entertained\n",
      "1442 entertaining\n",
      "887 entertainment\n",
      "86 enthusiasm\n",
      "1460 entire\n",
      "532 entirely\n",
      "191 entry\n",
      "196 environment\n",
      "366 epic\n",
      "2597 episode\n",
      "176 equal\n",
      "432 equally\n",
      "84 equipment\n",
      "89 equivalent\n",
      "88 er\n",
      "642 era\n",
      "265 eric\n",
      "201 erotic\n",
      "171 error\n",
      "701 escape\n",
      "110 escaped\n",
      "2535 especially\n",
      "138 essence\n",
      "168 essential\n",
      "257 essentially\n",
      "164 established\n",
      "136 estate\n",
      "75 esther\n",
      "103 et\n",
      "1212 etc\n",
      "72 eternal\n",
      "92 ethan\n",
      "75 ethnic\n",
      "83 eugene\n",
      "216 europe\n",
      "340 european\n",
      "140 eva\n",
      "119 eve\n",
      "12646 even\n",
      "259 evening\n",
      "1283 event\n",
      "720 eventually\n",
      "5994 ever\n",
      "3978 every\n",
      "411 everybody\n",
      "167 everyday\n",
      "2222 everyone\n",
      "2321 everything\n",
      "190 everywhere\n",
      "227 evidence\n",
      "127 evident\n",
      "72 evidently\n",
      "1463 evil\n",
      "468 ex\n",
      "189 exact\n",
      "995 exactly\n",
      "120 exaggerated\n",
      "80 examination\n",
      "1554 example\n",
      "2069 excellent\n",
      "1129 except\n",
      "471 exception\n",
      "148 exceptional\n",
      "86 exceptionally\n",
      "96 excess\n",
      "87 excessive\n",
      "105 exchange\n",
      "230 excited\n",
      "224 excitement\n",
      "515 exciting\n",
      "473 excuse\n",
      "241 executed\n",
      "204 execution\n",
      "183 executive\n",
      "140 exercise\n",
      "300 exist\n",
      "114 existed\n",
      "264 existence\n",
      "161 existent\n",
      "161 exists\n",
      "104 exotic\n",
      "1176 expect\n",
      "462 expectation\n",
      "704 expected\n",
      "588 expecting\n",
      "82 expedition\n",
      "140 expensive\n",
      "1259 experience\n",
      "192 experienced\n",
      "260 experiment\n",
      "79 experimental\n",
      "218 expert\n",
      "450 explain\n",
      "285 explained\n",
      "107 explaining\n",
      "193 explains\n",
      "327 explanation\n",
      "119 explicit\n",
      "120 exploit\n",
      "234 exploitation\n",
      "100 exploration\n",
      "118 explore\n",
      "106 explored\n",
      "221 explosion\n",
      "88 expose\n",
      "117 exposed\n",
      "79 exposition\n",
      "88 exposure\n",
      "227 express\n",
      "83 expressed\n",
      "324 expression\n",
      "115 extended\n",
      "172 extent\n",
      "72 exterior\n",
      "543 extra\n",
      "173 extraordinary\n",
      "382 extreme\n",
      "1069 extremely\n",
      "2065 eye\n",
      "133 eyed\n",
      "115 eyre\n",
      "178 fabulous\n",
      "1990 face\n",
      "204 faced\n",
      "178 facial\n",
      "96 facing\n",
      "3747 fact\n",
      "279 factor\n",
      "151 factory\n",
      "117 fade\n",
      "285 fail\n",
      "483 failed\n",
      "137 failing\n",
      "606 fails\n",
      "288 failure\n",
      "456 fair\n",
      "587 fairly\n",
      "216 fairy\n",
      "299 faith\n",
      "178 faithful\n",
      "482 fake\n",
      "122 falk\n",
      "1621 fall\n",
      "165 fallen\n",
      "383 falling\n",
      "193 false\n",
      "230 fame\n",
      "539 familiar\n",
      "3440 family\n",
      "771 famous\n",
      "3331 fan\n",
      "85 fanatic\n",
      "127 fancy\n",
      "798 fantastic\n",
      "725 fantasy\n",
      "2978 far\n",
      "126 farce\n",
      "226 fare\n",
      "119 farm\n",
      "99 farmer\n",
      "90 farrell\n",
      "88 fascinated\n",
      "391 fascinating\n",
      "78 fascination\n",
      "384 fashion\n",
      "138 fashioned\n",
      "897 fast\n",
      "100 faster\n",
      "278 fat\n",
      "124 fatal\n",
      "289 fate\n",
      "2199 father\n",
      "337 fault\n",
      "269 favor\n",
      "1419 favorite\n",
      "102 favour\n",
      "376 favourite\n",
      "121 fay\n",
      "153 fbi\n",
      "664 fear\n",
      "78 feat\n",
      "1434 feature\n",
      "192 featured\n",
      "277 featuring\n",
      "92 fed\n",
      "112 feed\n",
      "3759 feel\n",
      "1540 feeling\n",
      "121 felix\n",
      "351 fell\n",
      "393 fellow\n",
      "1528 felt\n",
      "1022 female\n",
      "98 feminist\n",
      "80 femme\n",
      "137 fest\n",
      "449 festival\n",
      "103 fetched\n",
      "116 fever\n",
      "661 fi\n",
      "133 fianc\n",
      "480 fiction\n",
      "188 fictional\n",
      "84 fido\n",
      "369 field\n",
      "127 fifteen\n",
      "148 fifty\n",
      "1433 fight\n",
      "150 fighter\n",
      "607 fighting\n",
      "949 figure\n",
      "187 figured\n",
      "125 file\n",
      "286 fill\n",
      "551 filled\n",
      "77 filler\n",
      "78 filling\n",
      "47026 film\n",
      "762 filmed\n",
      "393 filming\n",
      "900 filmmaker\n",
      "1336 final\n",
      "271 finale\n",
      "1536 finally\n",
      "105 financial\n",
      "5079 find\n",
      "370 finding\n",
      "1326 fine\n",
      "278 finest\n",
      "174 finger\n",
      "456 finish\n",
      "302 finished\n",
      "676 fire\n",
      "133 fired\n",
      "85 firm\n",
      "9062 first\n",
      "90 firstly\n",
      "161 fish\n",
      "129 fisher\n",
      "78 fishing\n",
      "96 fist\n",
      "704 fit\n",
      "137 fitting\n",
      "936 five\n",
      "108 fix\n",
      "79 flair\n",
      "96 flame\n",
      "207 flash\n",
      "416 flashback\n",
      "589 flat\n",
      "502 flaw\n",
      "156 flawed\n",
      "125 flawless\n",
      "253 flesh\n",
      "1615 flick\n",
      "188 flight\n",
      "89 floating\n",
      "313 floor\n",
      "133 flop\n",
      "107 florida\n",
      "205 flow\n",
      "121 flower\n",
      "338 fly\n",
      "357 flying\n",
      "147 flynn\n",
      "691 focus\n",
      "203 focused\n",
      "106 focusing\n",
      "77 fog\n",
      "462 folk\n",
      "785 follow\n",
      "373 followed\n",
      "565 following\n",
      "499 follows\n",
      "102 fond\n",
      "112 fonda\n",
      "72 fontaine\n",
      "338 food\n",
      "273 fool\n",
      "95 fooled\n",
      "495 foot\n",
      "656 footage\n",
      "221 football\n",
      "120 forbidden\n",
      "786 force\n",
      "660 forced\n",
      "305 ford\n",
      "237 foreign\n",
      "208 forest\n",
      "392 forever\n",
      "716 forget\n",
      "205 forgettable\n",
      "131 forgive\n",
      "178 forgot\n",
      "353 forgotten\n",
      "865 form\n",
      "188 format\n",
      "509 former\n",
      "271 formula\n",
      "98 formulaic\n",
      "190 forth\n",
      "159 fortunately\n",
      "166 fortune\n",
      "156 forty\n",
      "672 forward\n",
      "197 foster\n",
      "81 fought\n",
      "112 foul\n",
      "2576 found\n",
      "915 four\n",
      "178 fourth\n",
      "367 fox\n",
      "78 foxx\n",
      "312 frame\n",
      "77 framed\n",
      "284 france\n",
      "162 franchise\n",
      "91 francis\n",
      "123 francisco\n",
      "98 franco\n",
      "477 frank\n",
      "92 frankenstein\n",
      "90 frankie\n",
      "256 frankly\n",
      "192 freak\n",
      "303 fred\n",
      "287 freddy\n",
      "705 free\n",
      "249 freedom\n",
      "188 freeman\n",
      "72 freeze\n",
      "790 french\n",
      "87 frequent\n",
      "157 frequently\n",
      "376 fresh\n",
      "201 friday\n",
      "3230 friend\n",
      "193 friendly\n",
      "316 friendship\n",
      "78 frightened\n",
      "195 frightening\n",
      "619 front\n",
      "125 frustrated\n",
      "79 frustrating\n",
      "122 frustration\n",
      "284 fu\n",
      "114 fulci\n",
      "1780 full\n",
      "83 fuller\n",
      "426 fully\n",
      "2694 fun\n",
      "116 function\n",
      "113 funeral\n",
      "173 funnier\n",
      "358 funniest\n",
      "4288 funny\n",
      "83 furious\n",
      "106 furthermore\n",
      "85 fury\n",
      "909 future\n",
      "120 futuristic\n",
      "121 fx\n",
      "90 gabriel\n",
      "140 gadget\n",
      "402 gag\n",
      "165 gain\n",
      "96 gal\n",
      "73 gambling\n",
      "1621 game\n",
      "122 gandhi\n",
      "465 gang\n",
      "353 gangster\n",
      "99 gap\n",
      "460 garbage\n",
      "103 garbo\n",
      "143 garden\n",
      "72 garner\n",
      "272 gary\n",
      "201 gas\n",
      "130 gate\n",
      "98 gather\n",
      "1216 gave\n",
      "645 gay\n",
      "82 gear\n",
      "82 geek\n",
      "429 gem\n",
      "102 gender\n",
      "282 gene\n",
      "784 general\n",
      "468 generally\n",
      "85 generated\n",
      "315 generation\n",
      "106 generic\n",
      "124 generous\n",
      "75 genie\n",
      "494 genius\n",
      "1367 genre\n",
      "116 gentle\n",
      "143 gentleman\n",
      "256 genuine\n",
      "251 genuinely\n",
      "929 george\n",
      "91 gerard\n",
      "606 german\n",
      "227 germany\n",
      "81 gesture\n",
      "12513 get\n",
      "1627 getting\n",
      "665 ghost\n",
      "118 giallo\n",
      "440 giant\n",
      "164 gift\n",
      "84 gifted\n",
      "73 gillian\n",
      "96 gimmick\n",
      "73 gina\n",
      "107 ginger\n",
      "4064 girl\n",
      "684 girlfriend\n",
      "4952 give\n",
      "1849 given\n",
      "840 giving\n",
      "450 glad\n",
      "74 glance\n",
      "237 glass\n",
      "118 glenn\n",
      "198 glimpse\n",
      "94 global\n",
      "77 globe\n",
      "104 glorious\n",
      "154 glory\n",
      "142 glover\n",
      "72 glowing\n",
      "7600 go\n",
      "193 goal\n",
      "1271 god\n",
      "137 godfather\n",
      "105 godzilla\n",
      "98 goer\n",
      "4146 going\n",
      "295 gold\n",
      "123 goldberg\n",
      "259 golden\n",
      "754 gone\n",
      "241 gonna\n",
      "15196 good\n",
      "106 goodness\n",
      "161 goofy\n",
      "245 gordon\n",
      "1040 gore\n",
      "368 gorgeous\n",
      "234 gory\n",
      "3583 got\n",
      "136 gothic\n",
      "129 gotta\n",
      "286 gotten\n",
      "450 government\n",
      "188 grab\n",
      "358 grace\n",
      "475 grade\n",
      "118 gradually\n",
      "86 graham\n",
      "307 grand\n",
      "101 grandfather\n",
      "134 grandmother\n",
      "276 grant\n",
      "201 granted\n",
      "408 graphic\n",
      "104 grasp\n",
      "237 gratuitous\n",
      "189 grave\n",
      "125 gray\n",
      "89 grayson\n",
      "9087 great\n",
      "174 greater\n",
      "745 greatest\n",
      "154 greatly\n",
      "82 greatness\n",
      "86 greed\n",
      "91 greedy\n",
      "122 greek\n",
      "413 green\n",
      "83 greg\n",
      "79 gregory\n",
      "250 grew\n",
      "164 grey\n",
      "85 grief\n",
      "118 griffith\n",
      "188 grim\n",
      "103 grinch\n",
      "83 grip\n",
      "150 gripping\n",
      "196 gritty\n",
      "180 gross\n",
      "79 grotesque\n",
      "415 ground\n",
      "1142 group\n",
      "224 grow\n",
      "297 growing\n",
      "252 grown\n",
      "134 grows\n",
      "74 grudge\n",
      "163 gruesome\n",
      "101 guarantee\n",
      "72 guaranteed\n",
      "228 guard\n",
      "1315 guess\n",
      "99 guessed\n",
      "151 guessing\n",
      "205 guest\n",
      "146 guide\n",
      "144 guilt\n",
      "198 guilty\n",
      "80 guitar\n",
      "846 gun\n",
      "94 gundam\n",
      "214 gut\n",
      "4339 guy\n",
      "187 ha\n",
      "88 habit\n",
      "95 hack\n",
      "76 hackneyed\n",
      "519 hair\n",
      "75 haired\n",
      "105 hal\n",
      "2100 half\n",
      "173 halfway\n",
      "235 hall\n",
      "244 halloween\n",
      "110 ham\n",
      "129 hamilton\n",
      "147 hamlet\n",
      "122 hammer\n",
      "72 hammy\n",
      "84 han\n",
      "1889 hand\n",
      "181 handed\n",
      "129 handful\n",
      "233 handle\n",
      "209 handled\n",
      "75 handling\n",
      "228 handsome\n",
      "208 hang\n",
      "217 hanging\n",
      "245 hank\n",
      "1043 happen\n",
      "1076 happened\n",
      "425 happening\n",
      "1080 happens\n",
      "152 happily\n",
      "186 happiness\n",
      "965 happy\n",
      "2667 hard\n",
      "102 hardcore\n",
      "112 harder\n",
      "613 hardly\n",
      "179 hardy\n",
      "89 harm\n",
      "256 harris\n",
      "461 harry\n",
      "200 harsh\n",
      "109 hart\n",
      "101 hartley\n",
      "107 harvey\n",
      "275 hat\n",
      "893 hate\n",
      "296 hated\n",
      "121 hatred\n",
      "72 haunt\n",
      "217 haunted\n",
      "229 haunting\n",
      "79 hawke\n",
      "108 hbo\n",
      "1832 head\n",
      "72 headache\n",
      "169 headed\n",
      "74 heading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 health\n",
      "733 hear\n",
      "1111 heard\n",
      "238 hearing\n",
      "78 hears\n",
      "1463 heart\n",
      "74 heartbreaking\n",
      "225 hearted\n",
      "75 heartfelt\n",
      "133 heat\n",
      "355 heaven\n",
      "180 heavily\n",
      "502 heavy\n",
      "222 heck\n",
      "83 heel\n",
      "155 height\n",
      "76 heist\n",
      "391 held\n",
      "152 helen\n",
      "123 helicopter\n",
      "1032 hell\n",
      "90 hello\n",
      "2255 help\n",
      "324 helped\n",
      "179 helping\n",
      "155 hence\n",
      "83 henchman\n",
      "407 henry\n",
      "72 hepburn\n",
      "1388 hero\n",
      "131 heroic\n",
      "315 heroine\n",
      "136 heston\n",
      "409 hey\n",
      "342 hidden\n",
      "275 hide\n",
      "103 hideous\n",
      "144 hiding\n",
      "2174 high\n",
      "289 higher\n",
      "106 highest\n",
      "327 highlight\n",
      "1147 highly\n",
      "973 hilarious\n",
      "86 hilariously\n",
      "395 hill\n",
      "250 hint\n",
      "190 hip\n",
      "84 hippie\n",
      "84 hippy\n",
      "191 hire\n",
      "188 hired\n",
      "72 historic\n",
      "406 historical\n",
      "86 historically\n",
      "1337 history\n",
      "1360 hit\n",
      "210 hitchcock\n",
      "308 hitler\n",
      "137 hitting\n",
      "74 hk\n",
      "137 ho\n",
      "188 hoffman\n",
      "845 hold\n",
      "209 holding\n",
      "534 hole\n",
      "180 holiday\n",
      "113 hollow\n",
      "107 holly\n",
      "1911 hollywood\n",
      "163 holmes\n",
      "114 holy\n",
      "153 homage\n",
      "1939 home\n",
      "140 homeless\n",
      "104 homer\n",
      "108 homosexual\n",
      "481 honest\n",
      "453 honestly\n",
      "99 honesty\n",
      "191 hong\n",
      "191 honor\n",
      "189 hood\n",
      "127 hook\n",
      "139 hooked\n",
      "101 hop\n",
      "1720 hope\n",
      "144 hoped\n",
      "214 hopefully\n",
      "82 hopeless\n",
      "74 hopelessly\n",
      "407 hoping\n",
      "76 hopkins\n",
      "99 hopper\n",
      "134 horrendous\n",
      "1201 horrible\n",
      "214 horribly\n",
      "116 horrid\n",
      "159 horrific\n",
      "95 horrifying\n",
      "3716 horror\n",
      "443 horse\n",
      "370 hospital\n",
      "176 host\n",
      "688 hot\n",
      "412 hotel\n",
      "2167 hour\n",
      "2289 house\n",
      "91 household\n",
      "102 housewife\n",
      "260 howard\n",
      "3537 however\n",
      "141 hudson\n",
      "944 huge\n",
      "111 hugh\n",
      "74 hughes\n",
      "138 huh\n",
      "1914 human\n",
      "285 humanity\n",
      "94 humble\n",
      "1316 humor\n",
      "263 humorous\n",
      "441 humour\n",
      "300 hundred\n",
      "98 hung\n",
      "77 hungry\n",
      "235 hunt\n",
      "329 hunter\n",
      "143 hunting\n",
      "481 hurt\n",
      "1105 husband\n",
      "96 hyde\n",
      "137 hype\n",
      "72 hyped\n",
      "117 hysterical\n",
      "132 ian\n",
      "283 ice\n",
      "123 icon\n",
      "2638 idea\n",
      "147 ideal\n",
      "130 identify\n",
      "294 identity\n",
      "303 idiot\n",
      "145 idiotic\n",
      "74 ignorance\n",
      "96 ignorant\n",
      "177 ignore\n",
      "120 ignored\n",
      "366 ii\n",
      "135 iii\n",
      "300 ill\n",
      "90 illegal\n",
      "100 illness\n",
      "98 illogical\n",
      "92 im\n",
      "850 image\n",
      "186 imagery\n",
      "75 imaginable\n",
      "384 imagination\n",
      "140 imaginative\n",
      "737 imagine\n",
      "116 imagined\n",
      "702 imdb\n",
      "103 imitation\n",
      "80 immediate\n",
      "462 immediately\n",
      "102 immensely\n",
      "89 immigrant\n",
      "378 impact\n",
      "108 implausible\n",
      "135 importance\n",
      "931 important\n",
      "127 importantly\n",
      "496 impossible\n",
      "128 impress\n",
      "352 impressed\n",
      "444 impression\n",
      "500 impressive\n",
      "97 improve\n",
      "115 improved\n",
      "100 improvement\n",
      "79 inability\n",
      "95 inane\n",
      "90 inappropriate\n",
      "166 incident\n",
      "375 include\n",
      "272 included\n",
      "321 includes\n",
      "1052 including\n",
      "138 incoherent\n",
      "89 incompetent\n",
      "89 incomprehensible\n",
      "72 inconsistent\n",
      "131 increasingly\n",
      "563 incredible\n",
      "626 incredibly\n",
      "722 indeed\n",
      "319 independent\n",
      "179 india\n",
      "527 indian\n",
      "76 indication\n",
      "195 indie\n",
      "363 individual\n",
      "83 inducing\n",
      "80 indulgent\n",
      "75 industrial\n",
      "357 industry\n",
      "175 inept\n",
      "131 inevitable\n",
      "82 inevitably\n",
      "75 inexplicably\n",
      "148 infamous\n",
      "78 infected\n",
      "95 inferior\n",
      "252 influence\n",
      "107 influenced\n",
      "337 information\n",
      "134 ingredient\n",
      "76 inhabitant\n",
      "212 initial\n",
      "178 initially\n",
      "74 injury\n",
      "76 inmate\n",
      "210 inner\n",
      "154 innocence\n",
      "434 innocent\n",
      "110 innovative\n",
      "241 insane\n",
      "607 inside\n",
      "250 insight\n",
      "167 inspector\n",
      "171 inspiration\n",
      "348 inspired\n",
      "135 inspiring\n",
      "141 installment\n",
      "341 instance\n",
      "106 instant\n",
      "128 instantly\n",
      "2190 instead\n",
      "139 instinct\n",
      "77 institution\n",
      "274 insult\n",
      "128 insulting\n",
      "73 insurance\n",
      "82 integrity\n",
      "203 intellectual\n",
      "327 intelligence\n",
      "534 intelligent\n",
      "388 intended\n",
      "344 intense\n",
      "159 intensity\n",
      "130 intent\n",
      "293 intention\n",
      "90 intentionally\n",
      "176 interaction\n",
      "1115 interest\n",
      "650 interested\n",
      "3128 interesting\n",
      "88 interior\n",
      "266 international\n",
      "155 internet\n",
      "209 interpretation\n",
      "338 interview\n",
      "88 intimate\n",
      "127 intrigue\n",
      "120 intrigued\n",
      "301 intriguing\n",
      "84 introduce\n",
      "313 introduced\n",
      "99 introduces\n",
      "173 introduction\n",
      "101 invasion\n",
      "80 invented\n",
      "97 inventive\n",
      "112 investigate\n",
      "146 investigation\n",
      "200 invisible\n",
      "126 invite\n",
      "72 invited\n",
      "99 involve\n",
      "1076 involved\n",
      "116 involvement\n",
      "224 involves\n",
      "465 involving\n",
      "93 iran\n",
      "85 iraq\n",
      "115 ireland\n",
      "195 irish\n",
      "120 iron\n",
      "162 ironic\n",
      "123 ironically\n",
      "163 irony\n",
      "85 irrelevant\n",
      "231 irritating\n",
      "77 ish\n",
      "575 island\n",
      "98 isolated\n",
      "85 israel\n",
      "705 issue\n",
      "562 italian\n",
      "153 italy\n",
      "144 item\n",
      "76 iv\n",
      "933 jack\n",
      "74 jacket\n",
      "232 jackie\n",
      "346 jackson\n",
      "177 jail\n",
      "160 jake\n",
      "1069 james\n",
      "128 jamie\n",
      "657 jane\n",
      "292 japan\n",
      "714 japanese\n",
      "331 jason\n",
      "177 jaw\n",
      "159 jay\n",
      "109 jazz\n",
      "121 jealous\n",
      "373 jean\n",
      "300 jeff\n",
      "107 jeffrey\n",
      "259 jennifer\n",
      "94 jenny\n",
      "138 jeremy\n",
      "163 jerk\n",
      "380 jerry\n",
      "73 jess\n",
      "130 jesse\n",
      "171 jessica\n",
      "289 jesus\n",
      "141 jet\n",
      "111 jew\n",
      "79 jewel\n",
      "164 jewish\n",
      "489 jim\n",
      "274 jimmy\n",
      "302 joan\n",
      "2459 job\n",
      "690 joe\n",
      "81 joel\n",
      "128 joey\n",
      "2222 john\n",
      "313 johnny\n",
      "197 johnson\n",
      "244 join\n",
      "90 joined\n",
      "1600 joke\n",
      "187 jon\n",
      "99 jonathan\n",
      "410 jones\n",
      "233 joseph\n",
      "83 josh\n",
      "136 journalist\n",
      "457 journey\n",
      "320 joy\n",
      "302 jr\n",
      "316 judge\n",
      "102 judging\n",
      "100 judy\n",
      "188 julia\n",
      "86 julian\n",
      "174 julie\n",
      "460 jump\n",
      "92 jumped\n",
      "125 jumping\n",
      "90 june\n",
      "204 jungle\n",
      "92 junior\n",
      "190 junk\n",
      "418 justice\n",
      "98 justify\n",
      "89 justin\n",
      "112 juvenile\n",
      "135 kane\n",
      "79 kansa\n",
      "114 kapoor\n",
      "116 karen\n",
      "78 karl\n",
      "148 karloff\n",
      "300 kate\n",
      "91 kay\n",
      "308 keaton\n",
      "2243 keep\n",
      "276 keeping\n",
      "88 keith\n",
      "429 kelly\n",
      "124 ken\n",
      "111 kennedy\n",
      "116 kenneth\n",
      "750 kept\n",
      "289 kevin\n",
      "478 key\n",
      "102 khan\n",
      "397 kick\n",
      "98 kicked\n",
      "92 kicking\n",
      "3039 kid\n",
      "109 kidding\n",
      "72 kidman\n",
      "128 kidnapped\n",
      "1764 kill\n",
      "1111 killed\n",
      "1699 killer\n",
      "826 killing\n",
      "209 kim\n",
      "2974 kind\n",
      "275 kinda\n",
      "1037 king\n",
      "97 kingdom\n",
      "114 kirk\n",
      "202 kiss\n",
      "83 kissing\n",
      "123 kitchen\n",
      "74 kitty\n",
      "73 kline\n",
      "897 knew\n",
      "188 knife\n",
      "117 knight\n",
      "195 knock\n",
      "76 knocked\n",
      "7067 know\n",
      "447 knowing\n",
      "284 knowledge\n",
      "1080 known\n",
      "78 kolchak\n",
      "270 kong\n",
      "144 korean\n",
      "133 kubrick\n",
      "128 kudos\n",
      "80 kumar\n",
      "243 kung\n",
      "84 kurosawa\n",
      "149 kurt\n",
      "96 kyle\n",
      "614 la\n",
      "137 lab\n",
      "76 label\n",
      "74 labor\n",
      "1423 lack\n",
      "121 lacked\n",
      "277 lacking\n",
      "79 lackluster\n",
      "78 ladder\n",
      "1144 lady\n",
      "160 laid\n",
      "255 lake\n",
      "742 lame\n",
      "439 land\n",
      "87 landing\n",
      "201 landscape\n",
      "171 lane\n",
      "566 language\n",
      "555 large\n",
      "225 largely\n",
      "143 larger\n",
      "183 larry\n",
      "2993 last\n",
      "99 lasted\n",
      "1211 late\n",
      "96 lately\n",
      "2200 later\n",
      "201 latest\n",
      "95 latin\n",
      "362 latter\n",
      "2032 laugh\n",
      "422 laughable\n",
      "82 laughably\n",
      "366 laughed\n",
      "528 laughing\n",
      "247 laughter\n",
      "166 laura\n",
      "123 laurel\n",
      "75 laurence\n",
      "606 law\n",
      "111 lawrence\n",
      "245 lawyer\n",
      "131 lay\n",
      "92 layer\n",
      "168 lazy\n",
      "2229 le\n",
      "2060 lead\n",
      "311 leader\n",
      "622 leading\n",
      "704 leaf\n",
      "196 league\n",
      "78 lean\n",
      "90 leap\n",
      "720 learn\n",
      "254 learned\n",
      "175 learning\n",
      "226 learns\n",
      "3112 least\n",
      "1107 leave\n",
      "482 leaving\n",
      "330 led\n",
      "817 lee\n",
      "2125 left\n",
      "272 leg\n",
      "80 legacy\n",
      "87 legal\n",
      "370 legend\n",
      "192 legendary\n",
      "76 leigh\n",
      "111 lemmon\n",
      "99 lena\n",
      "361 length\n",
      "89 lengthy\n",
      "98 leo\n",
      "81 leon\n",
      "111 leonard\n",
      "242 lesbian\n",
      "173 leslie\n",
      "165 lesser\n",
      "407 lesson\n",
      "72 lester\n",
      "2675 let\n",
      "234 letter\n",
      "146 letting\n",
      "1198 level\n",
      "276 lewis\n",
      "116 li\n",
      "137 liberal\n",
      "79 liberty\n",
      "143 library\n",
      "461 lie\n",
      "8032 life\n",
      "126 lifestyle\n",
      "190 lifetime\n",
      "106 lift\n",
      "1157 light\n",
      "361 lighting\n",
      "382 likable\n",
      "20738 like\n",
      "1516 liked\n",
      "422 likely\n",
      "89 likewise\n",
      "109 liking\n",
      "145 lily\n",
      "167 limit\n",
      "303 limited\n",
      "176 lincoln\n",
      "105 linda\n",
      "75 lindsay\n",
      "3421 line\n",
      "249 liner\n",
      "129 link\n",
      "205 lion\n",
      "182 lip\n",
      "186 lisa\n",
      "623 list\n",
      "125 listed\n",
      "331 listen\n",
      "189 listening\n",
      "109 lit\n",
      "468 literally\n",
      "76 literary\n",
      "91 literature\n",
      "6434 little\n",
      "1552 live\n",
      "382 lived\n",
      "88 lively\n",
      "1063 living\n",
      "139 lloyd\n",
      "216 load\n",
      "85 loaded\n",
      "945 local\n",
      "74 locale\n",
      "600 location\n",
      "110 lock\n",
      "167 locked\n",
      "81 logan\n",
      "252 logic\n",
      "122 logical\n",
      "72 lois\n",
      "106 lol\n",
      "460 london\n",
      "87 lone\n",
      "76 loneliness\n",
      "188 lonely\n",
      "3449 long\n",
      "477 longer\n",
      "6558 look\n",
      "1010 looked\n",
      "2483 looking\n",
      "278 loose\n",
      "118 loosely\n",
      "373 lord\n",
      "123 los\n",
      "345 lose\n",
      "228 loser\n",
      "263 loses\n",
      "220 losing\n",
      "298 loss\n",
      "1554 lost\n",
      "4778 lot\n",
      "124 lou\n",
      "436 loud\n",
      "225 louis\n",
      "90 louise\n",
      "220 lousy\n",
      "144 lovable\n",
      "6857 love\n",
      "1428 loved\n",
      "425 lovely\n",
      "688 lover\n",
      "316 loving\n",
      "1817 low\n",
      "222 lower\n",
      "92 lowest\n",
      "100 loyal\n",
      "89 loyalty\n",
      "143 lucas\n",
      "262 luck\n",
      "130 luckily\n",
      "258 lucky\n",
      "160 lucy\n",
      "176 ludicrous\n",
      "195 lugosi\n",
      "259 luke\n",
      "100 lumet\n",
      "92 lundgren\n",
      "73 lush\n",
      "118 lust\n",
      "146 lying\n",
      "278 lynch\n",
      "124 lyric\n",
      "98 macarthur\n",
      "437 machine\n",
      "71 macho\n",
      "102 macy\n",
      "499 mad\n",
      "8361 made\n",
      "162 madness\n",
      "111 madonna\n",
      "133 mafia\n",
      "177 magazine\n",
      "105 maggie\n",
      "468 magic\n",
      "194 magical\n",
      "261 magnificent\n",
      "94 maid\n",
      "95 mail\n",
      "2265 main\n",
      "393 mainly\n",
      "202 mainstream\n",
      "93 maintain\n",
      "946 major\n",
      "238 majority\n",
      "12225 make\n",
      "657 maker\n",
      "210 makeup\n",
      "2978 making\n",
      "727 male\n",
      "98 mall\n",
      "81 malone\n",
      "6019 man\n",
      "272 manage\n",
      "425 managed\n",
      "168 manager\n",
      "583 manages\n",
      "121 manhattan\n",
      "99 maniac\n",
      "88 manipulative\n",
      "90 mankind\n",
      "124 mann\n",
      "461 manner\n",
      "169 mansion\n",
      "6673 many\n",
      "87 map\n",
      "116 mar\n",
      "79 marc\n",
      "119 march\n",
      "114 margaret\n",
      "175 maria\n",
      "253 marie\n",
      "86 marine\n",
      "99 mario\n",
      "86 marion\n",
      "765 mark\n",
      "224 market\n",
      "84 marketing\n",
      "447 marriage\n",
      "590 married\n",
      "226 marry\n",
      "81 marshall\n",
      "73 martha\n",
      "328 martial\n",
      "96 martian\n",
      "371 martin\n",
      "90 marty\n",
      "76 marvel\n",
      "160 marvelous\n",
      "567 mary\n",
      "300 mask\n",
      "244 mass\n",
      "153 massacre\n",
      "192 massive\n",
      "560 master\n",
      "87 masterful\n",
      "697 masterpiece\n",
      "689 match\n",
      "89 matched\n",
      "183 mate\n",
      "789 material\n",
      "197 matrix\n",
      "227 matt\n",
      "1351 matter\n",
      "156 matthau\n",
      "129 matthew\n",
      "182 mature\n",
      "215 max\n",
      "3392 may\n",
      "2340 maybe\n",
      "72 mayhem\n",
      "96 mayor\n",
      "75 mccoy\n",
      "83 mclaglen\n",
      "2444 mean\n",
      "509 meaning\n",
      "144 meaningful\n",
      "108 meaningless\n",
      "614 meant\n",
      "249 meanwhile\n",
      "134 measure\n",
      "129 meat\n",
      "85 mechanical\n",
      "153 medical\n",
      "365 mediocre\n",
      "449 medium\n",
      "1345 meet\n",
      "263 meeting\n",
      "87 meg\n",
      "119 mel\n",
      "209 melodrama\n",
      "122 melodramatic\n",
      "89 melody\n",
      "101 melting\n",
      "879 member\n",
      "666 memorable\n",
      "585 memory\n",
      "1915 men\n",
      "105 menace\n",
      "126 menacing\n",
      "309 mental\n",
      "73 mentality\n",
      "160 mentally\n",
      "896 mention\n",
      "564 mentioned\n",
      "85 mentioning\n",
      "182 mere\n",
      "359 merely\n",
      "193 merit\n",
      "72 mermaid\n",
      "83 meryl\n",
      "661 mess\n",
      "959 message\n",
      "91 messed\n",
      "286 met\n",
      "190 metal\n",
      "107 metaphor\n",
      "188 method\n",
      "193 mexican\n",
      "185 mexico\n",
      "199 mgm\n",
      "1370 michael\n",
      "172 michelle\n",
      "108 mickey\n",
      "319 mid\n",
      "958 middle\n",
      "178 midnight\n",
      "2919 might\n",
      "87 mighty\n",
      "132 miike\n",
      "288 mike\n",
      "133 mild\n",
      "174 mildly\n",
      "115 mildred\n",
      "383 mile\n",
      "464 military\n",
      "83 milk\n",
      "144 mill\n",
      "170 miller\n",
      "546 million\n",
      "87 millionaire\n",
      "139 min\n",
      "2180 mind\n",
      "162 minded\n",
      "154 mindless\n",
      "314 mine\n",
      "219 mini\n",
      "118 minimal\n",
      "85 minimum\n",
      "74 minister\n",
      "409 minor\n",
      "81 minority\n",
      "82 minus\n",
      "3735 minute\n",
      "127 miracle\n",
      "234 mirror\n",
      "143 miscast\n",
      "100 miserable\n",
      "124 miserably\n",
      "97 misery\n",
      "76 misleading\n",
      "1001 miss\n",
      "565 missed\n",
      "75 missile\n",
      "594 missing\n",
      "303 mission\n",
      "626 mistake\n",
      "108 mistaken\n",
      "96 mistress\n",
      "125 mitchell\n",
      "406 mix\n",
      "287 mixed\n",
      "104 mixture\n",
      "83 miyazaki\n",
      "83 mm\n",
      "165 mob\n",
      "81 mode\n",
      "345 model\n",
      "931 modern\n",
      "122 modesty\n",
      "91 molly\n",
      "382 mom\n",
      "2775 moment\n",
      "84 mon\n",
      "2368 money\n",
      "128 monk\n",
      "230 monkey\n",
      "93 monologue\n",
      "932 monster\n",
      "136 montage\n",
      "97 montana\n",
      "420 month\n",
      "458 mood\n",
      "98 moody\n",
      "301 moon\n",
      "234 moore\n",
      "422 moral\n",
      "122 morality\n",
      "73 morbid\n",
      "75 moreover\n",
      "275 morgan\n",
      "84 mormon\n",
      "271 morning\n",
      "112 moron\n",
      "83 moronic\n",
      "98 morris\n",
      "941 mostly\n",
      "1593 mother\n",
      "147 motif\n",
      "501 motion\n",
      "217 motivation\n",
      "306 mountain\n",
      "228 mouse\n",
      "386 mouth\n",
      "1257 move\n",
      "322 moved\n",
      "321 movement\n",
      "51694 movie\n",
      "853 moving\n",
      "1708 mr\n",
      "180 mst\n",
      "86 mtv\n",
      "9764 much\n",
      "76 muddled\n",
      "140 multi\n",
      "190 multiple\n",
      "115 mummy\n",
      "86 mundane\n",
      "73 muppet\n",
      "1430 murder\n",
      "260 murdered\n",
      "226 murderer\n",
      "76 murdering\n",
      "109 murderous\n",
      "215 murphy\n",
      "81 murray\n",
      "103 museum\n",
      "3061 music\n",
      "1167 musical\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154 musician\n",
      "136 muslim\n",
      "3249 must\n",
      "98 mutant\n",
      "97 myers\n",
      "404 mysterious\n",
      "955 mystery\n",
      "91 myth\n",
      "169 nail\n",
      "218 naive\n",
      "437 naked\n",
      "1995 name\n",
      "799 named\n",
      "96 namely\n",
      "228 nancy\n",
      "197 narration\n",
      "448 narrative\n",
      "129 narrator\n",
      "342 nasty\n",
      "82 nathan\n",
      "249 nation\n",
      "258 national\n",
      "304 native\n",
      "463 natural\n",
      "267 naturally\n",
      "723 nature\n",
      "74 naughty\n",
      "162 navy\n",
      "304 nazi\n",
      "145 nd\n",
      "824 near\n",
      "109 nearby\n",
      "815 nearly\n",
      "149 neat\n",
      "180 necessarily\n",
      "324 necessary\n",
      "164 neck\n",
      "182 ned\n",
      "2648 need\n",
      "683 needed\n",
      "198 needle\n",
      "377 negative\n",
      "228 neighbor\n",
      "158 neighborhood\n",
      "134 neil\n",
      "537 neither\n",
      "153 nelson\n",
      "79 nemesis\n",
      "112 neo\n",
      "99 nephew\n",
      "115 nerd\n",
      "119 nerve\n",
      "97 nervous\n",
      "73 net\n",
      "233 network\n",
      "6484 never\n",
      "236 nevertheless\n",
      "4310 new\n",
      "77 newcomer\n",
      "89 newly\n",
      "79 newman\n",
      "331 news\n",
      "139 newspaper\n",
      "1716 next\n",
      "2012 nice\n",
      "299 nicely\n",
      "92 nicholas\n",
      "113 nicholson\n",
      "296 nick\n",
      "82 nicole\n",
      "73 niece\n",
      "2293 night\n",
      "426 nightmare\n",
      "163 nine\n",
      "88 ninety\n",
      "128 ninja\n",
      "100 niro\n",
      "158 noble\n",
      "462 nobody\n",
      "102 nod\n",
      "419 noir\n",
      "214 noise\n",
      "75 nolan\n",
      "222 nominated\n",
      "160 nomination\n",
      "898 non\n",
      "1032 none\n",
      "163 nonetheless\n",
      "288 nonsense\n",
      "82 nonsensical\n",
      "77 norm\n",
      "458 normal\n",
      "304 normally\n",
      "127 norman\n",
      "213 north\n",
      "173 nose\n",
      "91 nostalgia\n",
      "97 nostalgic\n",
      "173 notable\n",
      "119 notably\n",
      "207 notch\n",
      "841 note\n",
      "144 noted\n",
      "4292 nothing\n",
      "408 notice\n",
      "272 noticed\n",
      "140 notion\n",
      "172 notorious\n",
      "86 novak\n",
      "1126 novel\n",
      "167 nowadays\n",
      "446 nowhere\n",
      "86 nuance\n",
      "129 nuclear\n",
      "202 nude\n",
      "596 nudity\n",
      "1409 number\n",
      "270 numerous\n",
      "85 nun\n",
      "140 nurse\n",
      "162 nut\n",
      "86 nyc\n",
      "217 object\n",
      "87 objective\n",
      "163 obnoxious\n",
      "119 obscure\n",
      "94 observation\n",
      "235 obsessed\n",
      "175 obsession\n",
      "74 obstacle\n",
      "1066 obvious\n",
      "1163 obviously\n",
      "181 occasion\n",
      "193 occasional\n",
      "256 occasionally\n",
      "112 occur\n",
      "114 occurred\n",
      "131 occurs\n",
      "116 ocean\n",
      "582 odd\n",
      "160 oddly\n",
      "108 odds\n",
      "104 offended\n",
      "212 offensive\n",
      "724 offer\n",
      "192 offered\n",
      "141 offering\n",
      "588 office\n",
      "387 officer\n",
      "145 official\n",
      "1601 often\n",
      "1450 oh\n",
      "143 oil\n",
      "1017 ok\n",
      "706 okay\n",
      "4579 old\n",
      "656 older\n",
      "220 oliver\n",
      "107 olivier\n",
      "95 ollie\n",
      "90 omen\n",
      "27738 one\n",
      "89 online\n",
      "328 onto\n",
      "923 open\n",
      "155 opened\n",
      "982 opening\n",
      "429 opera\n",
      "116 operation\n",
      "1047 opinion\n",
      "478 opportunity\n",
      "122 opposed\n",
      "282 opposite\n",
      "105 option\n",
      "91 orange\n",
      "1060 order\n",
      "79 ordered\n",
      "267 ordinary\n",
      "123 origin\n",
      "3427 original\n",
      "171 originality\n",
      "290 originally\n",
      "85 orleans\n",
      "96 orson\n",
      "1007 oscar\n",
      "87 othello\n",
      "1595 others\n",
      "670 otherwise\n",
      "114 ought\n",
      "73 out\n",
      "132 outcome\n",
      "104 outer\n",
      "175 outfit\n",
      "93 outing\n",
      "124 outrageous\n",
      "597 outside\n",
      "417 outstanding\n",
      "87 overacting\n",
      "1438 overall\n",
      "154 overcome\n",
      "120 overdone\n",
      "92 overlook\n",
      "128 overlooked\n",
      "249 overly\n",
      "114 overrated\n",
      "109 overwhelming\n",
      "98 owen\n",
      "326 owner\n",
      "102 oz\n",
      "561 pace\n",
      "300 paced\n",
      "295 pacing\n",
      "202 pacino\n",
      "210 pack\n",
      "85 package\n",
      "157 packed\n",
      "463 page\n",
      "358 paid\n",
      "430 pain\n",
      "417 painful\n",
      "240 painfully\n",
      "242 paint\n",
      "102 painted\n",
      "83 painter\n",
      "205 painting\n",
      "254 pair\n",
      "127 pal\n",
      "86 palace\n",
      "79 palance\n",
      "77 pale\n",
      "108 palma\n",
      "94 paltrow\n",
      "85 pamela\n",
      "153 pan\n",
      "123 panic\n",
      "97 pant\n",
      "281 paper\n",
      "229 par\n",
      "87 parade\n",
      "77 paradise\n",
      "145 parallel\n",
      "77 paramount\n",
      "93 paranoia\n",
      "881 parent\n",
      "405 paris\n",
      "411 park\n",
      "247 parker\n",
      "287 parody\n",
      "5233 part\n",
      "734 particular\n",
      "1079 particularly\n",
      "127 partly\n",
      "346 partner\n",
      "637 party\n",
      "420 pas\n",
      "106 pass\n",
      "80 passable\n",
      "79 passage\n",
      "246 passed\n",
      "94 passenger\n",
      "191 passing\n",
      "331 passion\n",
      "99 passionate\n",
      "1275 past\n",
      "145 pat\n",
      "246 path\n",
      "468 pathetic\n",
      "81 patience\n",
      "247 patient\n",
      "73 patricia\n",
      "223 patrick\n",
      "88 pattern\n",
      "898 paul\n",
      "119 paulie\n",
      "91 pause\n",
      "76 paxton\n",
      "716 pay\n",
      "184 paying\n",
      "206 peace\n",
      "120 peak\n",
      "103 pearl\n",
      "75 peck\n",
      "76 penn\n",
      "82 penny\n",
      "9384 people\n",
      "161 per\n",
      "73 perception\n",
      "1600 perfect\n",
      "144 perfection\n",
      "637 perfectly\n",
      "150 perform\n",
      "4717 performance\n",
      "192 performed\n",
      "253 performer\n",
      "131 performing\n",
      "84 performs\n",
      "1681 perhaps\n",
      "815 period\n",
      "123 perry\n",
      "1686 person\n",
      "143 persona\n",
      "630 personal\n",
      "494 personality\n",
      "446 personally\n",
      "306 perspective\n",
      "200 pet\n",
      "80 pete\n",
      "853 peter\n",
      "83 petty\n",
      "145 pg\n",
      "90 phantom\n",
      "90 phenomenon\n",
      "80 phil\n",
      "173 philip\n",
      "99 philosophical\n",
      "122 philosophy\n",
      "370 phone\n",
      "86 phony\n",
      "181 photo\n",
      "100 photograph\n",
      "126 photographed\n",
      "137 photographer\n",
      "406 photography\n",
      "110 phrase\n",
      "307 physical\n",
      "151 physically\n",
      "126 piano\n",
      "621 pick\n",
      "329 picked\n",
      "127 picking\n",
      "1936 picture\n",
      "153 pie\n",
      "1960 piece\n",
      "86 pierce\n",
      "144 pig\n",
      "225 pile\n",
      "348 pilot\n",
      "96 pin\n",
      "88 pink\n",
      "114 pirate\n",
      "116 pit\n",
      "164 pitch\n",
      "76 pitiful\n",
      "199 pitt\n",
      "230 pity\n",
      "2822 place\n",
      "188 placed\n",
      "165 plague\n",
      "581 plain\n",
      "625 plan\n",
      "375 plane\n",
      "534 planet\n",
      "101 planned\n",
      "124 planning\n",
      "130 plant\n",
      "149 plastic\n",
      "87 plausible\n",
      "4451 play\n",
      "2588 played\n",
      "586 player\n",
      "1633 playing\n",
      "234 pleasant\n",
      "124 pleasantly\n",
      "1045 please\n",
      "127 pleased\n",
      "347 pleasure\n",
      "632 plenty\n",
      "94 plight\n",
      "6870 plot\n",
      "651 plus\n",
      "77 pocket\n",
      "74 poe\n",
      "94 poem\n",
      "96 poetic\n",
      "92 poetry\n",
      "156 poignant\n",
      "4038 point\n",
      "135 pointed\n",
      "504 pointless\n",
      "83 poison\n",
      "93 pokemon\n",
      "106 polanski\n",
      "83 pole\n",
      "1098 police\n",
      "117 policeman\n",
      "78 policy\n",
      "85 polished\n",
      "608 political\n",
      "106 politically\n",
      "118 politician\n",
      "208 politics\n",
      "158 pool\n",
      "1896 poor\n",
      "713 poorly\n",
      "404 pop\n",
      "112 popcorn\n",
      "550 popular\n",
      "83 popularity\n",
      "120 population\n",
      "368 porn\n",
      "99 porno\n",
      "143 portion\n",
      "164 portrait\n",
      "264 portray\n",
      "578 portrayal\n",
      "601 portrayed\n",
      "227 portraying\n",
      "229 portrays\n",
      "101 pose\n",
      "76 posey\n",
      "211 position\n",
      "548 positive\n",
      "73 positively\n",
      "136 posse\n",
      "108 possessed\n",
      "205 possibility\n",
      "999 possible\n",
      "709 possibly\n",
      "503 post\n",
      "196 poster\n",
      "102 pot\n",
      "616 potential\n",
      "91 potentially\n",
      "127 pound\n",
      "133 poverty\n",
      "214 powell\n",
      "1265 power\n",
      "620 powerful\n",
      "234 practically\n",
      "134 practice\n",
      "196 praise\n",
      "72 praised\n",
      "76 prank\n",
      "309 pre\n",
      "121 precious\n",
      "77 precisely\n",
      "72 predator\n",
      "104 predecessor\n",
      "854 predictable\n",
      "175 prefer\n",
      "178 pregnant\n",
      "108 prejudice\n",
      "83 premiere\n",
      "73 preminger\n",
      "746 premise\n",
      "77 prepare\n",
      "171 prepared\n",
      "75 preposterous\n",
      "85 prequel\n",
      "416 presence\n",
      "823 present\n",
      "164 presentation\n",
      "415 presented\n",
      "77 presenting\n",
      "262 president\n",
      "134 press\n",
      "109 pressure\n",
      "80 preston\n",
      "127 presumably\n",
      "154 pretend\n",
      "95 pretending\n",
      "269 pretentious\n",
      "3662 pretty\n",
      "123 prevent\n",
      "168 preview\n",
      "630 previous\n",
      "204 previously\n",
      "82 prey\n",
      "318 price\n",
      "86 priceless\n",
      "153 pride\n",
      "262 priest\n",
      "104 primarily\n",
      "106 primary\n",
      "205 prime\n",
      "80 primitive\n",
      "315 prince\n",
      "202 princess\n",
      "162 principal\n",
      "101 principle\n",
      "229 print\n",
      "192 prior\n",
      "504 prison\n",
      "178 prisoner\n",
      "280 private\n",
      "116 prize\n",
      "194 pro\n",
      "2841 probably\n",
      "2336 problem\n",
      "125 proceeding\n",
      "88 proceeds\n",
      "308 process\n",
      "281 produce\n",
      "555 produced\n",
      "927 producer\n",
      "108 producing\n",
      "284 product\n",
      "1973 production\n",
      "365 prof\n",
      "77 profanity\n",
      "388 professional\n",
      "224 professor\n",
      "139 profound\n",
      "319 program\n",
      "211 progress\n",
      "623 project\n",
      "116 prom\n",
      "303 promise\n",
      "97 promised\n",
      "207 promising\n",
      "155 proof\n",
      "150 prop\n",
      "203 propaganda\n",
      "229 proper\n",
      "167 properly\n",
      "98 property\n",
      "151 prostitute\n",
      "384 protagonist\n",
      "165 protect\n",
      "187 proud\n",
      "265 prove\n",
      "249 proved\n",
      "304 provide\n",
      "207 provided\n",
      "350 provides\n",
      "118 providing\n",
      "75 provocative\n",
      "166 provoking\n",
      "112 pseudo\n",
      "126 psychiatrist\n",
      "91 psychic\n",
      "263 psycho\n",
      "265 psychological\n",
      "73 psychology\n",
      "75 psychopath\n",
      "122 psychotic\n",
      "565 public\n",
      "526 pull\n",
      "273 pulled\n",
      "121 pulling\n",
      "117 pulp\n",
      "107 pun\n",
      "239 punch\n",
      "89 punishment\n",
      "132 punk\n",
      "143 puppet\n",
      "74 puppy\n",
      "104 purchase\n",
      "90 purchased\n",
      "562 pure\n",
      "169 purely\n",
      "117 purple\n",
      "523 purpose\n",
      "95 pursuit\n",
      "177 push\n",
      "127 pushed\n",
      "118 pushing\n",
      "2760 put\n",
      "369 putting\n",
      "73 puzzle\n",
      "75 quaid\n",
      "1509 quality\n",
      "90 quarter\n",
      "388 queen\n",
      "187 quest\n",
      "1164 question\n",
      "91 questionable\n",
      "338 quick\n",
      "639 quickly\n",
      "283 quiet\n",
      "76 quietly\n",
      "83 quinn\n",
      "174 quirky\n",
      "83 quit\n",
      "3738 quite\n",
      "243 quote\n",
      "104 rabbit\n",
      "424 race\n",
      "192 rachel\n",
      "99 racial\n",
      "153 racism\n",
      "179 racist\n",
      "72 radical\n",
      "298 radio\n",
      "115 rage\n",
      "223 rain\n",
      "79 raines\n",
      "241 raise\n",
      "170 raised\n",
      "94 raising\n",
      "147 ralph\n",
      "114 rambo\n",
      "83 ramones\n",
      "239 ran\n",
      "361 random\n",
      "85 randomly\n",
      "87 randy\n",
      "263 range\n",
      "123 ranger\n",
      "215 rank\n",
      "129 rap\n",
      "401 rape\n",
      "126 raped\n",
      "96 rapist\n",
      "442 rare\n",
      "315 rarely\n",
      "195 rat\n",
      "704 rate\n",
      "507 rated\n",
      "2733 rather\n",
      "1094 rating\n",
      "75 ratso\n",
      "82 rave\n",
      "176 raw\n",
      "393 ray\n",
      "92 raymond\n",
      "123 rd\n",
      "348 reach\n",
      "115 reached\n",
      "95 reaching\n",
      "108 react\n",
      "385 reaction\n",
      "2057 read\n",
      "139 reader\n",
      "703 reading\n",
      "337 ready\n",
      "4737 real\n",
      "126 realise\n",
      "75 realised\n",
      "279 realism\n",
      "757 realistic\n",
      "1050 reality\n",
      "654 realize\n",
      "317 realized\n",
      "199 realizes\n",
      "98 realizing\n",
      "11736 really\n",
      "84 realm\n",
      "74 rear\n",
      "2918 reason\n",
      "117 reasonable\n",
      "119 reasonably\n",
      "150 rebel\n",
      "258 recall\n",
      "112 receive\n",
      "262 received\n",
      "83 receives\n",
      "510 recent\n",
      "579 recently\n",
      "90 recognition\n",
      "195 recognize\n",
      "113 recognized\n",
      "1667 recommend\n",
      "84 recommendation\n",
      "489 recommended\n",
      "353 record\n",
      "104 recorded\n",
      "101 recording\n",
      "831 red\n",
      "326 redeeming\n",
      "144 redemption\n",
      "118 reduced\n",
      "169 reed\n",
      "81 reef\n",
      "116 reel\n",
      "73 refer\n",
      "415 reference\n",
      "74 referred\n",
      "92 reflect\n",
      "107 reflection\n",
      "71 reflects\n",
      "206 refreshing\n",
      "218 refuse\n",
      "78 refused\n",
      "229 regard\n",
      "74 regarded\n",
      "174 regarding\n",
      "125 regardless\n",
      "83 region\n",
      "223 regret\n",
      "292 regular\n",
      "86 reid\n",
      "81 reign\n",
      "94 reject\n",
      "73 rejected\n",
      "235 relate\n",
      "202 related\n",
      "196 relation\n",
      "1327 relationship\n",
      "215 relative\n",
      "213 relatively\n",
      "78 relax\n",
      "883 release\n",
      "986 released\n",
      "132 relevant\n",
      "247 relief\n",
      "106 relies\n",
      "262 religion\n",
      "310 religious\n",
      "72 reluctant\n",
      "75 rely\n",
      "209 remain\n",
      "78 remained\n",
      "120 remaining\n",
      "439 remains\n",
      "660 remake\n",
      "136 remark\n",
      "309 remarkable\n",
      "105 remarkably\n",
      "1702 remember\n",
      "258 remembered\n",
      "157 remind\n",
      "347 reminded\n",
      "297 reminds\n",
      "175 reminiscent\n",
      "164 remote\n",
      "189 remotely\n",
      "82 remove\n",
      "108 removed\n",
      "72 renaissance\n",
      "102 rendition\n",
      "742 rent\n",
      "228 rental\n",
      "337 rented\n",
      "177 renting\n",
      "190 repeat\n",
      "204 repeated\n",
      "119 repeatedly\n",
      "123 repetitive\n",
      "78 replace\n",
      "162 replaced\n",
      "76 reply\n",
      "153 report\n",
      "235 reporter\n",
      "103 represent\n",
      "80 representation\n",
      "99 represented\n",
      "132 represents\n",
      "202 reputation\n",
      "77 require\n",
      "190 required\n",
      "132 requires\n",
      "258 rescue\n",
      "225 research\n",
      "115 resemblance\n",
      "80 resemble\n",
      "113 resembles\n",
      "145 resident\n",
      "84 resist\n",
      "138 resolution\n",
      "111 resort\n",
      "101 resource\n",
      "552 respect\n",
      "83 respected\n",
      "73 respective\n",
      "79 respectively\n",
      "145 response\n",
      "106 responsibility\n",
      "275 responsible\n",
      "1836 rest\n",
      "138 restaurant\n",
      "90 restored\n",
      "906 result\n",
      "82 resulting\n",
      "161 retarded\n",
      "82 retired\n",
      "931 return\n",
      "117 returned\n",
      "126 returning\n",
      "155 reunion\n",
      "189 reveal\n",
      "256 revealed\n",
      "122 revealing\n",
      "183 reveals\n",
      "167 revelation\n",
      "555 revenge\n",
      "1566 review\n",
      "507 reviewer\n",
      "198 revolution\n",
      "116 revolutionary\n",
      "154 revolves\n",
      "82 reward\n",
      "131 rex\n",
      "129 reynolds\n",
      "77 rhythm\n",
      "610 rich\n",
      "847 richard\n",
      "90 richards\n",
      "92 richardson\n",
      "105 rick\n",
      "118 rid\n",
      "88 ridden\n",
      "475 ride\n",
      "99 rider\n",
      "964 ridiculous\n",
      "112 ridiculously\n",
      "155 riding\n",
      "100 rifle\n",
      "3496 right\n",
      "497 ring\n",
      "95 riot\n",
      "373 rip\n",
      "138 ripped\n",
      "314 rise\n",
      "89 rising\n",
      "194 risk\n",
      "80 rita\n",
      "135 ritter\n",
      "76 ritual\n",
      "205 rival\n",
      "344 river\n",
      "99 riveting\n",
      "470 road\n",
      "252 rob\n",
      "80 robber\n",
      "102 robbery\n",
      "80 robbins\n",
      "1093 robert\n",
      "249 robin\n",
      "102 robinson\n",
      "329 robot\n",
      "157 rochester\n",
      "1014 rock\n",
      "141 rocket\n",
      "86 rocky\n",
      "77 rod\n",
      "203 roger\n",
      "170 rogers\n",
      "4300 role\n",
      "386 roll\n",
      "83 rolled\n",
      "80 roller\n",
      "183 rolling\n",
      "118 roman\n",
      "747 romance\n",
      "863 romantic\n",
      "72 rome\n",
      "117 romero\n",
      "93 romp\n",
      "183 ron\n",
      "86 roof\n",
      "1044 room\n",
      "111 roommate\n",
      "87 rooney\n",
      "204 root\n",
      "89 rope\n",
      "272 rose\n",
      "74 rosemary\n",
      "83 ross\n",
      "96 roth\n",
      "96 rotten\n",
      "187 rough\n",
      "293 round\n",
      "73 route\n",
      "256 routine\n",
      "139 row\n",
      "228 roy\n",
      "94 royal\n",
      "90 rubber\n",
      "276 rubbish\n",
      "94 ruby\n",
      "77 rude\n",
      "287 ruin\n",
      "227 ruined\n",
      "86 rukh\n",
      "419 rule\n",
      "1731 run\n",
      "80 runner\n",
      "992 running\n",
      "110 rural\n",
      "172 rush\n",
      "138 rushed\n",
      "209 russell\n",
      "79 russia\n",
      "341 russian\n",
      "122 ruth\n",
      "101 ruthless\n",
      "224 ryan\n",
      "84 sabrina\n",
      "167 sacrifice\n",
      "995 sad\n",
      "112 sadistic\n",
      "575 sadly\n",
      "112 sadness\n",
      "227 safe\n",
      "98 safety\n",
      "108 saga\n",
      "2196 said\n",
      "88 sailor\n",
      "117 saint\n",
      "255 sake\n",
      "105 sale\n",
      "134 sally\n",
      "457 sam\n",
      "131 samurai\n",
      "186 san\n",
      "81 sand\n",
      "174 sandler\n",
      "98 sandra\n",
      "279 santa\n",
      "94 sappy\n",
      "192 sarah\n",
      "293 sat\n",
      "129 satan\n",
      "276 satire\n",
      "106 satisfied\n",
      "87 satisfy\n",
      "216 satisfying\n",
      "222 saturday\n",
      "145 savage\n",
      "1166 save\n",
      "276 saved\n",
      "289 saving\n",
      "3171 saw\n",
      "6504 say\n",
      "952 saying\n",
      "224 scale\n",
      "408 scare\n",
      "144 scarecrow\n",
      "304 scared\n",
      "988 scary\n",
      "232 scenario\n",
      "10584 scene\n",
      "419 scenery\n",
      "138 scheme\n",
      "1725 school\n",
      "658 sci\n",
      "556 science\n",
      "119 scientific\n",
      "477 scientist\n",
      "136 scooby\n",
      "98 scope\n",
      "1121 score\n",
      "84 scotland\n",
      "587 scott\n",
      "93 scottish\n",
      "75 scratch\n",
      "385 scream\n",
      "271 screaming\n",
      "2566 screen\n",
      "190 screening\n",
      "735 screenplay\n",
      "222 screenwriter\n",
      "88 screw\n",
      "3173 script\n",
      "118 scripted\n",
      "96 scrooge\n",
      "291 sea\n",
      "156 seagal\n",
      "263 sean\n",
      "327 search\n",
      "147 searching\n",
      "1010 season\n",
      "282 seat\n",
      "2309 second\n",
      "76 secondary\n",
      "104 secondly\n",
      "724 secret\n",
      "144 secretary\n",
      "93 secretly\n",
      "248 section\n",
      "195 security\n",
      "12010 see\n",
      "123 seed\n",
      "2098 seeing\n",
      "265 seek\n",
      "161 seeking\n",
      "2175 seem\n",
      "1363 seemed\n",
      "347 seemingly\n",
      "3618 seems\n",
      "6678 seen\n",
      "394 segment\n",
      "80 seldom\n",
      "88 selection\n",
      "1202 self\n",
      "112 selfish\n",
      "285 sell\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 seller\n",
      "129 selling\n",
      "211 semi\n",
      "231 send\n",
      "139 sends\n",
      "79 sens\n",
      "2325 sense\n",
      "101 senseless\n",
      "89 sensibility\n",
      "181 sensitive\n",
      "395 sent\n",
      "156 sentence\n",
      "109 sentiment\n",
      "146 sentimental\n",
      "79 sentinel\n",
      "165 separate\n",
      "93 september\n",
      "1042 sequel\n",
      "1603 sequence\n",
      "204 serf\n",
      "85 sergeant\n",
      "446 serial\n",
      "3415 series\n",
      "989 serious\n",
      "1002 seriously\n",
      "136 servant\n",
      "167 serve\n",
      "158 served\n",
      "258 service\n",
      "87 serving\n",
      "85 session\n",
      "3307 set\n",
      "809 setting\n",
      "125 settle\n",
      "74 setup\n",
      "360 seven\n",
      "129 seventy\n",
      "1420 several\n",
      "98 severe\n",
      "73 severely\n",
      "1712 sex\n",
      "710 sexual\n",
      "152 sexuality\n",
      "134 sexually\n",
      "445 sexy\n",
      "103 sh\n",
      "96 shade\n",
      "298 shadow\n",
      "127 shake\n",
      "297 shakespeare\n",
      "80 shaky\n",
      "133 shall\n",
      "259 shallow\n",
      "674 shame\n",
      "81 shanghai\n",
      "188 shape\n",
      "439 share\n",
      "75 shared\n",
      "183 shark\n",
      "78 sharon\n",
      "209 sharp\n",
      "100 shaw\n",
      "92 shed\n",
      "247 sheer\n",
      "80 sheet\n",
      "140 shelf\n",
      "72 shell\n",
      "95 shelley\n",
      "248 sheriff\n",
      "107 shift\n",
      "258 shine\n",
      "129 shining\n",
      "426 ship\n",
      "103 shirley\n",
      "161 shirt\n",
      "431 shock\n",
      "209 shocked\n",
      "339 shocking\n",
      "75 shoddy\n",
      "175 shoe\n",
      "602 shoot\n",
      "511 shooting\n",
      "84 shootout\n",
      "290 shop\n",
      "2013 short\n",
      "77 shortcoming\n",
      "122 shortly\n",
      "2998 shot\n",
      "153 shoulder\n",
      "8603 show\n",
      "128 showcase\n",
      "85 showdown\n",
      "489 showed\n",
      "172 shower\n",
      "786 showing\n",
      "994 shown\n",
      "167 shut\n",
      "129 shy\n",
      "100 sibling\n",
      "487 sick\n",
      "82 sid\n",
      "1434 side\n",
      "127 sidekick\n",
      "180 sidney\n",
      "372 sight\n",
      "380 sign\n",
      "89 signed\n",
      "82 significance\n",
      "182 significant\n",
      "143 silence\n",
      "439 silent\n",
      "77 silliness\n",
      "888 silly\n",
      "155 silver\n",
      "852 similar\n",
      "144 similarity\n",
      "98 similarly\n",
      "84 simmons\n",
      "274 simon\n",
      "1023 simple\n",
      "91 simplicity\n",
      "101 simplistic\n",
      "1965 simply\n",
      "146 simpson\n",
      "78 simultaneously\n",
      "205 sin\n",
      "246 sinatra\n",
      "2906 since\n",
      "86 sincere\n",
      "270 sing\n",
      "353 singer\n",
      "520 singing\n",
      "939 single\n",
      "148 sings\n",
      "163 sinister\n",
      "128 sink\n",
      "187 sir\n",
      "93 sirk\n",
      "83 sissy\n",
      "1025 sister\n",
      "709 sit\n",
      "211 sitcom\n",
      "284 site\n",
      "95 sits\n",
      "459 sitting\n",
      "1155 situation\n",
      "387 six\n",
      "116 sixty\n",
      "141 size\n",
      "123 sketch\n",
      "447 skill\n",
      "220 skin\n",
      "327 skip\n",
      "138 skit\n",
      "98 skull\n",
      "350 sky\n",
      "143 slap\n",
      "177 slapstick\n",
      "539 slasher\n",
      "92 slaughter\n",
      "172 slave\n",
      "164 sleazy\n",
      "389 sleep\n",
      "179 sleeping\n",
      "94 slice\n",
      "93 slick\n",
      "79 slide\n",
      "137 slight\n",
      "122 slightest\n",
      "541 slightly\n",
      "92 slip\n",
      "117 sloppy\n",
      "1131 slow\n",
      "412 slowly\n",
      "90 slug\n",
      "1647 small\n",
      "105 smaller\n",
      "417 smart\n",
      "76 smash\n",
      "355 smile\n",
      "87 smiling\n",
      "496 smith\n",
      "132 smoke\n",
      "134 smoking\n",
      "129 smooth\n",
      "181 snake\n",
      "109 sneak\n",
      "78 sniper\n",
      "97 snl\n",
      "152 snow\n",
      "304 soap\n",
      "101 soccer\n",
      "584 social\n",
      "707 society\n",
      "83 soderbergh\n",
      "297 soft\n",
      "162 sold\n",
      "770 soldier\n",
      "120 sole\n",
      "97 solely\n",
      "504 solid\n",
      "108 solo\n",
      "106 solution\n",
      "139 solve\n",
      "299 somebody\n",
      "757 somehow\n",
      "2363 someone\n",
      "5077 something\n",
      "1218 sometimes\n",
      "965 somewhat\n",
      "483 somewhere\n",
      "1506 son\n",
      "1915 song\n",
      "1222 soon\n",
      "72 sooner\n",
      "129 sophisticated\n",
      "129 soprano\n",
      "771 sorry\n",
      "1662 sort\n",
      "542 soul\n",
      "1965 sound\n",
      "175 sounded\n",
      "97 sounding\n",
      "778 soundtrack\n",
      "72 soup\n",
      "260 source\n",
      "472 south\n",
      "164 southern\n",
      "125 soviet\n",
      "786 space\n",
      "83 spacey\n",
      "85 spain\n",
      "81 span\n",
      "281 spanish\n",
      "114 spare\n",
      "127 spark\n",
      "522 speak\n",
      "405 speaking\n",
      "202 speaks\n",
      "2135 special\n",
      "85 specially\n",
      "97 specie\n",
      "149 specific\n",
      "101 specifically\n",
      "250 spectacular\n",
      "264 speech\n",
      "266 speed\n",
      "175 spell\n",
      "508 spend\n",
      "133 spending\n",
      "187 spends\n",
      "535 spent\n",
      "101 spider\n",
      "154 spielberg\n",
      "137 spike\n",
      "174 spin\n",
      "650 spirit\n",
      "127 spirited\n",
      "125 spiritual\n",
      "77 spit\n",
      "179 spite\n",
      "104 splatter\n",
      "123 splendid\n",
      "157 split\n",
      "92 spock\n",
      "230 spoil\n",
      "123 spoiled\n",
      "971 spoiler\n",
      "91 spoke\n",
      "169 spoken\n",
      "211 spoof\n",
      "124 spooky\n",
      "336 sport\n",
      "522 spot\n",
      "107 spread\n",
      "158 spring\n",
      "76 springer\n",
      "269 spy\n",
      "80 squad\n",
      "107 square\n",
      "360 st\n",
      "80 stab\n",
      "87 stack\n",
      "112 staff\n",
      "785 stage\n",
      "111 staged\n",
      "79 stake\n",
      "87 stale\n",
      "87 stalker\n",
      "91 stallone\n",
      "144 stan\n",
      "1213 stand\n",
      "803 standard\n",
      "250 standing\n",
      "168 stanley\n",
      "161 stanwyck\n",
      "3782 star\n",
      "102 stare\n",
      "83 stargate\n",
      "93 staring\n",
      "76 stark\n",
      "183 starred\n",
      "484 starring\n",
      "2920 start\n",
      "963 started\n",
      "283 starting\n",
      "867 state\n",
      "132 stated\n",
      "242 statement\n",
      "367 station\n",
      "83 statue\n",
      "182 status\n",
      "969 stay\n",
      "182 stayed\n",
      "109 staying\n",
      "72 steady\n",
      "459 steal\n",
      "144 stealing\n",
      "99 steel\n",
      "102 stellar\n",
      "519 step\n",
      "342 stephen\n",
      "343 stereotype\n",
      "177 stereotypical\n",
      "483 steve\n",
      "241 steven\n",
      "95 stevens\n",
      "472 stewart\n",
      "586 stick\n",
      "138 stiff\n",
      "5655 still\n",
      "117 stiller\n",
      "93 stilted\n",
      "147 stink\n",
      "117 stinker\n",
      "267 stock\n",
      "115 stole\n",
      "190 stolen\n",
      "177 stomach\n",
      "412 stone\n",
      "133 stood\n",
      "126 stooge\n",
      "1242 stop\n",
      "230 stopped\n",
      "587 store\n",
      "174 storm\n",
      "13164 story\n",
      "822 storyline\n",
      "173 storytelling\n",
      "868 straight\n",
      "76 straightforward\n",
      "926 strange\n",
      "166 strangely\n",
      "236 stranger\n",
      "103 streep\n",
      "968 street\n",
      "152 streisand\n",
      "306 strength\n",
      "109 stress\n",
      "180 stretch\n",
      "83 stretched\n",
      "129 strictly\n",
      "266 strike\n",
      "137 striking\n",
      "191 string\n",
      "169 strip\n",
      "74 stroke\n",
      "1095 strong\n",
      "133 stronger\n",
      "222 strongly\n",
      "128 struck\n",
      "216 structure\n",
      "482 struggle\n",
      "198 struggling\n",
      "80 stuart\n",
      "350 stuck\n",
      "752 student\n",
      "698 studio\n",
      "320 study\n",
      "1181 stuff\n",
      "111 stumble\n",
      "78 stumbled\n",
      "82 stunned\n",
      "405 stunning\n",
      "275 stunt\n",
      "1701 stupid\n",
      "166 stupidity\n",
      "1715 style\n",
      "158 stylish\n",
      "385 sub\n",
      "816 subject\n",
      "79 subjected\n",
      "121 subplot\n",
      "87 subplots\n",
      "121 subsequent\n",
      "230 substance\n",
      "207 subtitle\n",
      "434 subtle\n",
      "136 subtlety\n",
      "150 succeed\n",
      "106 succeeded\n",
      "167 succeeds\n",
      "613 success\n",
      "524 successful\n",
      "161 successfully\n",
      "457 suck\n",
      "251 sucked\n",
      "248 sudden\n",
      "538 suddenly\n",
      "97 sue\n",
      "181 suffer\n",
      "150 suffered\n",
      "257 suffering\n",
      "201 suffers\n",
      "84 suffice\n",
      "78 sugar\n",
      "377 suggest\n",
      "81 suggested\n",
      "76 suggestion\n",
      "145 suggests\n",
      "331 suicide\n",
      "432 suit\n",
      "94 suitable\n",
      "111 suited\n",
      "193 sullivan\n",
      "235 sum\n",
      "185 summary\n",
      "388 summer\n",
      "199 sun\n",
      "195 sunday\n",
      "77 sung\n",
      "77 sunny\n",
      "117 sunshine\n",
      "501 super\n",
      "670 superb\n",
      "126 superbly\n",
      "112 superficial\n",
      "119 superhero\n",
      "344 superior\n",
      "314 superman\n",
      "202 supernatural\n",
      "92 supply\n",
      "429 support\n",
      "81 supported\n",
      "899 supporting\n",
      "397 suppose\n",
      "1516 supposed\n",
      "365 supposedly\n",
      "2683 sure\n",
      "417 surely\n",
      "202 surface\n",
      "102 surfing\n",
      "918 surprise\n",
      "801 surprised\n",
      "302 surprising\n",
      "466 surprisingly\n",
      "208 surreal\n",
      "73 surround\n",
      "134 surrounded\n",
      "134 surrounding\n",
      "106 survival\n",
      "260 survive\n",
      "91 survived\n",
      "104 surviving\n",
      "186 survivor\n",
      "186 susan\n",
      "457 suspect\n",
      "84 suspend\n",
      "739 suspense\n",
      "192 suspenseful\n",
      "73 suspension\n",
      "77 suspicion\n",
      "88 suspicious\n",
      "160 sutherland\n",
      "113 swear\n",
      "112 swedish\n",
      "595 sweet\n",
      "93 swim\n",
      "97 swimming\n",
      "94 swing\n",
      "135 switch\n",
      "231 sword\n",
      "95 symbol\n",
      "118 symbolism\n",
      "229 sympathetic\n",
      "72 sympathize\n",
      "219 sympathy\n",
      "113 synopsis\n",
      "410 system\n",
      "218 table\n",
      "72 taboo\n",
      "79 tacky\n",
      "97 tad\n",
      "109 tag\n",
      "93 tail\n",
      "5699 take\n",
      "986 taken\n",
      "955 taking\n",
      "956 tale\n",
      "1201 talent\n",
      "586 talented\n",
      "1062 talk\n",
      "126 talked\n",
      "954 talking\n",
      "121 tall\n",
      "111 tame\n",
      "111 tank\n",
      "128 tap\n",
      "298 tape\n",
      "82 tarantino\n",
      "263 target\n",
      "292 tarzan\n",
      "210 task\n",
      "515 taste\n",
      "74 tasteless\n",
      "95 taught\n",
      "75 tax\n",
      "92 taxi\n",
      "316 taylor\n",
      "78 te\n",
      "136 tea\n",
      "214 teach\n",
      "391 teacher\n",
      "101 teaching\n",
      "883 team\n",
      "462 tear\n",
      "82 tech\n",
      "306 technical\n",
      "195 technically\n",
      "80 technicolor\n",
      "286 technique\n",
      "252 technology\n",
      "198 ted\n",
      "218 tedious\n",
      "537 teen\n",
      "325 teenage\n",
      "438 teenager\n",
      "183 teeth\n",
      "909 television\n",
      "2598 tell\n",
      "617 telling\n",
      "130 temple\n",
      "851 ten\n",
      "101 tenant\n",
      "215 tend\n",
      "78 tendency\n",
      "102 tender\n",
      "86 tends\n",
      "153 tense\n",
      "574 tension\n",
      "600 term\n",
      "1638 terrible\n",
      "274 terribly\n",
      "433 terrific\n",
      "142 terrifying\n",
      "134 territory\n",
      "209 terror\n",
      "192 terrorist\n",
      "112 terry\n",
      "260 test\n",
      "85 testament\n",
      "210 texas\n",
      "172 text\n",
      "746 th\n",
      "439 thank\n",
      "182 thankfully\n",
      "470 thanks\n",
      "345 thats\n",
      "1057 theater\n",
      "359 theatre\n",
      "228 theatrical\n",
      "1239 theme\n",
      "248 theory\n",
      "335 therefore\n",
      "116 thick\n",
      "214 thief\n",
      "360 thin\n",
      "8213 thing\n",
      "7733 think\n",
      "1179 thinking\n",
      "776 third\n",
      "203 thirty\n",
      "254 thomas\n",
      "85 thompson\n",
      "350 thoroughly\n",
      "4564 though\n",
      "3656 thought\n",
      "98 thoughtful\n",
      "298 thousand\n",
      "111 thread\n",
      "147 threat\n",
      "126 threatening\n",
      "2297 three\n",
      "112 threw\n",
      "240 thrill\n",
      "1046 thriller\n",
      "157 thrilling\n",
      "159 throat\n",
      "1360 throughout\n",
      "569 throw\n",
      "173 throwing\n",
      "409 thrown\n",
      "97 thru\n",
      "149 thug\n",
      "173 thumb\n",
      "77 thunderbird\n",
      "418 thus\n",
      "176 ticket\n",
      "203 tie\n",
      "149 tied\n",
      "75 tierney\n",
      "109 tiger\n",
      "182 tight\n",
      "213 till\n",
      "309 tim\n",
      "15958 time\n",
      "121 timeless\n",
      "169 timing\n",
      "94 timon\n",
      "118 timothy\n",
      "214 tiny\n",
      "95 tip\n",
      "380 tired\n",
      "92 tiresome\n",
      "190 titanic\n",
      "1669 title\n",
      "121 titled\n",
      "1282 today\n",
      "112 todd\n",
      "2243 together\n",
      "144 toilet\n",
      "1063 told\n",
      "785 tom\n",
      "111 tomato\n",
      "113 tommy\n",
      "77 tomorrow\n",
      "201 ton\n",
      "546 tone\n",
      "167 tongue\n",
      "96 tonight\n",
      "522 tony\n",
      "1100 took\n",
      "102 tool\n",
      "86 tooth\n",
      "1911 top\n",
      "223 topic\n",
      "101 topless\n",
      "154 torn\n",
      "321 torture\n",
      "116 tortured\n",
      "632 total\n",
      "1307 totally\n",
      "678 touch\n",
      "170 touched\n",
      "436 touching\n",
      "483 tough\n",
      "161 tour\n",
      "91 tourist\n",
      "285 toward\n",
      "637 towards\n",
      "130 tower\n",
      "1318 town\n",
      "276 toy\n",
      "80 trace\n",
      "504 track\n",
      "75 tracking\n",
      "166 tracy\n",
      "156 trade\n",
      "99 trademark\n",
      "207 tradition\n",
      "257 traditional\n",
      "387 tragedy\n",
      "348 tragic\n",
      "97 trail\n",
      "479 trailer\n",
      "463 train\n",
      "97 trained\n",
      "219 training\n",
      "81 trait\n",
      "106 transfer\n",
      "122 transformation\n",
      "128 transition\n",
      "74 translated\n",
      "123 translation\n",
      "173 trap\n",
      "188 trapped\n",
      "514 trash\n",
      "99 trashy\n",
      "351 travel\n",
      "101 traveling\n",
      "84 travesty\n",
      "217 treasure\n",
      "417 treat\n",
      "275 treated\n",
      "246 treatment\n",
      "279 tree\n",
      "264 trek\n",
      "127 tremendous\n",
      "90 trend\n",
      "217 trial\n",
      "107 tribe\n",
      "144 tribute\n",
      "308 trick\n",
      "772 tried\n",
      "222 trilogy\n",
      "114 trio\n",
      "520 trip\n",
      "78 tripe\n",
      "152 trite\n",
      "141 triumph\n",
      "109 troop\n",
      "606 trouble\n",
      "145 troubled\n",
      "191 truck\n",
      "2333 true\n",
      "1743 truly\n",
      "328 trust\n",
      "752 truth\n",
      "3109 try\n",
      "2473 trying\n",
      "92 tube\n",
      "264 tune\n",
      "86 tunnel\n",
      "189 turkey\n",
      "2610 turn\n",
      "925 turned\n",
      "123 turner\n",
      "345 turning\n",
      "2790 tv\n",
      "115 twelve\n",
      "330 twenty\n",
      "387 twice\n",
      "89 twilight\n",
      "207 twin\n",
      "1043 twist\n",
      "199 twisted\n",
      "6907 two\n",
      "74 tyler\n",
      "1371 type\n",
      "778 typical\n",
      "130 typically\n",
      "354 ugly\n",
      "230 uk\n",
      "248 ultimate\n",
      "520 ultimately\n",
      "140 ultra\n",
      "191 un\n",
      "247 unable\n",
      "83 unaware\n",
      "116 unbearable\n",
      "434 unbelievable\n",
      "116 unbelievably\n",
      "347 uncle\n",
      "149 uncomfortable\n",
      "186 unconvincing\n",
      "74 uncut\n",
      "175 underground\n",
      "82 underlying\n",
      "77 underneath\n",
      "235 underrated\n",
      "1643 understand\n",
      "97 understandable\n",
      "277 understanding\n",
      "90 understated\n",
      "179 understood\n",
      "73 underworld\n",
      "107 undoubtedly\n",
      "107 uneven\n",
      "250 unexpected\n",
      "79 unexpectedly\n",
      "80 unfair\n",
      "76 unfold\n",
      "104 unfolds\n",
      "143 unforgettable\n",
      "210 unfortunate\n",
      "1352 unfortunately\n",
      "267 unfunny\n",
      "96 unhappy\n",
      "110 uniform\n",
      "123 uninspired\n",
      "108 unintentional\n",
      "135 unintentionally\n",
      "198 uninteresting\n",
      "139 union\n",
      "634 unique\n",
      "104 unit\n",
      "216 united\n",
      "217 universal\n",
      "198 universe\n",
      "141 university\n",
      "308 unknown\n",
      "675 unless\n",
      "76 unlikable\n",
      "585 unlike\n",
      "211 unlikely\n",
      "307 unnecessary\n",
      "83 unoriginal\n",
      "110 unpleasant\n",
      "82 unpredictable\n",
      "84 unreal\n",
      "226 unrealistic\n",
      "83 unseen\n",
      "99 unsettling\n",
      "310 unusual\n",
      "106 unwatchable\n",
      "81 uplifting\n",
      "859 upon\n",
      "160 upper\n",
      "266 ups\n",
      "171 upset\n",
      "189 urban\n",
      "121 urge\n",
      "545 us\n",
      "164 usa\n",
      "1803 use\n",
      "1878 used\n",
      "94 useful\n",
      "128 useless\n",
      "162 user\n",
      "801 using\n",
      "83 ustinov\n",
      "965 usual\n",
      "981 usually\n",
      "240 utter\n",
      "454 utterly\n",
      "102 uwe\n",
      "160 vacation\n",
      "74 vader\n",
      "129 vague\n",
      "90 vaguely\n",
      "85 valentine\n",
      "84 valley\n",
      "97 valuable\n",
      "991 value\n",
      "675 vampire\n",
      "507 van\n",
      "79 vance\n",
      "75 variation\n",
      "194 variety\n",
      "603 various\n",
      "100 vast\n",
      "150 vega\n",
      "297 vehicle\n",
      "88 vein\n",
      "94 vengeance\n",
      "90 venture\n",
      "118 verhoeven\n",
      "2410 version\n",
      "109 versus\n",
      "76 vet\n",
      "250 veteran\n",
      "282 vhs\n",
      "169 via\n",
      "98 vice\n",
      "116 vicious\n",
      "792 victim\n",
      "240 victor\n",
      "233 victoria\n",
      "83 victory\n",
      "1866 video\n",
      "192 vietnam\n",
      "1144 view\n",
      "210 viewed\n",
      "2047 viewer\n",
      "838 viewing\n",
      "72 viewpoint\n",
      "270 village\n",
      "878 villain\n",
      "145 vincent\n",
      "1091 violence\n",
      "523 violent\n",
      "176 virgin\n",
      "94 virginia\n",
      "214 virtually\n",
      "72 virtue\n",
      "145 virus\n",
      "89 visible\n",
      "375 vision\n",
      "351 visit\n",
      "75 visited\n",
      "76 visiting\n",
      "77 visitor\n",
      "523 visual\n",
      "259 visually\n",
      "253 visuals\n",
      "109 vivid\n",
      "88 vocal\n",
      "1374 voice\n",
      "98 voiced\n",
      "111 voight\n",
      "93 volume\n",
      "184 von\n",
      "284 vote\n",
      "78 vulgar\n",
      "91 vulnerable\n",
      "84 wacky\n",
      "77 wagner\n",
      "752 wait\n",
      "95 waited\n",
      "549 waiting\n",
      "90 waitress\n",
      "217 wake\n",
      "724 walk\n",
      "195 walked\n",
      "142 walken\n",
      "129 walker\n",
      "439 walking\n",
      "500 wall\n",
      "117 wallace\n",
      "86 walsh\n",
      "251 walter\n",
      "95 wandering\n",
      "81 wang\n",
      "158 wanna\n",
      "134 wannabe\n",
      "4990 want\n",
      "1352 wanted\n",
      "298 wanting\n",
      "2382 war\n",
      "145 ward\n",
      "227 warm\n",
      "109 warming\n",
      "86 warmth\n",
      "153 warn\n",
      "174 warned\n",
      "200 warner\n",
      "344 warning\n",
      "117 warren\n",
      "202 warrior\n",
      "273 washington\n",
      "1509 waste\n",
      "560 wasted\n",
      "150 wasting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7095 watch\n",
      "307 watchable\n",
      "2236 watched\n",
      "75 watcher\n",
      "4605 watching\n",
      "655 water\n",
      "86 watson\n",
      "266 wave\n",
      "72 wax\n",
      "8830 way\n",
      "243 wayne\n",
      "761 weak\n",
      "99 weakest\n",
      "147 weakness\n",
      "109 wealth\n",
      "153 wealthy\n",
      "316 weapon\n",
      "352 wear\n",
      "326 wearing\n",
      "120 web\n",
      "126 website\n",
      "321 wedding\n",
      "662 week\n",
      "216 weekend\n",
      "142 weight\n",
      "664 weird\n",
      "226 welcome\n",
      "10782 well\n",
      "251 welles\n",
      "98 wendy\n",
      "1463 went\n",
      "331 werewolf\n",
      "121 wes\n",
      "476 west\n",
      "763 western\n",
      "98 wet\n",
      "131 whale\n",
      "732 whatever\n",
      "85 whats\n",
      "319 whatsoever\n",
      "85 wheel\n",
      "270 whenever\n",
      "149 whereas\n",
      "856 whether\n",
      "280 whilst\n",
      "1546 white\n",
      "202 whoever\n",
      "3078 whole\n",
      "82 wholly\n",
      "84 whoopi\n",
      "986 whose\n",
      "118 wicked\n",
      "281 wide\n",
      "87 widely\n",
      "155 widmark\n",
      "112 widow\n",
      "2245 wife\n",
      "82 wig\n",
      "441 wild\n",
      "72 wilder\n",
      "80 wilderness\n",
      "77 wildly\n",
      "596 william\n",
      "344 williams\n",
      "85 willie\n",
      "320 willing\n",
      "92 willis\n",
      "200 wilson\n",
      "655 win\n",
      "376 wind\n",
      "322 window\n",
      "246 wing\n",
      "271 winner\n",
      "353 winning\n",
      "242 winter\n",
      "95 wire\n",
      "89 wisdom\n",
      "361 wise\n",
      "1111 wish\n",
      "95 wished\n",
      "85 wishing\n",
      "268 wit\n",
      "414 witch\n",
      "832 within\n",
      "3267 without\n",
      "299 witness\n",
      "88 witnessed\n",
      "273 witty\n",
      "122 wizard\n",
      "156 wolf\n",
      "4624 woman\n",
      "1163 wonder\n",
      "126 wondered\n",
      "1656 wonderful\n",
      "324 wonderfully\n",
      "358 wondering\n",
      "101 wont\n",
      "80 woo\n",
      "671 wood\n",
      "330 wooden\n",
      "240 woody\n",
      "1811 word\n",
      "98 wore\n",
      "5649 work\n",
      "635 worked\n",
      "294 worker\n",
      "817 working\n",
      "3978 world\n",
      "98 worm\n",
      "88 worn\n",
      "117 worried\n",
      "172 worry\n",
      "1468 worse\n",
      "2731 worst\n",
      "2277 worth\n",
      "126 worthless\n",
      "187 worthwhile\n",
      "362 worthy\n",
      "12436 would\n",
      "152 wound\n",
      "84 wounded\n",
      "432 wow\n",
      "114 wrap\n",
      "92 wrapped\n",
      "146 wreck\n",
      "104 wrestling\n",
      "79 wretched\n",
      "670 write\n",
      "1814 writer\n",
      "94 writes\n",
      "1332 writing\n",
      "1616 written\n",
      "1831 wrong\n",
      "573 wrote\n",
      "85 ww\n",
      "78 wwe\n",
      "158 wwii\n",
      "103 ya\n",
      "103 yard\n",
      "77 yawn\n",
      "462 yeah\n",
      "6876 year\n",
      "85 yell\n",
      "92 yelling\n",
      "111 yellow\n",
      "1535 yes\n",
      "119 yesterday\n",
      "2753 yet\n",
      "78 yeti\n",
      "811 york\n",
      "3663 young\n",
      "503 younger\n",
      "74 youngest\n",
      "299 youth\n",
      "388 zero\n",
      "85 zizek\n",
      "71 zombi\n",
      "1258 zombie\n",
      "155 zone\n"
     ]
    }
   ],
   "source": [
    "#print the counts of each word in the vocabulary\n",
    "import numpy as np\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features_bw, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print count, tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression for BWV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Best CV score: 0.942\n",
      "('Best parameters set:', {'warm_start': False, 'C': 0.25, 'n_jobs': None, 'verbose': 0, 'intercept_scaling': 1, 'fit_intercept': True, 'max_iter': 100, 'penalty': 'l1', 'multi_class': 'warn', 'random_state': None, 'dual': False, 'tol': 0.0001, 'solver': 'warn', 'class_weight': None})\n"
     ]
    }
   ],
   "source": [
    "grid_search_bw = GridSearchCV(log_regression, param_grid, n_jobs=-1, cv=5, scoring=\"roc_auc\")\n",
    "print(\"Performing grid search...\")\n",
    "\n",
    "grid_search_bw.fit(train_data_features_bw, train.sentiment)\n",
    "print(\"Best CV score: %0.3f\" % grid_search_bw.best_score_)\n",
    "print(\"Best parameters set:\", grid_search_bw.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features_bw = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features_bw = test_data_features_bw.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = grid_search_bw.best_estimator_.predict(test_data_features_bw)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"Bag_of_Words_LinReg_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest for BWV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "print \"Training the random forest...\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "forest = forest.fit(train_data_features, train[\"sentiment\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the test data\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
    "                   quoting=3 )\n",
    "\n",
    "# Verify that there are 25,000 rows and 2 columns\n",
    "print test.shape\n",
    "\n",
    "# Create an empty list and append the clean reviews one by one\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = [] \n",
    "\n",
    "print \"Cleaning and parsing the test set movie reviews...\\n\"\n",
    "for i in xrange(0,num_reviews):\n",
    "    if( (i+1) % 1000 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % (i+1, num_reviews)\n",
    "    clean_review = review_to_words( test[\"review\"][i] )\n",
    "    clean_test_reviews.append( clean_review )\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"Bag_of_Words_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_bw = mlp.fit(train_data_features_bw, train[\"sentiment\"])\n",
    "\n",
    "# Make sentiment label predictions\n",
    "result = mlp_bw.predict(test_data_features_bw)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"Bag_of_Words_mlp_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading unlabled data for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from unlabeledTrainData\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print \"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
    " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Hash numbers and remove non-letters \n",
    "    hashed_review_text = re.sub('\\d', '#', review_text) \n",
    "    lim_hashed_review_text = re.sub(\"#####+\", \"#####\", hashed_review_text)\n",
    "    letters_only = re.sub(\"[^a-zA-Z#]\", \" \", lim_hashed_review_text) \n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = letters_only.lower().split()\n",
    "    # \n",
    "    # 4. Lemmatize\n",
    "    words = [WordNetLemmatizer().lemmatize(w) for w in words]   \n",
    "    #\n",
    "    # 5. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\\"The Classic War of the Worlds\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\"critics\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\"critics\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\"critics\\\" perceive to be its shortcomings.\"\n",
      "[u'the', u'classic', u'war', u'of', u'the', u'world', u'by', u'timothy', u'hines', u'is', u'a', u'very', u'entertaining', u'film', u'that', u'obviously', u'go', u'to', u'great', u'effort', u'and', u'length', u'to', u'faithfully', u'recreate', u'h', u'g', u'well', u'classic', u'book', u'mr', u'hines', u'succeeds', u'in', u'doing', u'so', u'i', u'and', u'those', u'who', u'watched', u'his', u'film', u'with', u'me', u'appreciated', u'the', u'fact', u'that', u'it', u'wa', u'not', u'the', u'standard', u'predictable', u'hollywood', u'fare', u'that', u'come', u'out', u'every', u'year', u'e', u'g', u'the', u'spielberg', u'version', u'with', u'tom', u'cruise', u'that', u'had', u'only', u'the', u'slightest', u'resemblance', u'to', u'the', u'book', u'obviously', u'everyone', u'look', u'for', u'different', u'thing', u'in', u'a', u'movie', u'those', u'who', u'envision', u'themselves', u'a', u'amateur', u'critic', u'look', u'only', u'to', u'criticize', u'everything', u'they', u'can', u'others', u'rate', u'a', u'movie', u'on', u'more', u'important', u'base', u'like', u'being', u'entertained', u'which', u'is', u'why', u'most', u'people', u'never', u'agree', u'with', u'the', u'critic', u'we', u'enjoyed', u'the', u'effort', u'mr', u'hines', u'put', u'into', u'being', u'faithful', u'to', u'h', u'g', u'well', u'classic', u'novel', u'and', u'we', u'found', u'it', u'to', u'be', u'very', u'entertaining', u'this', u'made', u'it', u'easy', u'to', u'overlook', u'what', u'the', u'critic', u'perceive', u'to', u'be', u'it', u'shortcoming']\n"
     ]
    }
   ],
   "source": [
    "# cheking the cleaner\n",
    "print train['review'][1]\n",
    "a=review_to_wordlist(train['review'][1])\n",
    "print a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the data into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('/Users/Bai/nltk_data/tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "\n",
    "    raw_sentences = tokenizer.tokenize(review.decode('utf-8').strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:273: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:336: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:336: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:336: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:336: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:273: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:336: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:336: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "[u'maybe', u'i', u'just', u'want', u'to', u'get', u'a', u'certain', u'insight', u'into', u'this', u'guy', u'who', u'i', u'thought', u'wa', u'really', u'cool', u'in', u'the', u'eighty', u'just', u'to', u'maybe', u'make', u'up', u'my', u'mind', u'whether', u'he', u'is', u'guilty', u'or', u'innocent']\n"
     ]
    }
   ],
   "source": [
    "#cheking the len and sen0\n",
    "print len(sentences)\n",
    "print sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Saving Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-05 14:23:57,273 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-06-05 14:23:57,293 : INFO : collecting all words and their counts\n",
      "2019-06-05 14:23:57,297 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-06-05 14:23:57,484 : INFO : PROGRESS: at sentence #10000, processed 227242 words, keeping 16049 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-05 14:23:57,756 : INFO : PROGRESS: at sentence #20000, processed 454580 words, keeping 22412 word types\n",
      "2019-06-05 14:23:57,908 : INFO : PROGRESS: at sentence #30000, processed 675279 words, keeping 26966 word types\n",
      "2019-06-05 14:23:58,055 : INFO : PROGRESS: at sentence #40000, processed 903019 words, keeping 30813 word types\n",
      "2019-06-05 14:23:58,198 : INFO : PROGRESS: at sentence #50000, processed 1123513 words, keeping 33853 word types\n",
      "2019-06-05 14:23:58,357 : INFO : PROGRESS: at sentence #60000, processed 1346275 words, keeping 36516 word types\n",
      "2019-06-05 14:23:58,587 : INFO : PROGRESS: at sentence #70000, processed 1570750 words, keeping 38854 word types\n",
      "2019-06-05 14:23:58,758 : INFO : PROGRESS: at sentence #80000, processed 1791260 words, keeping 41015 word types\n",
      "2019-06-05 14:23:58,914 : INFO : PROGRESS: at sentence #90000, processed 2016735 words, keeping 43205 word types\n",
      "2019-06-05 14:23:59,164 : INFO : PROGRESS: at sentence #100000, processed 2239910 words, keeping 45067 word types\n",
      "2019-06-05 14:23:59,303 : INFO : PROGRESS: at sentence #110000, processed 2460915 words, keeping 46763 word types\n",
      "2019-06-05 14:23:59,459 : INFO : PROGRESS: at sentence #120000, processed 2684322 words, keeping 48620 word types\n",
      "2019-06-05 14:23:59,652 : INFO : PROGRESS: at sentence #130000, processed 2911264 words, keeping 50197 word types\n",
      "2019-06-05 14:23:59,823 : INFO : PROGRESS: at sentence #140000, processed 3125277 words, keeping 51555 word types\n",
      "2019-06-05 14:23:59,989 : INFO : PROGRESS: at sentence #150000, processed 3352208 words, keeping 53110 word types\n",
      "2019-06-05 14:24:00,141 : INFO : PROGRESS: at sentence #160000, processed 3576101 words, keeping 54552 word types\n",
      "2019-06-05 14:24:00,351 : INFO : PROGRESS: at sentence #170000, processed 3800697 words, keeping 55888 word types\n",
      "2019-06-05 14:24:00,494 : INFO : PROGRESS: at sentence #180000, processed 4022584 words, keeping 57178 word types\n",
      "2019-06-05 14:24:00,637 : INFO : PROGRESS: at sentence #190000, processed 4249089 words, keeping 58346 word types\n",
      "2019-06-05 14:24:00,876 : INFO : PROGRESS: at sentence #200000, processed 4474491 words, keeping 59506 word types\n",
      "2019-06-05 14:24:01,042 : INFO : PROGRESS: at sentence #210000, processed 4697250 words, keeping 60694 word types\n",
      "2019-06-05 14:24:01,294 : INFO : PROGRESS: at sentence #220000, processed 4923593 words, keeping 61913 word types\n",
      "2019-06-05 14:24:01,490 : INFO : PROGRESS: at sentence #230000, processed 5147466 words, keeping 63074 word types\n",
      "2019-06-05 14:24:01,650 : INFO : PROGRESS: at sentence #240000, processed 5376299 words, keeping 64207 word types\n",
      "2019-06-05 14:24:01,800 : INFO : PROGRESS: at sentence #250000, processed 5591667 words, keeping 65311 word types\n",
      "2019-06-05 14:24:01,974 : INFO : PROGRESS: at sentence #260000, processed 5812931 words, keeping 66351 word types\n",
      "2019-06-05 14:24:02,162 : INFO : PROGRESS: at sentence #270000, processed 6035567 words, keeping 67535 word types\n",
      "2019-06-05 14:24:02,311 : INFO : PROGRESS: at sentence #280000, processed 6262708 words, keeping 69029 word types\n",
      "2019-06-05 14:24:02,487 : INFO : PROGRESS: at sentence #290000, processed 6487136 words, keeping 70410 word types\n",
      "2019-06-05 14:24:02,641 : INFO : PROGRESS: at sentence #300000, processed 6713087 words, keeping 71656 word types\n",
      "2019-06-05 14:24:02,798 : INFO : PROGRESS: at sentence #310000, processed 6939654 words, keeping 72884 word types\n",
      "2019-06-05 14:24:02,958 : INFO : PROGRESS: at sentence #320000, processed 7165821 words, keeping 74138 word types\n",
      "2019-06-05 14:24:03,156 : INFO : PROGRESS: at sentence #330000, processed 7388888 words, keeping 75291 word types\n",
      "2019-06-05 14:24:03,310 : INFO : PROGRESS: at sentence #340000, processed 7619666 words, keeping 76473 word types\n",
      "2019-06-05 14:24:03,524 : INFO : PROGRESS: at sentence #350000, processed 7844267 words, keeping 77551 word types\n",
      "2019-06-05 14:24:03,666 : INFO : PROGRESS: at sentence #360000, processed 8066076 words, keeping 78645 word types\n",
      "2019-06-05 14:24:03,857 : INFO : PROGRESS: at sentence #370000, processed 8294518 words, keeping 79690 word types\n",
      "2019-06-05 14:24:04,097 : INFO : PROGRESS: at sentence #380000, processed 8520986 words, keeping 80804 word types\n",
      "2019-06-05 14:24:04,293 : INFO : PROGRESS: at sentence #390000, processed 8752088 words, keeping 81748 word types\n",
      "2019-06-05 14:24:04,448 : INFO : PROGRESS: at sentence #400000, processed 8976322 words, keeping 82687 word types\n",
      "2019-06-05 14:24:04,599 : INFO : PROGRESS: at sentence #410000, processed 9198947 words, keeping 83587 word types\n",
      "2019-06-05 14:24:04,751 : INFO : PROGRESS: at sentence #420000, processed 9421336 words, keeping 84553 word types\n",
      "2019-06-05 14:24:04,918 : INFO : PROGRESS: at sentence #430000, processed 9650288 words, keeping 85504 word types\n",
      "2019-06-05 14:24:05,136 : INFO : PROGRESS: at sentence #440000, processed 9878405 words, keeping 86432 word types\n",
      "2019-06-05 14:24:05,365 : INFO : PROGRESS: at sentence #450000, processed 10103397 words, keeping 87513 word types\n",
      "2019-06-05 14:24:05,522 : INFO : PROGRESS: at sentence #460000, processed 10337526 words, keeping 88491 word types\n",
      "2019-06-05 14:24:05,688 : INFO : PROGRESS: at sentence #470000, processed 10566712 words, keeping 89279 word types\n",
      "2019-06-05 14:24:05,853 : INFO : PROGRESS: at sentence #480000, processed 10788358 words, keeping 90165 word types\n",
      "2019-06-05 14:24:06,016 : INFO : PROGRESS: at sentence #490000, processed 11016521 words, keeping 91145 word types\n",
      "2019-06-05 14:24:06,179 : INFO : PROGRESS: at sentence #500000, processed 11239500 words, keeping 91983 word types\n",
      "2019-06-05 14:24:06,345 : INFO : PROGRESS: at sentence #510000, processed 11466083 words, keeping 92859 word types\n",
      "2019-06-05 14:24:06,518 : INFO : PROGRESS: at sentence #520000, processed 11690805 words, keeping 93710 word types\n",
      "2019-06-05 14:24:06,677 : INFO : PROGRESS: at sentence #530000, processed 11916533 words, keeping 94470 word types\n",
      "2019-06-05 14:24:06,845 : INFO : PROGRESS: at sentence #540000, processed 12142521 words, keeping 95301 word types\n",
      "2019-06-05 14:24:07,059 : INFO : PROGRESS: at sentence #550000, processed 12369374 words, keeping 96141 word types\n",
      "2019-06-05 14:24:07,318 : INFO : PROGRESS: at sentence #560000, processed 12591945 words, keeping 96962 word types\n",
      "2019-06-05 14:24:07,572 : INFO : PROGRESS: at sentence #570000, processed 12822312 words, keeping 97704 word types\n",
      "2019-06-05 14:24:07,767 : INFO : PROGRESS: at sentence #580000, processed 13045078 words, keeping 98534 word types\n",
      "2019-06-05 14:24:08,005 : INFO : PROGRESS: at sentence #590000, processed 13271813 words, keeping 99331 word types\n",
      "2019-06-05 14:24:08,217 : INFO : PROGRESS: at sentence #600000, processed 13495335 words, keeping 100012 word types\n",
      "2019-06-05 14:24:08,359 : INFO : PROGRESS: at sentence #610000, processed 13717636 words, keeping 100846 word types\n",
      "2019-06-05 14:24:08,515 : INFO : PROGRESS: at sentence #620000, processed 13945398 words, keeping 101555 word types\n",
      "2019-06-05 14:24:08,655 : INFO : PROGRESS: at sentence #630000, processed 14170975 words, keeping 102278 word types\n",
      "2019-06-05 14:24:08,804 : INFO : PROGRESS: at sentence #640000, processed 14393000 words, keeping 103058 word types\n",
      "2019-06-05 14:24:09,062 : INFO : PROGRESS: at sentence #650000, processed 14620209 words, keeping 103801 word types\n",
      "2019-06-05 14:24:09,292 : INFO : PROGRESS: at sentence #660000, processed 14844277 words, keeping 104505 word types\n",
      "2019-06-05 14:24:09,526 : INFO : PROGRESS: at sentence #670000, processed 15068937 words, keeping 105171 word types\n",
      "2019-06-05 14:24:09,727 : INFO : PROGRESS: at sentence #680000, processed 15295100 words, keeping 105847 word types\n",
      "2019-06-05 14:24:09,955 : INFO : PROGRESS: at sentence #690000, processed 15518518 words, keeping 106576 word types\n",
      "2019-06-05 14:24:10,095 : INFO : PROGRESS: at sentence #700000, processed 15748446 words, keeping 107353 word types\n",
      "2019-06-05 14:24:10,243 : INFO : PROGRESS: at sentence #710000, processed 15972732 words, keeping 107972 word types\n",
      "2019-06-05 14:24:10,385 : INFO : PROGRESS: at sentence #720000, processed 16199319 words, keeping 108559 word types\n",
      "2019-06-05 14:24:10,573 : INFO : PROGRESS: at sentence #730000, processed 16426948 words, keeping 109247 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-05 14:24:10,714 : INFO : PROGRESS: at sentence #740000, processed 16649327 words, keeping 109934 word types\n",
      "2019-06-05 14:24:10,874 : INFO : PROGRESS: at sentence #750000, processed 16868987 words, keeping 110521 word types\n",
      "2019-06-05 14:24:11,013 : INFO : PROGRESS: at sentence #760000, processed 17089664 words, keeping 111110 word types\n",
      "2019-06-05 14:24:11,163 : INFO : PROGRESS: at sentence #770000, processed 17318152 words, keeping 111848 word types\n",
      "2019-06-05 14:24:11,327 : INFO : PROGRESS: at sentence #780000, processed 17549660 words, keeping 112521 word types\n",
      "2019-06-05 14:24:11,485 : INFO : PROGRESS: at sentence #790000, processed 17777980 words, keeping 113169 word types\n",
      "2019-06-05 14:24:11,572 : INFO : collected 113595 word types from a corpus of 17901783 raw words and 795538 sentences\n",
      "2019-06-05 14:24:11,574 : INFO : Loading a fresh vocabulary\n",
      "2019-06-05 14:24:11,876 : INFO : effective_min_count=40 retains 14983 unique words (13% of original 113595, drops 98612)\n",
      "2019-06-05 14:24:11,877 : INFO : effective_min_count=40 leaves 17403617 word corpus (97% of original 17901783, drops 498166)\n",
      "2019-06-05 14:24:11,959 : INFO : deleting the raw counts dictionary of 113595 items\n",
      "2019-06-05 14:24:11,967 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-06-05 14:24:11,969 : INFO : downsampling leaves estimated 12820768 word corpus (73.7% of prior 17403617)\n",
      "2019-06-05 14:24:12,076 : INFO : estimated required memory for 14983 words and 300 dimensions: 43450700 bytes\n",
      "2019-06-05 14:24:12,078 : INFO : resetting layer weights\n",
      "2019-06-05 14:24:12,450 : INFO : training model with 4 workers on 14983 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-06-05 14:24:13,466 : INFO : EPOCH 1 - PROGRESS: at 3.17% examples, 404561 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:24:14,472 : INFO : EPOCH 1 - PROGRESS: at 6.97% examples, 442607 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:15,474 : INFO : EPOCH 1 - PROGRESS: at 9.62% examples, 406972 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:16,477 : INFO : EPOCH 1 - PROGRESS: at 12.38% examples, 392608 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:24:17,483 : INFO : EPOCH 1 - PROGRESS: at 16.02% examples, 406502 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:24:18,485 : INFO : EPOCH 1 - PROGRESS: at 19.60% examples, 413548 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:24:19,488 : INFO : EPOCH 1 - PROGRESS: at 23.19% examples, 419755 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:24:20,494 : INFO : EPOCH 1 - PROGRESS: at 26.76% examples, 424169 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:21,509 : INFO : EPOCH 1 - PROGRESS: at 29.98% examples, 422371 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:22,524 : INFO : EPOCH 1 - PROGRESS: at 34.19% examples, 432262 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:23,530 : INFO : EPOCH 1 - PROGRESS: at 38.29% examples, 440832 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:24,540 : INFO : EPOCH 1 - PROGRESS: at 42.28% examples, 446576 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:25,565 : INFO : EPOCH 1 - PROGRESS: at 46.17% examples, 449808 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:26,579 : INFO : EPOCH 1 - PROGRESS: at 49.81% examples, 450946 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:27,579 : INFO : EPOCH 1 - PROGRESS: at 53.69% examples, 453777 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:24:28,605 : INFO : EPOCH 1 - PROGRESS: at 56.94% examples, 451034 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:29,631 : INFO : EPOCH 1 - PROGRESS: at 60.56% examples, 451937 words/s, in_qsize 7, out_qsize 1\n",
      "2019-06-05 14:24:30,621 : INFO : EPOCH 1 - PROGRESS: at 63.69% examples, 449003 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:31,622 : INFO : EPOCH 1 - PROGRESS: at 67.34% examples, 450191 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:24:32,666 : INFO : EPOCH 1 - PROGRESS: at 70.47% examples, 446738 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:33,668 : INFO : EPOCH 1 - PROGRESS: at 74.30% examples, 448903 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:34,680 : INFO : EPOCH 1 - PROGRESS: at 78.36% examples, 451962 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:35,688 : INFO : EPOCH 1 - PROGRESS: at 82.57% examples, 455435 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:24:36,714 : INFO : EPOCH 1 - PROGRESS: at 86.63% examples, 457709 words/s, in_qsize 7, out_qsize 1\n",
      "2019-06-05 14:24:37,729 : INFO : EPOCH 1 - PROGRESS: at 90.79% examples, 460545 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:24:38,731 : INFO : EPOCH 1 - PROGRESS: at 94.89% examples, 462831 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:24:39,736 : INFO : EPOCH 1 - PROGRESS: at 98.69% examples, 463855 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:40,063 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-05 14:24:40,073 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-05 14:24:40,076 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-05 14:24:40,090 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-05 14:24:40,092 : INFO : EPOCH - 1 : training on 17901783 raw words (12822450 effective words) took 27.6s, 463947 effective words/s\n",
      "2019-06-05 14:24:41,112 : INFO : EPOCH 2 - PROGRESS: at 3.46% examples, 438102 words/s, in_qsize 6, out_qsize 1\n",
      "2019-06-05 14:24:42,121 : INFO : EPOCH 2 - PROGRESS: at 6.97% examples, 441590 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:43,133 : INFO : EPOCH 2 - PROGRESS: at 10.63% examples, 447400 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:44,155 : INFO : EPOCH 2 - PROGRESS: at 13.91% examples, 437080 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:45,177 : INFO : EPOCH 2 - PROGRESS: at 17.59% examples, 440507 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:46,200 : INFO : EPOCH 2 - PROGRESS: at 21.67% examples, 452304 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:47,202 : INFO : EPOCH 2 - PROGRESS: at 25.71% examples, 460929 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:48,208 : INFO : EPOCH 2 - PROGRESS: at 29.81% examples, 468968 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:49,209 : INFO : EPOCH 2 - PROGRESS: at 33.96% examples, 474624 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:50,222 : INFO : EPOCH 2 - PROGRESS: at 38.07% examples, 479606 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:51,241 : INFO : EPOCH 2 - PROGRESS: at 42.11% examples, 482411 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:24:52,242 : INFO : EPOCH 2 - PROGRESS: at 46.17% examples, 485567 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:24:53,256 : INFO : EPOCH 2 - PROGRESS: at 50.33% examples, 488913 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:24:54,266 : INFO : EPOCH 2 - PROGRESS: at 54.39% examples, 490877 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:55,267 : INFO : EPOCH 2 - PROGRESS: at 57.31% examples, 483419 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-05 14:24:56,278 : INFO : EPOCH 2 - PROGRESS: at 60.78% examples, 481014 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:24:57,296 : INFO : EPOCH 2 - PROGRESS: at 63.80% examples, 475001 words/s, in_qsize 7, out_qsize 1\n",
      "2019-06-05 14:24:58,317 : INFO : EPOCH 2 - PROGRESS: at 66.80% examples, 469865 words/s, in_qsize 7, out_qsize 1\n",
      "2019-06-05 14:24:59,320 : INFO : EPOCH 2 - PROGRESS: at 70.08% examples, 467003 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:00,334 : INFO : EPOCH 2 - PROGRESS: at 73.64% examples, 466256 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:01,340 : INFO : EPOCH 2 - PROGRESS: at 77.15% examples, 465382 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:02,383 : INFO : EPOCH 2 - PROGRESS: at 80.18% examples, 460927 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:03,389 : INFO : EPOCH 2 - PROGRESS: at 83.24% examples, 457916 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:04,400 : INFO : EPOCH 2 - PROGRESS: at 86.53% examples, 456280 words/s, in_qsize 7, out_qsize 1\n",
      "2019-06-05 14:25:05,417 : INFO : EPOCH 2 - PROGRESS: at 89.18% examples, 451424 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:06,420 : INFO : EPOCH 2 - PROGRESS: at 92.17% examples, 448901 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-05 14:25:07,464 : INFO : EPOCH 2 - PROGRESS: at 94.49% examples, 442483 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:08,465 : INFO : EPOCH 2 - PROGRESS: at 97.93% examples, 442491 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:09,161 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-05 14:25:09,176 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-05 14:25:09,180 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-05 14:25:09,184 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-05 14:25:09,185 : INFO : EPOCH - 2 : training on 17901783 raw words (12819318 effective words) took 29.1s, 440724 effective words/s\n",
      "2019-06-05 14:25:10,212 : INFO : EPOCH 3 - PROGRESS: at 2.55% examples, 323445 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:11,224 : INFO : EPOCH 3 - PROGRESS: at 6.30% examples, 397805 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:12,243 : INFO : EPOCH 3 - PROGRESS: at 10.41% examples, 435790 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:13,255 : INFO : EPOCH 3 - PROGRESS: at 14.54% examples, 455885 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:14,276 : INFO : EPOCH 3 - PROGRESS: at 18.75% examples, 470412 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:15,290 : INFO : EPOCH 3 - PROGRESS: at 22.85% examples, 477168 words/s, in_qsize 7, out_qsize 1\n",
      "2019-06-05 14:25:16,294 : INFO : EPOCH 3 - PROGRESS: at 26.42% examples, 474095 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:17,306 : INFO : EPOCH 3 - PROGRESS: at 30.37% examples, 477383 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:18,321 : INFO : EPOCH 3 - PROGRESS: at 34.46% examples, 481332 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:19,319 : INFO : EPOCH 3 - PROGRESS: at 38.57% examples, 485534 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:20,329 : INFO : EPOCH 3 - PROGRESS: at 42.67% examples, 488986 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:21,330 : INFO : EPOCH 3 - PROGRESS: at 46.73% examples, 491616 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:22,344 : INFO : EPOCH 3 - PROGRESS: at 50.72% examples, 492891 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:23,358 : INFO : EPOCH 3 - PROGRESS: at 54.72% examples, 493889 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-05 14:25:24,360 : INFO : EPOCH 3 - PROGRESS: at 58.73% examples, 495668 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:25,361 : INFO : EPOCH 3 - PROGRESS: at 62.79% examples, 497218 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:26,366 : INFO : EPOCH 3 - PROGRESS: at 66.85% examples, 498559 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:27,371 : INFO : EPOCH 3 - PROGRESS: at 70.86% examples, 499328 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:28,384 : INFO : EPOCH 3 - PROGRESS: at 74.97% examples, 500752 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:29,397 : INFO : EPOCH 3 - PROGRESS: at 79.04% examples, 501378 words/s, in_qsize 7, out_qsize 1\n",
      "2019-06-05 14:25:30,415 : INFO : EPOCH 3 - PROGRESS: at 83.18% examples, 502184 words/s, in_qsize 7, out_qsize 1\n",
      "2019-06-05 14:25:31,423 : INFO : EPOCH 3 - PROGRESS: at 87.36% examples, 503520 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:32,426 : INFO : EPOCH 3 - PROGRESS: at 91.45% examples, 504551 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:33,431 : INFO : EPOCH 3 - PROGRESS: at 95.70% examples, 505793 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-05 14:25:34,434 : INFO : EPOCH 3 - PROGRESS: at 99.63% examples, 506063 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:34,480 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-05 14:25:34,504 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-05 14:25:34,507 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-05 14:25:34,509 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-05 14:25:34,511 : INFO : EPOCH - 3 : training on 17901783 raw words (12819736 effective words) took 25.3s, 506355 effective words/s\n",
      "2019-06-05 14:25:35,521 : INFO : EPOCH 4 - PROGRESS: at 3.79% examples, 485183 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:36,529 : INFO : EPOCH 4 - PROGRESS: at 7.95% examples, 504370 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:37,544 : INFO : EPOCH 4 - PROGRESS: at 12.05% examples, 507694 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:38,552 : INFO : EPOCH 4 - PROGRESS: at 16.13% examples, 510404 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:39,569 : INFO : EPOCH 4 - PROGRESS: at 20.33% examples, 512176 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:40,571 : INFO : EPOCH 4 - PROGRESS: at 24.36% examples, 512541 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:41,585 : INFO : EPOCH 4 - PROGRESS: at 28.37% examples, 511819 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:42,605 : INFO : EPOCH 4 - PROGRESS: at 32.55% examples, 512542 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:43,611 : INFO : EPOCH 4 - PROGRESS: at 36.69% examples, 514015 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:44,628 : INFO : EPOCH 4 - PROGRESS: at 40.74% examples, 513900 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:45,637 : INFO : EPOCH 4 - PROGRESS: at 44.92% examples, 515495 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:46,645 : INFO : EPOCH 4 - PROGRESS: at 48.87% examples, 515015 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:47,655 : INFO : EPOCH 4 - PROGRESS: at 53.07% examples, 516260 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:48,674 : INFO : EPOCH 4 - PROGRESS: at 57.10% examples, 516273 words/s, in_qsize 7, out_qsize 2\n",
      "2019-06-05 14:25:49,684 : INFO : EPOCH 4 - PROGRESS: at 61.21% examples, 516866 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:50,690 : INFO : EPOCH 4 - PROGRESS: at 64.74% examples, 512604 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:25:51,697 : INFO : EPOCH 4 - PROGRESS: at 68.09% examples, 507590 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:52,710 : INFO : EPOCH 4 - PROGRESS: at 71.57% examples, 504032 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:53,716 : INFO : EPOCH 4 - PROGRESS: at 75.42% examples, 503331 words/s, in_qsize 6, out_qsize 1\n",
      "2019-06-05 14:25:54,727 : INFO : EPOCH 4 - PROGRESS: at 79.21% examples, 502236 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:55,740 : INFO : EPOCH 4 - PROGRESS: at 83.12% examples, 501857 words/s, in_qsize 8, out_qsize 1\n",
      "2019-06-05 14:25:56,742 : INFO : EPOCH 4 - PROGRESS: at 86.75% examples, 500067 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:57,744 : INFO : EPOCH 4 - PROGRESS: at 90.73% examples, 500658 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:58,751 : INFO : EPOCH 4 - PROGRESS: at 94.65% examples, 500521 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:25:59,773 : INFO : EPOCH 4 - PROGRESS: at 98.64% examples, 500638 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:26:00,067 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-05 14:26:00,078 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-05 14:26:00,088 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-05 14:26:00,099 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-05 14:26:00,101 : INFO : EPOCH - 4 : training on 17901783 raw words (12818355 effective words) took 25.6s, 501049 effective words/s\n",
      "2019-06-05 14:26:01,123 : INFO : EPOCH 5 - PROGRESS: at 3.84% examples, 486928 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:02,128 : INFO : EPOCH 5 - PROGRESS: at 7.84% examples, 495475 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:03,134 : INFO : EPOCH 5 - PROGRESS: at 11.88% examples, 500899 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:04,137 : INFO : EPOCH 5 - PROGRESS: at 15.97% examples, 505914 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:26:05,149 : INFO : EPOCH 5 - PROGRESS: at 20.15% examples, 509227 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:06,153 : INFO : EPOCH 5 - PROGRESS: at 24.30% examples, 512223 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:26:07,164 : INFO : EPOCH 5 - PROGRESS: at 28.43% examples, 513746 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:08,173 : INFO : EPOCH 5 - PROGRESS: at 32.55% examples, 514060 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-05 14:26:09,189 : INFO : EPOCH 5 - PROGRESS: at 36.69% examples, 514798 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:10,199 : INFO : EPOCH 5 - PROGRESS: at 40.74% examples, 514990 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:11,224 : INFO : EPOCH 5 - PROGRESS: at 44.92% examples, 515715 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:12,241 : INFO : EPOCH 5 - PROGRESS: at 48.98% examples, 516038 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:13,257 : INFO : EPOCH 5 - PROGRESS: at 53.07% examples, 515901 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:14,269 : INFO : EPOCH 5 - PROGRESS: at 57.15% examples, 516295 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:15,274 : INFO : EPOCH 5 - PROGRESS: at 61.16% examples, 516475 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:26:16,290 : INFO : EPOCH 5 - PROGRESS: at 65.18% examples, 515917 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:17,308 : INFO : EPOCH 5 - PROGRESS: at 69.18% examples, 515301 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:18,353 : INFO : EPOCH 5 - PROGRESS: at 72.01% examples, 505863 words/s, in_qsize 8, out_qsize 1\n",
      "2019-06-05 14:26:19,350 : INFO : EPOCH 5 - PROGRESS: at 75.36% examples, 501862 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:26:20,363 : INFO : EPOCH 5 - PROGRESS: at 79.04% examples, 500092 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:21,376 : INFO : EPOCH 5 - PROGRESS: at 81.67% examples, 492058 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:22,384 : INFO : EPOCH 5 - PROGRESS: at 85.68% examples, 492955 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:23,395 : INFO : EPOCH 5 - PROGRESS: at 89.61% examples, 493424 words/s, in_qsize 8, out_qsize 0\n",
      "2019-06-05 14:26:24,390 : INFO : EPOCH 5 - PROGRESS: at 93.64% examples, 494277 words/s, in_qsize 7, out_qsize 0\n",
      "2019-06-05 14:26:25,403 : INFO : EPOCH 5 - PROGRESS: at 97.55% examples, 494510 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-05 14:26:25,987 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-05 14:26:26,005 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-05 14:26:26,010 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-05 14:26:26,019 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-05 14:26:26,021 : INFO : EPOCH - 5 : training on 17901783 raw words (12820517 effective words) took 25.9s, 494760 effective words/s\n",
      "2019-06-05 14:26:26,023 : INFO : training on a 89508915 raw words (64100376 effective words) took 133.6s, 479897 effective words/s\n",
      "2019-06-05 14:26:26,025 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-06-05 14:26:26,205 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2019-06-05 14:26:26,208 : INFO : not storing attribute vectors_norm\n",
      "2019-06-05 14:26:26,211 : INFO : not storing attribute cum_table\n",
      "2019-06-05 14:26:26,213 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-06-05 14:26:26,542 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen\n",
      "berlin\n",
      "paris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \n",
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print model.doesnt_match(\"man woman child kitchen\".split())\n",
    "print model.doesnt_match(\"france england germany berlin\".split())\n",
    "print model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'lad', 0.5136903524398804), (u'lady', 0.5087424516677856), (u'millionaire', 0.49499011039733887), (u'businessman', 0.493978351354599), (u'person', 0.493786096572876), (u'men', 0.480119526386261), (u'woman', 0.4776189923286438), (u'chap', 0.47036212682724), (u'loner', 0.46733981370925903), (u'widow', 0.4560745358467102)]\n",
      "[(u'princess', 0.626689612865448), (u'bombshell', 0.5967222452163696), (u'maria', 0.5821893215179443), (u'stepmother', 0.5769131183624268), (u'maid', 0.5767726898193359), (u'belle', 0.5742414593696594), (u'victoria', 0.5602350234985352), (u'blonde', 0.5532236099243164), (u'bride', 0.548600435256958), (u'latifah', 0.5465130805969238)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print model.most_similar(\"man\")\n",
    "print model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-05 14:26:56,918 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2019-06-05 14:26:56,923 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-06-05 14:26:57,215 : INFO : loading vocabulary recursively from 300features_40minwords_10context.vocabulary.* with mmap=None\n",
      "2019-06-05 14:26:57,216 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2019-06-05 14:26:57,219 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-06-05 14:26:57,221 : INFO : loading trainables recursively from 300features_40minwords_10context.trainables.* with mmap=None\n",
      "2019-06-05 14:26:57,223 : INFO : setting ignored attribute cum_table to None\n",
      "2019-06-05 14:26:57,224 : INFO : loaded 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    ">>> model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.syn0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14983, 300)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.45470645e-02, -6.92680757e-03, -1.49861231e-01,  2.51579899e-02,\n",
       "        8.99196565e-02,  4.62126359e-02,  9.31806639e-02,  1.36209959e-02,\n",
       "        2.38778722e-02,  3.92066091e-02, -4.91487272e-02, -6.64958209e-02,\n",
       "       -8.20645969e-03,  1.18766315e-01, -7.95729086e-02,  8.31209496e-02,\n",
       "       -1.40462890e-02, -6.15559854e-02, -7.47416914e-02, -1.30548447e-01,\n",
       "       -7.18428940e-02,  3.44755277e-02,  6.54032454e-02,  7.32879490e-02,\n",
       "       -5.84810860e-02, -1.88753521e-03, -6.38130382e-02, -5.36053292e-02,\n",
       "       -5.17491736e-02,  1.71523506e-03,  4.05737907e-02, -5.72793521e-02,\n",
       "        4.92551085e-03,  5.54895587e-02, -2.37005390e-02, -1.00924885e-02,\n",
       "        8.05830359e-02, -1.06857624e-02,  9.06864554e-02,  4.78935540e-02,\n",
       "        3.61586809e-02,  3.46396677e-02, -2.93709263e-02, -3.36149111e-02,\n",
       "        5.60174398e-02,  1.58735532e-02, -5.64414598e-02,  3.51388492e-02,\n",
       "       -7.06660300e-02,  9.27353799e-02,  1.59837473e-02,  5.55619486e-02,\n",
       "        4.90941703e-02, -3.04772947e-02, -6.60986602e-02, -1.57746263e-02,\n",
       "       -3.47218812e-02, -5.70892654e-02, -1.20852694e-01, -6.84919506e-02,\n",
       "       -6.41051009e-02,  3.74392280e-03,  3.97960991e-02, -1.24370763e-02,\n",
       "        2.01949980e-02, -8.49601813e-03, -5.46801761e-02,  3.06216460e-02,\n",
       "        1.89355183e-02, -3.93545777e-02, -1.10406823e-01, -8.24745558e-03,\n",
       "        8.38998556e-02, -2.15191278e-03,  1.40880700e-02,  3.05561777e-02,\n",
       "       -1.96131021e-02,  6.66357949e-02, -4.40146141e-02,  4.67312261e-02,\n",
       "       -5.19690439e-02,  7.16471439e-03, -3.18457782e-02, -1.13211922e-01,\n",
       "        3.54590043e-02,  2.36423463e-02, -3.10861506e-02, -5.05704246e-02,\n",
       "        2.57818047e-02,  1.96086825e-03, -5.53049557e-02, -1.84443798e-02,\n",
       "        7.55615626e-03, -2.82987556e-03,  1.69371516e-02, -1.97709631e-03,\n",
       "        8.52465630e-02, -2.87892781e-02, -2.57916134e-02, -3.73910926e-02,\n",
       "       -6.64603412e-02,  1.10435672e-02,  5.33552058e-02, -8.29220265e-02,\n",
       "        6.62095994e-02, -1.10712096e-01, -1.68269884e-03,  1.34743005e-02,\n",
       "       -4.46055122e-02, -1.31205425e-01,  4.85947803e-02,  6.20735213e-02,\n",
       "        1.57073393e-01,  1.41116586e-02,  6.43246025e-02, -4.33677882e-02,\n",
       "        6.66530877e-02,  1.44106532e-02,  6.35697022e-02,  9.06632021e-02,\n",
       "       -1.94758251e-02,  8.10957886e-03, -1.54264774e-02,  1.66165456e-02,\n",
       "        4.25364748e-02, -4.28881589e-03, -1.10784821e-01,  6.04538992e-02,\n",
       "        6.07969686e-02, -7.88654909e-02, -3.28062847e-03,  3.12219448e-02,\n",
       "       -8.31744969e-02,  3.45607614e-03, -3.39902006e-02,  5.68095893e-02,\n",
       "       -4.11471026e-03, -4.65482771e-02,  7.04951724e-03,  4.64562289e-02,\n",
       "       -2.31436938e-02, -5.40734418e-02,  6.53218254e-02,  1.80110950e-02,\n",
       "        1.68043718e-01, -1.77417919e-02, -3.51653025e-02,  4.12070826e-02,\n",
       "       -1.06309980e-01,  3.28079425e-02, -2.31459010e-02,  3.86465676e-02,\n",
       "       -1.34625128e-02,  3.10873091e-02, -3.05005256e-02,  3.62472199e-02,\n",
       "       -6.16032407e-02,  2.76889354e-02,  2.64240764e-02,  1.44582745e-02,\n",
       "       -4.61679138e-02,  3.61901894e-02, -8.78355093e-03, -5.42146116e-02,\n",
       "        3.85447592e-02, -1.52282724e-02, -1.04571268e-01,  7.48982430e-02,\n",
       "        7.37295821e-02,  1.30521715e-01,  1.58478208e-02, -6.84572533e-02,\n",
       "        4.65408005e-02, -1.79573484e-02,  1.06009379e-01, -1.05315670e-01,\n",
       "       -9.62459967e-02, -1.57544333e-02,  1.25376284e-01, -9.92739759e-03,\n",
       "        1.76983364e-02, -3.90439183e-02, -3.96356583e-02, -6.83485195e-02,\n",
       "        1.41607923e-03, -2.37648189e-02,  1.22930082e-02, -7.72124752e-02,\n",
       "        1.07876040e-01, -5.75132610e-04, -4.03048396e-02, -1.15497433e-01,\n",
       "        8.14587027e-02,  2.93291528e-02,  6.42178506e-02,  3.73214930e-02,\n",
       "       -3.87811288e-02,  1.40829891e-01,  3.11192721e-02, -1.81855373e-02,\n",
       "        9.49244872e-02,  5.51319234e-02, -1.07004847e-02, -1.80230364e-02,\n",
       "        7.05427006e-02, -7.55396858e-02, -7.24437237e-02,  6.08440815e-03,\n",
       "        6.04693294e-02,  1.15951292e-01, -3.39796357e-02,  1.46863307e-03,\n",
       "        1.20777227e-02,  7.24386498e-02,  1.15650482e-02,  1.95491873e-02,\n",
       "        4.37628999e-02, -2.78448425e-02,  1.27754286e-01,  1.44283287e-02,\n",
       "        8.69945064e-02,  2.84042116e-02,  1.07175084e-02,  5.18820435e-03,\n",
       "        5.88643039e-03, -3.19007561e-02, -1.65788271e-02, -7.20472857e-02,\n",
       "       -1.12725962e-02, -5.82655780e-02, -3.19882073e-02, -4.82241400e-02,\n",
       "        8.54860991e-02, -1.22062996e-01,  4.54285629e-02, -1.34153171e-02,\n",
       "       -2.21308284e-02, -4.48384807e-02,  8.93124864e-02,  2.48203091e-02,\n",
       "        3.13990824e-02,  1.51479835e-04,  5.00515942e-03,  6.32671406e-03,\n",
       "       -1.72829665e-02, -1.32517759e-02,  2.95203626e-02, -2.68610138e-02,\n",
       "        2.50704996e-02, -1.87282395e-02, -2.20740587e-02, -1.19333029e-01,\n",
       "       -2.82359663e-02,  3.86030450e-02,  1.93828810e-02,  1.03761628e-01,\n",
       "        1.73498585e-03, -6.80931006e-03,  7.48177245e-02, -2.14069407e-03,\n",
       "       -1.41219189e-02,  9.41511020e-02, -4.44827974e-02,  4.94513176e-02,\n",
       "       -4.28730575e-03,  1.82110332e-02,  1.07743330e-01,  3.70007269e-02,\n",
       "        1.85221457e-03,  4.30950746e-02, -1.27128391e-02,  3.13481130e-03,\n",
       "       -6.82096481e-02,  4.83533181e-02, -9.12669450e-02, -9.21239983e-03,\n",
       "       -8.98151920e-05, -7.10537564e-03, -2.15432886e-02,  3.49902846e-02,\n",
       "       -4.55498695e-02,  4.54635248e-02,  9.05402601e-02,  2.39685066e-02,\n",
       "       -8.24328214e-02,  1.07141323e-01, -6.72413707e-02, -8.79245475e-02,\n",
       "       -4.49871607e-02, -1.13630546e-02,  4.63614091e-02,  1.32435421e-02,\n",
       "        9.98675730e-03, -3.39359380e-02, -4.56872918e-02,  6.88517839e-02,\n",
       "       -1.17176369e-01,  7.26439282e-02,  3.94448191e-02,  1.35034442e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging the Feature Vectors for the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        #\n",
    "        # Print a status message every 1000th review\n",
    "        if counter%1000. == 0.:\n",
    "               print \"Review %d of %d\" % (counter, len(reviews))\n",
    "        # \n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, num_features)\n",
    "        #\n",
    "        # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning of train reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Creating average feature vecs for train reviews\n",
      "Review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning of test reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "print \"Cleaning of train reviews\"\n",
    "clean_train_reviews = []\n",
    "i=0\n",
    "for review in train[\"review\"]:\n",
    "    if i%1000. == 0.:\n",
    "        print \"Review %d of %d\" % (i, len(train[\"review\"]))\n",
    "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "    i+=1\n",
    "\n",
    "print \"Creating average feature vecs for train reviews\"\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print \"Cleaning of test reviews\"\n",
    "clean_test_reviews = []\n",
    "i=0\n",
    "for review in test[\"review\"]:\n",
    "    if i%1000. == 0.:\n",
    "        print \"Review %d of %d\" % (i, len(train[\"review\"]))\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "    i+=1\n",
    "\n",
    "print \"Creating average feature vecs for test reviews\"\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-48-ce9df56b8d21>\u001b[0m(46)\u001b[0;36mgetAvgFeatureVecs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     44 \u001b[0;31m        \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     45 \u001b[0;31m        \u001b[0;31m# Call the function (defined above) that makes average feature vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 46 \u001b[0;31m        \u001b[0mreviewFeatureVecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeFeatureVec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     47 \u001b[0;31m        \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     48 \u001b[0;31m        \u001b[0;31m# Increment the counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> print counter\n",
      "1.0\n",
      "ipdb> print type(counter)\n",
      "<type 'float'>\n",
      "\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#cheking the errors\\\n",
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "trainDataVecs = Imputer().fit_transform(trainDataVecs)\n",
    "testDataVecs = Imputer().fit_transform(testDataVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Best CV score: 0.937\n",
      "('Best parameters set:', {'warm_start': False, 'C': 1.0, 'n_jobs': None, 'verbose': 0, 'intercept_scaling': 1, 'fit_intercept': True, 'max_iter': 100, 'penalty': 'l1', 'multi_class': 'warn', 'random_state': None, 'dual': False, 'tol': 0.0001, 'solver': 'warn', 'class_weight': None})\n"
     ]
    }
   ],
   "source": [
    "grid_search_w2v = GridSearchCV(log_regression, param_grid, n_jobs=-1, cv=5, scoring=\"roc_auc\")\n",
    "print(\"Performing grid search...\")\n",
    "\n",
    "grid_search_w2v.fit(trainDataVecs, train.sentiment)\n",
    "print(\"Best CV score: %0.3f\" % grid_search_w2v.best_score_)\n",
    "print(\"Best parameters set:\", grid_search_w2v.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = grid_search_w2v.best_estimator_.predict(testDataVecs)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"Word2Vec_LinReg_AverageVectors.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_w2v = mlp.fit(trainDataVecs, train[\"sentiment\"])\n",
    "\n",
    "# Make sentiment label predictions\n",
    "result = mlp_w2v.predict(testDataVecs)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"Word2Vec_mlp_AverageVectors.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  533.072487116 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for K Means clustering: \", elapsed, \"seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.wv.index2word, idx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "[u'omg']\n",
      "\n",
      "Cluster 1\n",
      "[u'devos', u'emmanuelle']\n",
      "\n",
      "Cluster 2\n",
      "[u'hue', u'palette']\n",
      "\n",
      "Cluster 3\n",
      "[u'plate', u'scratch', u'smoke', u'brake', u'wheel', u'mobile', u'scrap', u'bulldozer', u'coin']\n",
      "\n",
      "Cluster 4\n",
      "[u'function', u'communication']\n",
      "\n",
      "Cluster 5\n",
      "[u'kaye', u'byron', u'carol', u'hatcher', u'raul', u'daniel', u'fleming', u'darcy', u'colin', u'ross', u'heather', u'graham', u'fiennes']\n",
      "\n",
      "Cluster 6\n",
      "[u'biker', u'vegetarian', u'headed']\n",
      "\n",
      "Cluster 7\n",
      "[u'iranian', u'experimental']\n",
      "\n",
      "Cluster 8\n",
      "[u'plagued']\n",
      "\n",
      "Cluster 9\n",
      "[u'enigmatic', u'unsavory', u'crafty', u'oddball', u'eccentric', u'affectionate', u'enthusiastic']\n"
     ]
    }
   ],
   "source": [
    "# For the first 10 clusters\n",
    "for cluster in xrange(0,10):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print \"\\nCluster %d\" % cluster\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in xrange(0,len(word_centroid_map.values())):\n",
    "        if( word_centroid_map.values()[i] == cluster ):\n",
    "            words.append(word_centroid_map.keys()[i])\n",
    "    print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Bag of Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression for Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Best CV score: 0.937\n",
      "('Best parameters set:', {'warm_start': False, 'C': 0.25, 'n_jobs': None, 'verbose': 0, 'intercept_scaling': 1, 'fit_intercept': True, 'max_iter': 100, 'penalty': 'l1', 'multi_class': 'warn', 'random_state': None, 'dual': False, 'tol': 0.0001, 'solver': 'warn', 'class_weight': None})\n"
     ]
    }
   ],
   "source": [
    "grid_search_c = GridSearchCV(log_regression, param_grid, n_jobs=-1, cv=5, scoring=\"roc_auc\")\n",
    "print(\"Performing grid search...\")\n",
    "\n",
    "grid_search_c.fit(train_centroids, train.sentiment)\n",
    "print(\"Best CV score: %0.3f\" % grid_search_c.best_score_)\n",
    "print(\"Best parameters set:\", grid_search_c.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = grid_search_c.best_estimator_.predict(test_centroids)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"BagOfCentroids_LinReg.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest for Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest and extract predictions \n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP for Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_c = mlp.fit(train_centroids, train[\"sentiment\"])\n",
    "\n",
    "# Make sentiment label predictions\n",
    "result = mlp_c.predict(test_centroids)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"BagOfCentroids_mlp.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
